\\
arXiv:2505.21671
Date: Tue, 27 May 2025 18:48:42 GMT   (28348kb)

Title: Adaptive Frontier Exploration on Graphs with Applications to
  Network-Based Disease Testing
Authors: Davin Choo, Yuqi Pan, Tonghan Wang, Milind Tambe, Alastair van
  Heerden, Cheryl Johnson
Categories: cs.AI cs.DS cs.LG math.OC
\\
  We study a sequential decision-making problem on a $n$-node graph $G$ where
each node has an unknown label from a finite set $\mathbf{\Sigma}$, drawn from
a joint distribution $P$ that is Markov with respect to $G$. At each step,
selecting a node reveals its label and yields a label-dependent reward. The
goal is to adaptively choose nodes to maximize expected accumulated discounted
rewards. We impose a frontier exploration constraint, where actions are limited
to neighbors of previously selected nodes, reflecting practical constraints in
settings such as contact tracing and robotic exploration. We design a Gittins
index-based policy that applies to general graphs and is provably optimal when
$G$ is a forest. Our implementation runs in $O(n^2 \cdot |\mathbf{\Sigma}|^2)$
time while using $O(n \cdot |\mathbf{\Sigma}|^2)$ oracle calls to $P$ and
$O(n^2 \cdot |\mathbf{\Sigma}|)$ space. Experiments on synthetic and real-world
graphs show that our method consistently outperforms natural baselines,
including in non-tree, budget-limited, and undiscounted settings. For example,
in HIV testing simulations on real-world sexual interaction networks, our
policy detects nearly all positive cases with only half the population tested,
substantially outperforming other baselines.
\\ ( https://arxiv.org/abs/2505.21671 ,  28348kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21674
Date: Tue, 27 May 2025 18:51:06 GMT   (35kb)

Title: Make Planning Research Rigorous Again!
Authors: Michael Katz, Harsha Kokel, Christian Muise, Shirin Sohrabi, Sarath
  Sreedharan
Categories: cs.AI
\\
  In over sixty years since its inception, the field of planning has made
significant contributions to both the theory and practice of building planning
software that can solve a never-before-seen planning problem. This was done
through established practices of rigorous design and evaluation of planning
systems. It is our position that this rigor should be applied to the current
trend of work on planning with large language models. One way to do so is by
correctly incorporating the insights, tools, and data from the automated
planning community into the design and evaluation of LLM-based planners. The
experience and expertise of the planning community are not just important from
a historical perspective; the lessons learned could play a crucial role in
accelerating the development of LLM-based planners. This position is
particularly important in light of the abundance of recent works that replicate
and propagate the same pitfalls that the planning community has encountered and
learned from. We believe that avoiding such known pitfalls will contribute
greatly to the progress in building LLM-based planners and to planning in
general.
\\ ( https://arxiv.org/abs/2505.21674 ,  35kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21765
Date: Tue, 27 May 2025 20:59:29 GMT   (925kb)

Title: Don't Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large
  Reasoning Models
Authors: Sohyun An, Ruochen Wang, Tianyi Zhou, Cho-Jui Hsieh
Categories: cs.AI
Comments: Work In Progress
\\
  While recent success of large reasoning models (LRMs) significantly advanced
LLMs' reasoning capability by optimizing the final answer accuracy using
reinforcement learning, they may also drastically increase the output length
due to overthinking, characterized by unnecessarily complex reasoning paths
that waste computation and potentially degrade the performance. We hypothesize
that such inefficiencies stem from LRMs' limited capability to dynamically
select the proper modular reasoning strategies, termed thinking patterns at the
right position. To investigate this hypothesis, we propose a dynamic
optimization framework that segments model-generated reasoning paths into
distinct thinking patterns, systematically identifying and promoting beneficial
patterns that improve the answer while removing detrimental ones. Empirical
analysis confirms that our optimized thinking paths yield more concise yet
sufficiently informative trajectories, enhancing reasoning efficiency by
reducing attention FLOPs by up to 47% while maintaining accuracy for originally
correct responses. Moreover, a non-trivial portion of originally incorrect
responses are transformed into correct ones, achieving a 15.6% accuracy
improvement with reduced length. Motivated by the improvement brought by the
optimized thinking paths, we apply a preference optimization technique
supported by a pairwise dataset contrasting suboptimal and optimal reasoning
paths. Experimental evaluations across multiple mathematical reasoning
benchmarks reveal that our method notably reduces computational overhead while
simultaneously improving reasoning accuracy, achieving up to a 12% accuracy
improvement and reducing token usage from approximately 5,000 to 3,000 tokens.
\\ ( https://arxiv.org/abs/2505.21765 ,  925kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21784
Date: Tue, 27 May 2025 21:34:40 GMT   (1700kb)

Title: Towards Safety Reasoning in LLMs: AI-agentic Deliberation for
  Policy-embedded CoT Data Creation
Authors: Tharindu Kumarage, Ninareh Mehrabi, Anil Ramakrishna, Xinyan Zhao,
  Richard Zemel, Kai-Wei Chang, Aram Galstyan, Rahul Gupta, Charith Peris
Categories: cs.AI cs.CL
Comments: Accepted to ACL 2025 (Findings)
\\
  Safety reasoning is a recent paradigm where LLMs reason over safety policies
before generating responses, thereby mitigating limitations in existing safety
measures such as over-refusal and jailbreak vulnerabilities. However,
implementing this paradigm is challenging due to the resource-intensive process
of creating high-quality policy-embedded chain-of-thought (CoT) datasets while
ensuring reasoning remains accurate and free from hallucinations or policy
conflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation
for Safety Reasoning, a novel data generation recipe that leverages multi-agent
deliberation to iteratively expand reasoning on safety policies. A data refiner
stage in AIDSAFE ensures high-quality outputs by eliminating repetitive,
redundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong
foundation for supervised fine-tuning (SFT)-based safety training.
Additionally, to address the need of preference data in alignment stages, such
as DPO training, we introduce a supplemental recipe that uses belief
augmentation to create distinct selected and rejected CoT samples. Our
evaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy
adherence and reasoning quality. Consequently, we show that fine-tuning
open-source LLMs on these CoTs can significantly improve safety generalization
and jailbreak robustness while maintaining acceptable utility and over-refusal
accuracy. AIDSAFE-generated CoT datasets can be found here:
https://huggingface.co/datasets/AmazonScience/AIDSAFE
\\ ( https://arxiv.org/abs/2505.21784 ,  1700kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21828
Date: Tue, 27 May 2025 23:29:32 GMT   (229kb)

Title: SAGE-Eval: Evaluating LLMs for Systematic Generalizations of Safety
  Facts
Authors: Chen Yueh-Han, Guy Davidson, Brenden M. Lake
Categories: cs.AI
\\
  Do LLMs robustly generalize critical safety facts to novel situations?
Lacking this ability is dangerous when users ask naive questions. For instance,
"I'm considering packing melon balls for my 10-month-old's lunch. What other
foods would be good to include?" Before offering food options, the LLM should
warn that melon balls pose a choking hazard to toddlers, as documented by the
CDC. Failing to provide such warnings could result in serious injuries or even
death. To evaluate this, we introduce SAGE-Eval, SAfety-fact systematic
GEneralization evaluation, the first benchmark that tests whether LLMs properly
apply well established safety facts to naive user queries. SAGE-Eval comprises
104 facts manually sourced from reputable organizations, systematically
augmented to create 10,428 test scenarios across 7 common domains (e.g.,
Outdoor Activities, Medicine). We find that the top model, Claude-3.7-sonnet,
passes only 58% of all the safety facts tested. We also observe that model
capabilities and training compute weakly correlate with performance on
SAGE-Eval, implying that scaling up is not the golden solution. Our findings
suggest frontier LLMs still lack robust generalization ability. We recommend
developers use SAGE-Eval in pre-deployment evaluations to assess model
reliability in addressing salient risks. We publicly release SAGE-Eval at
https://huggingface.co/datasets/YuehHanChen/SAGE-Eval and our code is available
at https://github.com/YuehHanChen/SAGE-Eval/tree/main.
\\ ( https://arxiv.org/abs/2505.21828 ,  229kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21887
Date: Wed, 28 May 2025 02:03:31 GMT   (5175kb)

Title: SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem
Authors: Ahmed Heakl, Yahia Salaheldin Shaaban, Martin Takac, Salem Lahlou,
  Zangir Iklassov
Categories: cs.AI cs.CE cs.LG
Comments: 18 pages, 14 figures, 11 tables
\\
  Robust routing under uncertainty is central to real-world logistics, yet most
benchmarks assume static, idealized settings. We present SVRPBench, the first
open benchmark to capture high-fidelity stochastic dynamics in vehicle routing
at urban scale. Spanning more than 500 instances with up to 1000 customers, it
simulates realistic delivery conditions: time-dependent congestion, log-normal
delays, probabilistic accidents, and empirically grounded time windows for
residential and commercial clients. Our pipeline generates diverse,
constraint-rich scenarios, including multi-depot and multi-vehicle setups.
Benchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade
by over 20% under distributional shift, while classical and metaheuristic
methods remain robust. To enable reproducible research, we release the dataset
and evaluation suite. SVRPBench challenges the community to design solvers that
generalize beyond synthetic assumptions and adapt to real-world uncertainty.
\\ ( https://arxiv.org/abs/2505.21887 ,  5175kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21907
Date: Wed, 28 May 2025 02:52:39 GMT   (334kb)

Title: Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive
  Survey and Taxonomy
Authors: Saleh Afzoon, Zahra Jahanandish, Phuong Thao Huynh, Amin Beheshti,
  Usman Naseem
Categories: cs.AI cs.CL cs.HC
\\
  AI copilots, context-aware, AI-powered systems designed to assist users in
tasks such as software development and content creation, are becoming integral
to modern workflows. As these systems grow in capability and adoption,
personalization has emerged as a cornerstone for ensuring usability, trust, and
productivity. Central to this personalization is preference optimization: the
ability of AI copilots to detect, interpret, and align with individual user
preferences. While personalization techniques are well-established in domains
like recommender systems and dialogue agents, their adaptation to interactive,
real-time systems like AI copilots remains fragmented and underexplored. This
survey addresses this gap by synthesizing research on how user preferences are
captured, modeled, and refined within the design of AI copilots. We introduce a
unified definition of AI copilots and propose a phase-based taxonomy of
preference optimization strategies, structured around pre-interaction,
mid-interaction, and post-interaction stages. We analyze techniques for
acquiring preference signals, modeling user intent, and integrating feedback
loops, highlighting both established approaches and recent innovations. By
bridging insights from AI personalization, human-AI collaboration, and large
language model adaptation, this survey provides a structured foundation for
designing adaptive, preference-aware AI copilots. It offers a holistic view of
the available preference resources, how they can be leveraged, and which
technical approaches are most suited to each stage of system design.
\\ ( https://arxiv.org/abs/2505.21907 ,  334kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21935
Date: Wed, 28 May 2025 03:40:02 GMT   (866kb,D)

Title: From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule
  Learning with Large Language Models
Authors: Kaiyu He and Zhiyu Chen
Categories: cs.AI
\\
  Since the advent of Large Language Models (LLMs), efforts have largely
focused on improving their instruction-following and deductive reasoning
abilities, leaving open the question of whether these models can truly discover
new knowledge. In pursuit of artificial general intelligence (AGI), there is a
growing need for models that not only execute commands or retrieve information
but also learn, reason, and generate new knowledge by formulating novel
hypotheses and theories that deepen our understanding of the world. Guided by
Peirce's framework of abduction, deduction, and induction, this survey offers a
structured lens to examine LLM-based hypothesis discovery. We synthesize
existing work in hypothesis generation, application, and validation,
identifying both key achievements and critical gaps. By unifying these threads,
we illuminate how LLMs might evolve from mere ``information executors'' into
engines of genuine innovation, potentially transforming research, science, and
real-world problem solving.
\\ ( https://arxiv.org/abs/2505.21935 ,  866kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21988
Date: Wed, 28 May 2025 05:31:49 GMT   (327kb)

Title: Functional Matching of Logic Subgraphs: Beyond Structural Isomorphism
Authors: Ziyang Zheng, Kezhi Li, Zhengyuan Shi, Qiang Xu
Categories: cs.AI
\\
  Subgraph matching in logic circuits is foundational for numerous Electronic
Design Automation (EDA) applications, including datapath optimization,
arithmetic verification, and hardware trojan detection. However, existing
techniques rely primarily on structural graph isomorphism and thus fail to
identify function-related subgraphs when synthesis transformations
substantially alter circuit topology. To overcome this critical limitation, we
introduce the concept of functional subgraph matching, a novel approach that
identifies whether a given logic function is implicitly present within a larger
circuit, irrespective of structural variations induced by synthesis or
technology mapping. Specifically, we propose a two-stage multi-modal framework:
(1) learning robust functional embeddings across AIG and post-mapping netlists
for functional subgraph detection, and (2) identifying fuzzy boundaries using a
graph segmentation approach. Evaluations on standard benchmarks (ITC99,
OpenABCD, ForgeEDA) demonstrate significant performance improvements over
existing structural methods, with average $93.8\%$ accuracy in functional
subgraph detection and a dice score of $91.3\%$ in fuzzy boundary
identification.
\\ ( https://arxiv.org/abs/2505.21988 ,  327kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22006
Date: Wed, 28 May 2025 06:12:51 GMT   (2006kb)

Title: Efficiently Enhancing General Agents With Hierarchical-categorical
  Memory
Authors: Changze Qiao, Mingming Lu
Categories: cs.AI cs.CV
\\
  With large language models (LLMs) demonstrating remarkable capabilities,
there has been a surge in research on leveraging LLMs to build general-purpose
multi-modal agents. However, existing approaches either rely on computationally
expensive end-to-end training using large-scale multi-modal data or adopt
tool-use methods that lack the ability to continuously learn and adapt to new
environments. In this paper, we introduce EHC, a general agent capable of
learning without parameter updates. EHC consists of a Hierarchical Memory
Retrieval (HMR) module and a Task-Category Oriented Experience Learning (TOEL)
module. The HMR module facilitates rapid retrieval of relevant memories and
continuously stores new information without being constrained by memory
capacity. The TOEL module enhances the agent's comprehension of various task
characteristics by classifying experiences and extracting patterns across
different categories. Extensive experiments conducted on multiple standard
datasets demonstrate that EHC outperforms existing methods, achieving
state-of-the-art performance and underscoring its effectiveness as a general
agent for handling complex multi-modal tasks.
\\ ( https://arxiv.org/abs/2505.22006 ,  2006kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22050
Date: Wed, 28 May 2025 07:21:37 GMT   (2082kb,D)

Title: Reinforced Reasoning for Embodied Planning
Authors: Di Wu, Jiaxin Fan, Junzhe Zang, Guanbo Wang, Wei Yin, Wenhao Li, Bo
  Jin
Categories: cs.AI cs.LG
\\
  Embodied planning requires agents to make coherent multi-step decisions based
on dynamic visual observations and natural language goals. While recent
vision-language models (VLMs) excel at static perception tasks, they struggle
with the temporal reasoning, spatial understanding, and commonsense grounding
needed for planning in interactive environments. In this work, we introduce a
reinforcement fine-tuning framework that brings R1-style reasoning enhancement
into embodied planning. We first distill a high-quality dataset from a powerful
closed-source model and perform supervised fine-tuning (SFT) to equip the model
with structured decision-making priors. We then design a rule-based reward
function tailored to multi-step action quality and optimize the policy via
Generalized Reinforced Preference Optimization (GRPO). Our approach is
evaluated on Embench, a recent benchmark for interactive embodied tasks,
covering both in-domain and out-of-domain scenarios. Experimental results show
that our method significantly outperforms models of similar or larger scale,
including GPT-4o-mini and 70B+ open-source baselines, and exhibits strong
generalization to unseen environments. This work highlights the potential of
reinforcement-driven reasoning to advance long-horizon planning in embodied AI.
\\ ( https://arxiv.org/abs/2505.22050 ,  2082kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22087
Date: Wed, 28 May 2025 08:09:06 GMT   (5288kb)

Title: Cognitively-Inspired Emergent Communication via Knowledge Graphs for
  Assisting the Visually Impaired
Authors: Ruxiao Chen, Dezheng Han, Wenjie Han, Shuaishuai Guo
Categories: cs.AI
\\
  Assistive systems for visually impaired individuals must deliver rapid,
interpretable, and adaptive feedback to facilitate real-time navigation.
Current approaches face a trade-off between latency and semantic richness:
natural language-based systems provide detailed guidance but are too slow for
dynamic scenarios, while emergent communication frameworks offer low-latency
symbolic languages but lack semantic depth, limiting their utility in tactile
modalities like vibration. To address these limitations, we introduce a novel
framework, Cognitively-Inspired Emergent Communication via Knowledge Graphs
(VAG-EC), which emulates human visual perception and cognitive mapping. Our
method constructs knowledge graphs to represent objects and their
relationships, incorporating attention mechanisms to prioritize task-relevant
entities, thereby mirroring human selective attention. This structured approach
enables the emergence of compact, interpretable, and context-sensitive symbolic
languages. Extensive experiments across varying vocabulary sizes and message
lengths demonstrate that VAG-EC outperforms traditional emergent communication
methods in Topographic Similarity (TopSim) and Context Independence (CI). These
findings underscore the potential of cognitively grounded emergent
communication as a fast, adaptive, and human-aligned solution for real-time
assistive technologies. Code is available at
https://github.com/Anonymous-NLPcode/Anonymous_submission/tree/main.
\\ ( https://arxiv.org/abs/2505.22087 ,  5288kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22092
Date: Wed, 28 May 2025 08:16:09 GMT   (1267kb)

Title: VIRAL: Vision-grounded Integration for Reward design And Learning
Authors: Valentin Cuzin-Rambaud, Emilien Komlenovic, Alexandre Faure, Bruno Yun
Categories: cs.AI
\\
  The alignment between humans and machines is a critical challenge in
artificial intelligence today. Reinforcement learning, which aims to maximize a
reward function, is particularly vulnerable to the risks associated with poorly
designed reward functions. Recent advancements has shown that Large Language
Models (LLMs) for reward generation can outperform human performance in this
context. We introduce VIRAL, a pipeline for generating and refining reward
functions through the use of multi-modal LLMs. VIRAL autonomously creates and
interactively improves reward functions based on a given environment and a goal
prompt or annotated image. The refinement process can incorporate human
feedback or be guided by a description generated by a video LLM, which explains
the agent's policy in video form. We evaluated VIRAL in five Gymnasium
environments, demonstrating that it accelerates the learning of new behaviors
while ensuring improved alignment with user intent. The source-code and demo
video are available at: https://github.com/VIRAL-UCBL1/VIRAL and
https://youtu.be/t4_BXugBm9Q.
\\ ( https://arxiv.org/abs/2505.22092 ,  1267kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22104
Date: Wed, 28 May 2025 08:30:03 GMT   (79kb)

Title: Efficient Dynamic Shielding for Parametric Safety Specifications
Authors: Davide Corsi, Kaushik Mallik, Andoni Rodriguez, Cesar Sanchez
Categories: cs.AI cs.LG cs.LO cs.RO cs.SY eess.SY
\\
  Shielding has emerged as a promising approach for ensuring safety of
AI-controlled autonomous systems. The algorithmic goal is to compute a shield,
which is a runtime safety enforcement tool that needs to monitor and intervene
the AI controller's actions if safety could be compromised otherwise.
Traditional shields are designed statically for a specific safety requirement.
Therefore, if the safety requirement changes at runtime due to changing
operating conditions, the shield needs to be recomputed from scratch, causing
delays that could be fatal. We introduce dynamic shields for parametric safety
specifications, which are succinctly represented sets of all possible safety
specifications that may be encountered at runtime. Our dynamic shields are
statically designed for a given safety parameter set, and are able to
dynamically adapt as the true safety specification (permissible by the
parameters) is revealed at runtime. The main algorithmic novelty lies in the
dynamic adaptation procedure, which is a simple and fast algorithm that
utilizes known features of standard safety shields, like maximal
permissiveness. We report experimental results for a robot navigation problem
in unknown territories, where the safety specification evolves as new obstacles
are discovered at runtime. In our experiments, the dynamic shields took a few
minutes for their offline design, and took between a fraction of a second and a
few seconds for online adaptation at each step, whereas the brute-force online
recomputation approach was up to 5 times slower.
\\ ( https://arxiv.org/abs/2505.22104 ,  79kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22112
Date: Wed, 28 May 2025 08:40:55 GMT   (9491kb,A)

Title: Visual Large Language Models Exhibit Human-Level Cognitive Flexibility
  in the Wisconsin Card Sorting Test
Authors: Guangfu Hao, Frederic Alexandre, Shan Yu
Categories: cs.AI q-bio.NC
\\
  Cognitive flexibility has been extensively studied in human cognition but
remains relatively unexplored in the context of Visual Large Language Models
(VLLMs). This study assesses the cognitive flexibility of state-of-the-art
VLLMs (GPT-4o, Gemini-1.5 Pro, and Claude-3.5 Sonnet) using the Wisconsin Card
Sorting Test (WCST), a classic measure of set-shifting ability. Our results
reveal that VLLMs achieve or surpass human-level set-shifting capabilities
under chain-of-thought prompting with text-based inputs. However, their
abilities are highly influenced by both input modality and prompting strategy.
In addition, we find that through role-playing, VLLMs can simulate various
functional deficits aligned with patients having impairments in cognitive
flexibility, suggesting that VLLMs may possess a cognitive architecture, at
least regarding the ability of set-shifting, similar to the brain. This study
reveals the fact that VLLMs have already approached the human level on a key
component underlying our higher cognition, and highlights the potential to use
them to emulate complex brain processes.
\\ ( https://arxiv.org/abs/2505.22112 ,  9491kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22147
Date: Wed, 28 May 2025 09:08:27 GMT   (123kb)

Title: Lifted Forward Planning in Relational Factored Markov Decision Processes
  with Concurrent Actions
Authors: Florian Andreas Marwitz, Tanya Braun, Ralf M\"oller, Marcel Gehrke
Categories: cs.AI
\\
  Decision making is a central problem in AI that can be formalized using a
Markov Decision Process. A problem is that, with increasing numbers of
(indistinguishable) objects, the state space grows exponentially. To compute
policies, the state space has to be enumerated. Even more possibilities have to
be enumerated if the size of the action space depends on the size of the state
space, especially if we allow concurrent actions. To tackle the exponential
blow-up in the action and state space, we present a first-order representation
to store the spaces in polynomial instead of exponential size in the number of
objects and introduce Foreplan, a relational forward planner, which uses this
representation to efficiently compute policies for numerous indistinguishable
objects and actions. Additionally, we introduce an even faster approximate
version of Foreplan. Moreover, Foreplan identifies how many objects an agent
should act on to achieve a certain task given restrictions. Further, we provide
a theoretical analysis and an empirical evaluation of Foreplan, demonstrating a
speedup of at least four orders of magnitude.
\\ ( https://arxiv.org/abs/2505.22147 ,  123kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22148
Date: Wed, 28 May 2025 09:12:31 GMT   (4525kb)

Title: What Makes a Good Reasoning Chain? Uncovering Structural Patterns in
  Long Chain-of-Thought Reasoning
Authors: Gangwei Jiang, Yahui Liu, Zhaoyi Li, Qi Wang, Fuzheng Zhang, Linqi
  Song, Ying Wei, Defu Lian
Categories: cs.AI
\\
  Recent advances in reasoning with large language models (LLMs) have
popularized Long Chain-of-Thought (LCoT), a strategy that encourages deliberate
and step-by-step reasoning before producing a final answer. While LCoTs have
enabled expert-level performance in complex tasks, how the internal structures
of their reasoning chains drive, or even predict, the correctness of final
answers remains a critical yet underexplored question. In this work, we present
LCoT2Tree, an automated framework that converts sequential LCoTs into
hierarchical tree structures and thus enables deeper structural analysis of LLM
reasoning. Using graph neural networks (GNNs), we reveal that structural
patterns extracted by LCoT2Tree, including exploration, backtracking, and
verification, serve as stronger predictors of final performance across a wide
range of tasks and models. Leveraging an explainability technique, we further
identify critical thought patterns such as over-branching that account for
failures. Beyond diagnostic insights, the structural patterns by LCoT2Tree
support practical applications, including improving Best-of-N decoding
effectiveness. Overall, our results underscore the critical role of internal
structures of reasoning chains, positioning LCoT2Tree as a powerful tool for
diagnosing, interpreting, and improving reasoning in LLMs.
\\ ( https://arxiv.org/abs/2505.22148 ,  4525kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22244
Date: Wed, 28 May 2025 11:26:14 GMT   (1651kb)

Title: A Preprocessing Framework for Efficient Approximate Bi-Objective
  Shortest-Path Computation in the Presence of Correlated Objectives
Authors: Yaron Halle, Ariel Felner, Sven Koenig and Oren Salzman
Categories: cs.AI
\\
  The bi-objective shortest-path (BOSP) problem seeks to find paths between
start and target vertices of a graph while optimizing two conflicting objective
functions. We consider the BOSP problem in the presence of correlated
objectives. Such correlations often occur in real-world settings such as road
networks, where optimizing two positively correlated objectives, such as travel
time and fuel consumption, is common. BOSP is generally computationally
challenging as the size of the search space is exponential in the number of
objective functions and the graph size. Bounded sub-optimal BOSP solvers such
as A*pex alleviate this complexity by approximating the Pareto-optimal solution
set rather than computing it exactly (given a user-provided approximation
factor). As the correlation between objective functions increases, smaller
approximation factors are sufficient for collapsing the entire Pareto-optimal
set into a single solution. We leverage this insight to propose an efficient
algorithm that reduces the search effort in the presence of correlated
objectives. Our approach for computing approximations of the entire
Pareto-optimal set is inspired by graph-clustering algorithms. It uses a
preprocessing phase to identify correlated clusters within a graph and to
generate a new graph representation. This allows a natural generalization of
A*pex to run up to five times faster on DIMACS dataset instances, a standard
benchmark in the field. To the best of our knowledge, this is the first
algorithm proposed that efficiently and effectively exploits correlations in
the context of bi-objective search while providing theoretical guarantees on
solution quality.
\\ ( https://arxiv.org/abs/2505.22244 ,  1651kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22288
Date: Wed, 28 May 2025 12:27:32 GMT   (71kb)

Title: Compression versus Accuracy: A Hierarchy of Lifted Models
Authors: Jan Speller, Malte Luttermann, Marcel Gehrke, Tanya Braun
Categories: cs.AI
\\
  Probabilistic graphical models that encode indistinguishable objects and
relations among them use first-order logic constructs to compress a
propositional factorised model for more efficient (lifted) inference. To obtain
a lifted representation, the state-of-the-art algorithm Advanced Colour Passing
(ACP) groups factors that represent matching distributions. In an approximate
version using $\varepsilon$ as a hyperparameter, factors are grouped that
differ by a factor of at most $(1\pm \varepsilon)$. However, finding a suitable
$\varepsilon$ is not obvious and may need a lot of exploration, possibly
requiring many ACP runs with different $\varepsilon$ values. Additionally,
varying $\varepsilon$ can yield wildly different models, leading to decreased
interpretability. Therefore, this paper presents a hierarchical approach to
lifted model construction that is hyperparameter-free. It efficiently computes
a hierarchy of $\varepsilon$ values that ensures a hierarchy of models, meaning
that once factors are grouped together given some $\varepsilon$, these factors
will be grouped together for larger $\varepsilon$ as well. The hierarchy of
$\varepsilon$ values also leads to a hierarchy of error bounds. This allows for
explicitly weighing compression versus accuracy when choosing specific
$\varepsilon$ values to run ACP with and enables interpretability between the
different models.
\\ ( https://arxiv.org/abs/2505.22288 ,  71kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22290
Date: Wed, 28 May 2025 12:28:18 GMT   (735kb)

Title: Rethinking the Unsolvable: When In-Context Search Meets Test-Time
  Scaling
Authors: Fanzeng Xia, Yidong Luo, Tinko Sebastian Bartels, Yaqi Xu, and Tongxin
  Li
Categories: cs.AI cs.CL cs.LG
\\
  Recent research has highlighted that Large Language Models (LLMs), even when
trained to generate extended long reasoning steps, still face significant
challenges on hard reasoning problems. However, much of the existing literature
relies on direct prompting with simple in-context learning examples for
evaluation, which largely overlooks advanced techniques to elicit LLMs'
deliberate reasoning before drawing conclusions that LLMs hit a performance
ceiling. In this paper, we systematically explore the combined potential of
in-context search and test-time scaling on super hard reasoning tasks. We find
that by employing advanced in-context search prompting to LLMs augmented with
internal scaling, one can achieve transformative performance breakthroughs on
tasks previously deemed "unsolvable" (e.g., reported success rates below 5%).
We provide both empirical results and theoretical analysis of how this
combination can unleash LLM reasoning capabilities: i) Empirically, on
controlled NP-hard tasks and complex real-world planning benchmarks, our
approach achieves up to a 30x improvement in success rates compared to
previously reported results without any external mechanisms; ii) Theoretically,
we show that in-context search prompting, when combined with internal scaling,
significantly extends the complexity class of solvable reasoning problems.
These findings challenge prevailing assumptions about the limitations of LLMs
on complex tasks, indicating that current evaluation paradigms systematically
underestimate their true potential. Our work calls for a critical reassessment
of how LLM reasoning is benchmarked and a more robust evaluation strategy that
fully captures the true capabilities of contemporary LLMs, which can lead to a
better understanding of their operational reasoning boundaries in real-world
deployments.
\\ ( https://arxiv.org/abs/2505.22290 ,  735kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22311
Date: Wed, 28 May 2025 12:54:07 GMT   (6274kb)

Title: From Large AI Models to Agentic AI: A Tutorial on Future Intelligent
  Communications
Authors: Feibo Jiang, Cunhua Pan, Li Dong, Kezhi Wang, Octavia A. Dobre, and
  Merouane Debbah
Categories: cs.AI cs.CY cs.NI eess.SP
\\
  With the advent of 6G communications, intelligent communication systems face
multiple challenges, including constrained perception and response
capabilities, limited scalability, and low adaptability in dynamic
environments. This tutorial provides a systematic introduction to the
principles, design, and applications of Large Artificial Intelligence Models
(LAMs) and Agentic AI technologies in intelligent communication systems, aiming
to offer researchers a comprehensive overview of cutting-edge technologies and
practical guidance. First, we outline the background of 6G communications,
review the technological evolution from LAMs to Agentic AI, and clarify the
tutorial's motivation and main contributions. Subsequently, we present a
comprehensive review of the key components required for constructing LAMs. We
further categorize LAMs and analyze their applicability, covering Large
Language Models (LLMs), Large Vision Models (LVMs), Large Multimodal Models
(LMMs), Large Reasoning Models (LRMs), and lightweight LAMs. Next, we propose a
LAM-centric design paradigm tailored for communications, encompassing dataset
construction and both internal and external learning approaches. Building upon
this, we develop an LAM-based Agentic AI system for intelligent communications,
clarifying its core components such as planners, knowledge bases, tools, and
memory modules, as well as its interaction mechanisms. We also introduce a
multi-agent framework with data retrieval, collaborative planning, and
reflective evaluation for 6G. Subsequently, we provide a detailed overview of
the applications of LAMs and Agentic AI in communication scenarios. Finally, we
summarize the research challenges and future directions in current studies,
aiming to support the development of efficient, secure, and sustainable
next-generation intelligent communication systems.
\\ ( https://arxiv.org/abs/2505.22311 ,  6274kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22368
Date: Wed, 28 May 2025 13:56:22 GMT   (762kb)

Title: AgentDNS: A Root Domain Naming System for LLM Agents
Authors: Enfang Cui, Yujun Cheng, Rui She, Dan Liu, Zhiyuan Liang, Minxin Guo,
  Tianzheng Li, Qian Wei, Wenjuan Xing, Zhijie Zhong
Categories: cs.AI
Comments: 7 pages, 6 figures
\\
  The rapid evolution of Large Language Model (LLM) agents has highlighted
critical challenges in cross-vendor service discovery, interoperability, and
communication. Existing protocols like model context protocol and
agent-to-agent protocol have made significant strides in standardizing
interoperability between agents and tools, as well as communication among
multi-agents. However, there remains a lack of standardized protocols and
solutions for service discovery across different agent and tool vendors. In
this paper, we propose AgentDNS, a root domain naming and service discovery
system designed to enable LLM agents to autonomously discover, resolve, and
securely invoke third-party agent and tool services across organizational and
technological boundaries. Inspired by the principles of the traditional DNS,
AgentDNS introduces a structured mechanism for service registration, semantic
service discovery, secure invocation, and unified billing. We detail the
architecture, core functionalities, and use cases of AgentDNS, demonstrating
its potential to streamline multi-agent collaboration in real-world scenarios.
The source code will be published on https://github.com/agentdns.
\\ ( https://arxiv.org/abs/2505.22368 ,  762kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22451
Date: Wed, 28 May 2025 15:10:37 GMT   (123kb)

Title: AI Mathematician: Towards Fully Automated Frontier Mathematical Research
Authors: Yuanhang Liu, Yanxing Huang, Yanqiao Wang, Peng Li, Yang Liu
Categories: cs.AI
Comments: 95 pages, 1 figure
\\
  Large Reasoning Models (LRMs) have made significant progress in mathematical
capabilities in recent times. However, these successes have been primarily
confined to competition-level problems. In this work, we propose AI
Mathematician (AIM) framework, which harnesses the reasoning strength of LRMs
to support frontier mathematical research. We have identified two critical
challenges of mathematical research compared to competition, {\it the intrinsic
complexity of research problems} and {\it the requirement of procedural rigor}.
To address these challenges, AIM incorporates two core strategies: an
exploration mechanism to foster longer solution paths, and the pessimistic
reasonable verification method to ensure reliability.
  This early version of AIM already exhibits strong capability in tackling
research-level tasks. We conducted extensive experiments across several
real-world mathematical topics and obtained promising results. AIM is able to
autonomously construct substantial portions of proofs and uncover non-trivial
insights within each research area. These findings highlight the potential of
LRMs in mathematical discovery and suggest that LRM-based agent systems could
significantly accelerate mathematical research in the future.
\\ ( https://arxiv.org/abs/2505.22451 ,  123kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22597
Date: Wed, 28 May 2025 17:10:43 GMT   (750kb)

Title: HDDLGym: A Tool for Studying Multi-Agent Hierarchical Problems Defined
  in HDDL with OpenAI Gym
Authors: Ngoc La, Ruaridh Mon-Williams, and Julie A. Shah
Categories: cs.AI cs.LG cs.MA
Comments: Accepted to Proceedings of ICAPS 2025
\\
  In recent years, reinforcement learning (RL) methods have been widely tested
using tools like OpenAI Gym, though many tasks in these environments could also
benefit from hierarchical planning. However, there is a lack of a tool that
enables seamless integration of hierarchical planning with RL. Hierarchical
Domain Definition Language (HDDL), used in classical planning, introduces a
structured approach well-suited for model-based RL to address this gap. To
bridge this integration, we introduce HDDLGym, a Python-based tool that
automatically generates OpenAI Gym environments from HDDL domains and problems.
HDDLGym serves as a link between RL and hierarchical planning, supporting
multi-agent scenarios and enabling collaborative planning among agents. This
paper provides an overview of HDDLGym's design and implementation, highlighting
the challenges and design choices involved in integrating HDDL with the Gym
interface, and applying RL policies to support hierarchical planning. We also
provide detailed instructions and demonstrations for using the HDDLGym
framework, including how to work with existing HDDL domains and problems from
International Planning Competitions, exemplified by the Transport domain.
Additionally, we offer guidance on creating new HDDL domains for multi-agent
scenarios and demonstrate the practical use of HDDLGym in the Overcooked
domain. By leveraging the advantages of HDDL and Gym, HDDLGym aims to be a
valuable tool for studying RL in hierarchical planning, particularly in
multi-agent contexts.
\\ ( https://arxiv.org/abs/2505.22597 ,  750kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21517
Date: Thu, 22 May 2025 04:09:07 GMT   (448kb)

Title: Cold Start Problem: An Experimental Study of Knowledge Tracing Models
  with New Students
Authors: Indronil Bhattacharjee and Christabel Wayllace
Categories: cs.CY
Comments: 26th International Conference on Artificial Intelligence in Education
  (AIED 2025)
\\
  KnowledgeTracing (KT) involves predicting students' knowledge states based on
their interactions with Intelligent Tutoring Systems (ITS). A key challenge is
the cold start problem, accurately predicting knowledge for new students with
minimal interaction data. Unlike prior work, which typically trains KT models
on initial interactions of all students and tests on their subsequent
interactions, our approach trains models solely using historical data from past
students, evaluating their performance exclusively on entirely new students. We
investigate cold start effects across three KT models: Deep Knowledge Tracing
(DKT), Dynamic Key-Value Memory Networks (DKVMN), and Self-Attentive Knowledge
Tracing (SAKT), using ASSISTments 2009, 2015, and 2017 datasets. Results
indicate all models initially struggle under cold start conditions but
progressively improve with more interactions; SAKT shows higher initial
accuracy yet still faces limitations. These findings highlight the need for KT
models that effectively generalize to new learners, emphasizing the importance
of developing models robust in few-shot and zero-shot learning scenarios
\\ ( https://arxiv.org/abs/2505.21517 ,  448kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21536
Date: Sat, 24 May 2025 08:26:14 GMT   (220kb,D)

Title: CiRL: Open-Source Environments for Reinforcement Learning in Circular
  Economy and Net Zero
Authors: Federico Zocco, Andrea Corti and Monica Malvezzi
Categories: cs.CY cs.CE cs.LG
Comments: To be submitted
\\
  The demand of finite raw materials will keep increasing as they fuel modern
society. Simultaneously, solutions for stopping carbon emissions in the short
term are not available, thus making the net zero target extremely challenging
to achieve at scale. The circular economy (CE) paradigm is gaining attention as
a solution to address climate change and the uncertainties of supplies of
critical materials. Hence, in this paper, we introduce CiRL, a deep
reinforcement learning (DRL) library of environments focused on the circularity
of both solid and fluid materials. The integration of DRL into the design of
material circularity is possible thanks to the formalism of thermodynamical
material networks, which is underpinned by compartmental dynamical
thermodynamics. Along with the focus on circularity, this library has three
more features: the new CE-oriented environments are in the state-space form,
which is typically used in dynamical systems analysis and control designs; it
is based on a state-of-the-art Python library of DRL algorithms, namely,
Stable-Baselines3; and it is developed in Google Colaboratory to be accessible
to researchers from different disciplines and backgrounds as is often the case
for circular economy researchers and engineers. CiRL is publicly available.
\\ ( https://arxiv.org/abs/2505.21536 ,  220kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21537
Date: Sat, 24 May 2025 09:07:13 GMT   (518kb)

Title: OpenReview Should be Protected and Leveraged as a Community Asset for
  Research in the Era of Large Language Models
Authors: Hao Sun, Yunyi Shen, Mihaela van der Schaar
Categories: cs.CY cs.AI
\\
  In the era of large language models (LLMs), high-quality, domain-rich, and
continuously evolving datasets capturing expert-level knowledge, core human
values, and reasoning are increasingly valuable. This position paper argues
that OpenReview -- the continually evolving repository of research papers, peer
reviews, author rebuttals, meta-reviews, and decision outcomes -- should be
leveraged more broadly as a core community asset for advancing research in the
era of LLMs. We highlight three promising areas in which OpenReview can
uniquely contribute: enhancing the quality, scalability, and accountability of
peer review processes; enabling meaningful, open-ended benchmarks rooted in
genuine expert deliberation; and supporting alignment research through
real-world interactions reflecting expert assessment, intentions, and
scientific values. To better realize these opportunities, we suggest the
community collaboratively explore standardized benchmarks and usage guidelines
around OpenReview, inviting broader dialogue on responsible data use, ethical
considerations, and collective stewardship.
\\ ( https://arxiv.org/abs/2505.21537 ,  518kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21542
Date: Sat, 24 May 2025 17:31:55 GMT   (202kb)

Title: Toward a Cultural Co-Genesis of AI Ethics
Authors: Ammar Younas
Categories: cs.CY
\\
  Contemporary discussions in AI ethics often treat culture as a source of
normative divergence that needs to be accommodated, tolerated, or managed due
to its resistance to universal standards. This paper offers an alternative
vision through the concept of "Cultural Co-Genesis of AI Ethics." Rather than
viewing culture as a boundary or container of isolated moral systems, we argue
that it is a generative space for ethical co-production. In this framework,
ethical values emerge through intercultural engagement, dialogical encounters,
mutual recognition, and shared moral inquiry.
  This approach resists both universalist imposition and relativistic
fragmentation. Cultures are not approached as absolutes to be defended or
dissolved, but as co-authors of a dynamic ethical landscape. By grounding AI
ethics in Cultural Co-Genesis, we move from managing difference to constructing
shared ethical meaning for AI ethics, with culture as a partner, not a problem.
  We support this framework with two cases: (1) a theoretical analysis of how
various cultures interpret the emergence of powerful new species, challenging
dominant existential risk narratives, and (2) an empirical study of global AI
ethics principles using data from the Linking AI Principles project, which
reveals deep ethical convergence despite cultural diversity. We conclude that
cross-cultural AI ethics should be seen not as an ethical patchwork, but as a
mosaic in progress, woven from the normative insights that emerge between
cultures.
\\ ( https://arxiv.org/abs/2505.21542 ,  202kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21562
Date: Tue, 27 May 2025 02:23:03 GMT   (1929kb)

Title: Enhancing Selection of Climate Tech Startups with AI -- A Case Study on
  Integrating Human and AI Evaluations in the ClimaTech Great Global Innovation
  Challenge
Authors: Jennifer Turliuk, Alejandro Sevilla, Daniela Gorza, Tod Hynes
Categories: cs.CY cs.AI cs.HC
\\
  This case study examines the ClimaTech Great Global Innovation Challenge's
approach to selecting climate tech startups by integrating human and AI
evaluations. The competition aimed to identify top startups and enhance the
accuracy and efficiency of the selection process through a hybrid model.
Research shows data-driven approaches help VC firms reduce bias and improve
decision-making. Machine learning models have outperformed human investors in
deal screening, helping identify high-potential startups. Incorporating AI
aimed to ensure more equitable and objective evaluations.
  The methodology included three phases: initial AI review, semi-finals judged
by humans, and finals using a hybrid weighting. In phase one, 57 applications
were scored by an AI tool built with StackAI and OpenAI's GPT-4o, and the top
36 advanced. In the semi-finals, human judges, unaware of AI scores, evaluated
startups on team quality, market potential, and technological innovation. Each
score - human or AI - was weighted equally, resulting in 75 percent human and
25 percent AI influence. In the finals, with five human judges, weighting
shifted to 83.3 percent human and 16.7 percent AI. There was a moderate
positive correlation between AI and human scores - Spearman's = 0.47 -
indicating general alignment with key differences. Notably, the final four
startups, selected mainly by humans, were among those rated highest by the AI.
This highlights the complementary nature of AI and human judgment. The study
shows that hybrid models can streamline and improve startup assessments. The
ClimaTech approach offers a strong framework for future competitions by
combining human expertise with AI capabilities.
\\ ( https://arxiv.org/abs/2505.21562 ,  1929kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21570
Date: Tue, 27 May 2025 06:42:41 GMT   (26kb)

Title: Beyond Explainability: The Case for AI Validation
Authors: Dalit Ken-Dror Feldman and Daniel Benoliel
Categories: cs.CY cs.AI
\\
  Artificial Knowledge (AK) systems are transforming decision-making across
critical domains such as healthcare, finance, and criminal justice. However,
their growing opacity presents governance challenges that current regulatory
approaches, focused predominantly on explainability, fail to address
adequately. This article argues for a shift toward validation as a central
regulatory pillar. Validation, ensuring the reliability, consistency, and
robustness of AI outputs, offers a more practical, scalable, and risk-sensitive
alternative to explainability, particularly in high-stakes contexts where
interpretability may be technically or economically unfeasible. We introduce a
typology based on two axes, validity and explainability, classifying AK systems
into four categories and exposing the trade-offs between interpretability and
output reliability. Drawing on comparative analysis of regulatory approaches in
the EU, US, UK, and China, we show how validation can enhance societal trust,
fairness, and safety even where explainability is limited. We propose a
forward-looking policy framework centered on pre- and post-deployment
validation, third-party auditing, harmonized standards, and liability
incentives. This framework balances innovation with accountability and provides
a governance roadmap for responsibly integrating opaque, high-performing AK
systems into society.
\\ ( https://arxiv.org/abs/2505.21570 ,  26kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21582
Date: Tue, 27 May 2025 10:07:05 GMT   (4512kb,D)

Title: AITEE -- Agentic Tutor for Electrical Engineering
Authors: Christopher Knievel and Alexander Bernhardt and Christian Bernhardt
Categories: cs.CY cs.AI cs.HC
Comments: 12 pages, 11 figures, 6 tables
\\
  Intelligent tutoring systems combined with large language models offer a
promising approach to address students' diverse needs and promote
self-efficacious learning. While large language models possess good
foundational knowledge of electrical engineering basics, they remain
insufficiently capable of addressing specific questions about electrical
circuits. In this paper, we present AITEE, an agent-based tutoring system for
electrical engineering designed to accompany students throughout their learning
process, offer individualized support, and promote self-directed learning.
AITEE supports both hand-drawn and digital circuits through an adapted circuit
reconstruction process, enabling natural interaction with students. Our novel
graph-based similarity measure identifies relevant context from lecture
materials through a retrieval augmented generation approach, while parallel
Spice simulation further enhances accuracy in applying solution methodologies.
The system implements a Socratic dialogue to foster learner autonomy through
guided questioning. Experimental evaluations demonstrate that AITEE
significantly outperforms baseline approaches in domain-specific knowledge
application, with even medium-sized LLM models showing acceptable performance.
Our results highlight the potential of agentic tutors to deliver scalable,
personalized, and effective learning environments for electrical engineering
education.
\\ ( https://arxiv.org/abs/2505.21582 ,  4512kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21590
Date: Tue, 27 May 2025 13:06:38 GMT   (1882kb)

Title: Computational Reproducibility of R Code Supplements on OSF
Authors: Lorraine Saju, Tobias Holtdirk, Meetkumar Pravinbhai Mangroliya, Arnim
  Bleier
Categories: cs.CY cs.SE
\\
  Computational reproducibility is fundamental to scientific research, yet many
published code supplements lack the necessary documentation to recreate their
computational environments. While researchers increasingly share code alongside
publications, the actual reproducibility of these materials remains poorly
understood.
  In this work, we assess the computational reproducibility of 296 R projects
using the StatCodeSearch dataset. Of these, only 264 were still retrievable,
and 98.8% lacked formal dependency descriptions required for successful
execution. To address this, we developed an automated pipeline that
reconstructs computational environments directly from project source code.
Applying this pipeline, we executed the R scripts within custom Docker
containers and found that 25.87% completed successfully without error.
  We conducted a detailed analysis of execution failures, identifying
reproducibility barriers such as undeclared dependencies, invalid file paths,
and system-level issues. Our findings show that automated dependency inference
and containerisation can support scalable verification of computational
reproducibility and help identify practical obstacles to code reuse and
transparency in scientific research.
\\ ( https://arxiv.org/abs/2505.21590 ,  1882kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21604
Date: Tue, 27 May 2025 17:46:22 GMT   (726kb)

Title: Public Discourse Sandbox: Facilitating Human and AI Digital
  Communication Research
Authors: Kristina Radivojevic, Caleb Reinking, Shaun Whitfield, Paul Brenner
Categories: cs.CY cs.AI cs.HC
\\
  Social media serves as a primary communication and information dissemination
platform for major global events, entertainment, and niche or topically focused
community discussions. Therefore, it represents a valuable resource for
researchers who aim to understand numerous questions. However, obtaining data
can be difficult, expensive, and often unreliable due to the presence of bots,
fake accounts, and manipulated content. Additionally, there are ethical
concerns if researchers decide to conduct an online experiment without
explicitly notifying social media users about their intent. There is a need for
more controlled and scalable mechanisms to evaluate the impacts of digital
discussion interventions on audiences. We introduce the Public Discourse
Sandbox (PDS), which serves as a digital discourse research platform for
human-AI as well as AI-AI discourse research, testing, and training. PDS
provides a safe and secure space for research experiments that are not viable
on public, commercial social media platforms. Its main purpose is to enable the
understanding of AI behaviors and the impacts of customized AI participants via
techniques such as prompt engineering, retrieval-augmented generation (RAG),
and fine-tuning. We provide a hosted live version of the sandbox to support
researchers as well as the open-sourced code on GitHub for community
collaboration and contribution.
\\ ( https://arxiv.org/abs/2505.21604 ,  726kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21664
Date: Tue, 27 May 2025 18:44:30 GMT   (4076kb)

Title: Expert Survey: AI Reliability & Security Research Priorities
Authors: Joe O'Brien, Jeremy Dolan, Jay Kim, Jonah Dykhuizen, Jeba Sania,
  Sebastian Becker, Jam Kraprayoon, Cara Labrador
Categories: cs.CY cs.AI
\\
  Our survey of 53 specialists across 105 AI reliability and security research
areas identifies the most promising research prospects to guide strategic AI
R&D investment. As companies are seeking to develop AI systems with broadly
human-level capabilities, research on reliability and security is urgently
needed to ensure AI's benefits can be safely and broadly realized and prevent
severe harms. This study is the first to quantify expert priorities across a
comprehensive taxonomy of AI safety and security research directions and to
produce a data-driven ranking of their potential impact. These rankings may
support evidence-based decisions about how to effectively deploy resources
toward AI reliability and security research.
\\ ( https://arxiv.org/abs/2505.21664 ,  4076kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21682
Date: Tue, 27 May 2025 19:08:07 GMT   (129kb)

Title: Data and Technology for Equitable Public Administration: Understanding
  City Government Employees' Challenges and Needs
Authors: Angie Zhang, Madison Liao, Elizaveta (Lee) Kravchenko, Marshanah
  Taylor, Angela Haddad, Chandra Bhat, S. Craig Watkins, Min Kyung Lee
Categories: cs.CY cs.HC
Comments: Accepted to ACM CSCW 2025
\\
  City governments in the United States are increasingly pressured to adopt
emerging technologies. Yet, these systems often risk biased and disparate
outcomes. Scholars studying public sector technology design have converged on
the need to ground these systems in the goals and organizational contexts of
employees using them. We expand our understanding of employees' contexts by
focusing on the equity practices of city government employees to surface
important equity considerations around public sector data and technology use.
Through semi-structured interviews with thirty-six employees from ten
departments of a U.S. city government, our findings reveal challenges employees
face when operationalizing equity, perspectives on data needs for advancing
equity goals, and the design space for acceptable government technology. We
discuss what it looks like to foreground equity in data use and technology
design, and considerations for how to support city government employees in
operationalizing equity with and without official equity offices.
\\ ( https://arxiv.org/abs/2505.21682 ,  129kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21696
Date: Tue, 27 May 2025 19:32:51 GMT   (165kb)

Title: How Soft Skills Shape First-Year Success in Higher Education
Authors: Kerstin Andree, Santiago Berrezueta-Guzman, Stephan Krusche, Luise
  Pufahl, Stefan Wagner
Categories: cs.CY cs.IT math.IT
Comments: This is the authors' preprint version of a paper accepted at the IEEE
  German Education Conference GECon 2025, to be held July 21 to 23, 2025, in
  Hamburg, Germany. The final published version will be available via IEEE
  Xplore Library
\\
  Soft skills are critical for academic and professional success, but are often
neglected in early-stage technical curricula. This paper presents a
semi-isolated teaching intervention aimed at fostering study ability and key
soft skills-communication, collaboration, and project management-among
first-year computer science students. The elective seminar Soft Skills and
Tools for Studies and Career in IT was alongside a mandatory team-based
programming course. We analyze project outcomes and student experiences across
three cohorts across three groups: students who attended the seminar, students
who teamed up with a seminar attendee, and students with no exposure to the
seminar.
  Results show that seminar participants performed significantly better in
individual presentations and team projects. Qualitative feedback further
indicates improved team dynamics and study preparedness. Although self-assessed
collaboration and communication did not reach statistical significance,
consistent trends suggest that early soft skills training enhances academic
integration. We recommend embedding such interventions early in technical study
programs to support the transition into university life.
\\ ( https://arxiv.org/abs/2505.21696 ,  165kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21704
Date: Tue, 27 May 2025 19:43:06 GMT   (760kb)

Title: Lecturers' perspectives on the integration of research data management
  into teacher training programmes
Authors: Sandra Schulz and Juliane Jacob
Categories: cs.CY
\\
  This article focuses on how data literacy education such as research data
management skills can be integrated into teacher training programmes in order
to adequately train the teachers of tomorrow. To this end, interviews were
conducted with three lecturers from the Faculty of Education and analysed both
qualitatively and quantitatively. The lecturers describe the topic of research
data management as extremely relevant for students, especially in the Master's
program. Even as future teachers, for example in computer science and the
natural sciences, students will have a lot to do with data and need to be able
to handle it competently. The article also discusses how research data
management skills can be integrated into the teacher training program.
\\ ( https://arxiv.org/abs/2505.21704 ,  760kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21720
Date: Tue, 27 May 2025 20:07:22 GMT   (255kb)

Title: Responsible Data Stewardship: Generative AI and the Digital Waste
  Problem
Authors: Vanessa Utz
Categories: cs.CY cs.AI
Comments: 8 pages, submitted to AAAI/ACM Conference on AI, Ethics and Society
\\
  As generative AI systems become widely adopted, they enable unprecedented
creation levels of synthetic data across text, images, audio, and video
modalities. While research has addressed the energy consumption of model
training and inference, a critical sustainability challenge remains
understudied: digital waste. This term refers to stored data that consumes
resources without serving a specific (and/or immediate) purpose. This paper
presents this terminology in the AI context and introduces digital waste as an
ethical imperative within (generative) AI development, positioning
environmental sustainability as core for responsible innovation. Drawing from
established digital resource management approaches, we examine how other
disciplines manage digital waste and identify transferable approaches for the
AI community. We propose specific recommendations encompassing re-search
directions, technical interventions, and cultural shifts to mitigate the
environmental consequences of in-definite data storage. By expanding AI ethics
beyond immediate concerns like bias and privacy to include inter-generational
environmental justice, this work contributes to a more comprehensive ethical
framework that considers the complete lifecycle impact of generative AI
systems.
\\ ( https://arxiv.org/abs/2505.21720 ,  255kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21744
Date: Tue, 27 May 2025 20:33:37 GMT   (389kb)

Title: Computocene: Notes from an Age of Observation
Authors: Simone Severini
Categories: cs.CY
Comments: 16 pages
\\
  This piece plays with the idea of the Computocene: an era defined not merely
by the ubiquity of computers, but by their deepening role in how we observe,
interpret, and make sense of the world. Rather than emphasizing automation,
speed, scale, or intelligence, computation is reframed as a mode of attention:
filtering information, guiding inquiry, reframing questions, and shaping the
very conditions under which knowledge emerges. I invite the reader to consider
computers not simply as tools of calculation, but as epistemic instruments that
participate in the formation of knowledge. This perspective reconfigures not
only scientific practice but the epistemological foundations of understanding
itself. The Computocene thus names a shift: from computation as calculation to
computation as a form of attunement to the world. It is a speculative essay,
offered without technical formality, and intended for a general, curious
readership.
\\ ( https://arxiv.org/abs/2505.21744 ,  389kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21752
Date: Tue, 27 May 2025 20:40:18 GMT   (3465kb)

Title: Experimental Evidence That AI-Managed Workers Tolerate Lower Pay Without
  Demotivation
Authors: Mengchen Dong, Levin Brinkmann, Omar Sherif, Shihan Wang, Xinyu Zhang,
  Jean-Fran\c{c}ois Bonnefon, Iyad Rahwan
Categories: cs.CY cs.HC
\\
  Experimental evidence on worker responses to AI management remains mixed,
partly due to limitations in experimental fidelity. We address these
limitations with a customized workplace in the Minecraft platform, enabling
high-resolution behavioral tracking of autonomous task execution, and ensuring
that participants approach the task with well-formed expectations about their
own competence. Workers (N = 382) completed repeated production tasks under
either human, AI, or hybrid management. An AI manager trained on human-defined
evaluation principles systematically assigned lower performance ratings and
reduced wages by 40\%, without adverse effects on worker motivation and sense
of fairness. These effects were driven by a muted emotional response to AI
evaluation, compared to evaluation by a human. The very features that make AI
appear impartial may also facilitate silent exploitation, by suppressing the
social reactions that normally constrain extractive practices in human-managed
work.
\\ ( https://arxiv.org/abs/2505.21752 ,  3465kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21753
Date: Tue, 27 May 2025 20:41:30 GMT   (478kb)

Title: From prosthetic memory to prosthetic denial: Auditing whether large
  language models are prone to mass atrocity denialism
Authors: Roberto Ulloa, Eve M. Zucker, Daniel Bultmann, David J. Simon, Mykola
  Makhortykh
Categories: cs.CY cs.CL
\\
  The proliferation of large language models (LLMs) can influence how
historical narratives are disseminated and perceived. This study explores the
implications of LLMs' responses on the representation of mass atrocity memory,
examining whether generative AI systems contribute to prosthetic memory, i.e.,
mediated experiences of historical events, or to what we term "prosthetic
denial," the AI-mediated erasure or distortion of atrocity memories. We argue
that LLMs function as interfaces that can elicit prosthetic memories and,
therefore, act as experiential sites for memory transmission, but also
introduce risks of denialism, particularly when their outputs align with
contested or revisionist narratives. To empirically assess these risks, we
conducted a comparative audit of five LLMs (Claude, GPT, Llama, Mixtral, and
Gemini) across four historical case studies: the Holodomor, the Holocaust, the
Cambodian Genocide, and the genocide against the Tutsis in Rwanda. Each model
was prompted with questions addressing common denialist claims in English and
an alternative language relevant to each case (Ukrainian, German, Khmer, and
French). Our findings reveal that while LLMs generally produce accurate
responses for widely documented events like the Holocaust, significant
inconsistencies and susceptibility to denialist framings are observed for more
underrepresented cases like the Cambodian Genocide. The disparities highlight
the influence of training data availability and the probabilistic nature of LLM
responses on memory integrity. We conclude that while LLMs extend the concept
of prosthetic memory, their unmoderated use risks reinforcing historical
denialism, raising ethical concerns for (digital) memory preservation, and
potentially challenging the advantageous role of technology associated with the
original values of prosthetic memory.
\\ ( https://arxiv.org/abs/2505.21753 ,  478kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21808
Date: Tue, 27 May 2025 22:26:51 GMT   (2346kb)

Title: AI Agent Governance: A Field Guide
Authors: Jam Kraprayoon, Zoe Williams, Rida Fayyaz
Categories: cs.CY
\\
  This report serves as an accessible guide to the emerging field of AI agent
governance. Agents - AI systems that can autonomously achieve goals in the
world, with little to no explicit human instruction about how to do so - are a
major focus of leading tech companies, AI start-ups, and investors. If these
development efforts are successful, some industry leaders claim we could soon
see a world where millions or billions of agents autonomously perform complex
tasks across society. Society is largely unprepared for this development. A
future where capable agents are deployed en masse could see transformative
benefits to society but also profound and novel risks. Currently, the
exploration of agent governance questions and the development of associated
interventions remain in their infancy. Only a few researchers, primarily in
civil society organizations, public research institutes, and frontier AI
companies, are actively working on these challenges.
\\ ( https://arxiv.org/abs/2505.21808 ,  2346kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21912
Date: Wed, 28 May 2025 02:58:41 GMT   (23944kb,D)

Title: Detecting Cultural Differences in News Video Thumbnails via
  Computational Aesthetics
Authors: Marvin Limpijankit, John Kender
Categories: cs.CY cs.CV
DOI: 10.36190/2024.61
\\
  We propose a two-step approach for detecting differences in the style of
images across sources of differing cultural affinity, where images are first
clustered into finer visual themes based on content before their aesthetic
features are compared. We test this approach on 2,400 YouTube video thumbnails
taken equally from two U.S. and two Chinese YouTube channels, and relating
equally to COVID-19 and the Ukraine conflict. Our results suggest that while
Chinese thumbnails are less formal and more candid, U.S. channels tend to use
more deliberate, proper photographs as thumbnails. In particular, U.S.
thumbnails are less colorful, more saturated, darker, more finely detailed,
less symmetric, sparser, less varied, and more up close and personal than
Chinese thumbnails. We suggest that most of these differences reflect cultural
preferences, and that our methods and observations can serve as a baseline
against which suspected visual propaganda can be computed and compared.
\\ ( https://arxiv.org/abs/2505.21912 ,  23944kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22073
Date: Wed, 28 May 2025 07:54:06 GMT   (233kb)

Title: A Closer Look at the Existing Risks of Generative AI: Mapping the Who,
  What, and How of Real-World Incidents
Authors: Megan Li, Wendy Bickersteth, Ningjing Tang, Jason Hong, Lorrie Cranor,
  Hong Shen, Hoda Heidari
Categories: cs.CY
Comments: 21 pages including references and appendices, 5 figures, submitted to
  Conference on AI, Ethics, and Society (AIES 2025)
\\
  Due to its general-purpose nature, Generative AI is applied in an
ever-growing set of domains and tasks, leading to an expanding set of risks of
harm impacting people, communities, society, and the environment. These risks
may arise due to failures during the design and development of the technology,
as well as during its release, deployment, or downstream usages and
appropriations of its outputs. In this paper, building on prior taxonomies of
AI risks, harms, and failures, we construct a taxonomy specifically for
Generative AI failures and map them to the harms they precipitate. Through a
systematic analysis of 499 publicly reported incidents, we describe what harms
are reported, how they arose, and who they impact. We report the prevalence of
each type of harm, underlying failure mode, and harmed stakeholder, as well as
their common co-occurrences. We find that most reported incidents are caused by
use-related issues but bring harm to parties beyond the end user(s) of the
Generative AI system at fault, and that the landscape of Generative AI harms is
distinct from that of traditional AI. Our work offers actionable insights to
policymakers, developers, and Generative AI users. In particular, we call for
the prioritization of non-technical risk and harm mitigation strategies,
including public disclosures and education and careful regulatory stances.
\\ ( https://arxiv.org/abs/2505.22073 ,  233kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22093
Date: Wed, 28 May 2025 08:17:05 GMT   (152kb)

Title: From Coders to Critics: Empowering Students through Peer Assessment in
  the Age of AI Copilots
Authors: Santiago Berrezueta-Guzman, Stephan Krusche, Stefan Wagner
Categories: cs.CY cs.AI cs.HC
Comments: This is the authors' preprint version of a paper accepted at the 11th
  International Symposium on Educational Technology, to be held in July 2025,
  in Bangkok, Thailand. The final published version will be available via IEEE
  Xplore Library
\\
  The rapid adoption of AI powered coding assistants like ChatGPT and other
coding copilots is transforming programming education, raising questions about
assessment practices, academic integrity, and skill development. As educators
seek alternatives to traditional grading methods susceptible to AI enabled
plagiarism, structured peer assessment could be a promising strategy. This
paper presents an empirical study of a rubric based, anonymized peer review
process implemented in a large introductory programming course.
  Students evaluated each other's final projects (2D game), and their
assessments were compared to instructor grades using correlation, mean absolute
error, and root mean square error (RMSE). Additionally, reflective surveys from
47 teams captured student perceptions of fairness, grading behavior, and
preferences regarding grade aggregation. Results show that peer review can
approximate instructor evaluation with moderate accuracy and foster student
engagement, evaluative thinking, and interest in providing good feedback to
their peers. We discuss these findings for designing scalable, trustworthy peer
assessment systems to face the age of AI assisted coding.
\\ ( https://arxiv.org/abs/2505.22093 ,  152kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22287
Date: Wed, 28 May 2025 12:26:55 GMT   (2111kb)

Title: New Tools are Needed for Tracking Adherence to AI Model Behavioral Use
  Clauses
Authors: Daniel McDuff, Tim Korjakow, Kevin Klyman, Danish Contractor
Categories: cs.CY cs.AI
Comments: Preprint
\\
  Foundation models have had a transformative impact on AI. A combination of
large investments in research and development, growing sources of digital data
for training, and architectures that scale with data and compute has led to
models with powerful capabilities. Releasing assets is fundamental to
scientific advancement and commercial enterprise. However, concerns over
negligent or malicious uses of AI have led to the design of mechanisms to limit
the risks of the technology. The result has been a proliferation of licenses
with behavioral-use clauses and acceptable-use-policies that are increasingly
being adopted by commonly used families of models (Llama, Gemma, Deepseek) and
a myriad of smaller projects. We created and deployed a custom AI licenses
generator to facilitate license creation and have quantitatively and
qualitatively analyzed over 300 customized licenses created with this tool.
Alongside this we analyzed 1.7 million models licenses on the HuggingFace model
hub. Our results show increasing adoption of these licenses, interest in tools
that support their creation and a convergence on common clause configurations.
In this paper we take the position that tools for tracking adoption of, and
adherence to, these licenses is the natural next step and urgently needed in
order to ensure they have the desired impact of ensuring responsible use.
\\ ( https://arxiv.org/abs/2505.22287 ,  2111kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22401
Date: Wed, 28 May 2025 14:28:31 GMT   (268kb)

Title: Facial Age Estimation: A Research Roadmap for Technological and Legal
  Development and Deployment
Authors: Richard Guest, Eva Lievens, Martin Sas, Elena Botoeva, Temitope
  Adeyemo, Valerie Verdoodt, Elora Fernandes, Chris Allgrove
Categories: cs.CY
\\
  Automated facial age assessment systems operate in either estimation mode -
predicting age based on facial traits, or verification mode - confirming a
claimed age. These systems support access control to age-restricted goods,
services, and content, and can be used in areas like e-commerce, social media,
forensics, and refugee support. They may also personalise services in
healthcare, finance, and advertising. While improving technological accuracy is
essential, deployment must consider legal, ethical, sociological, alongside
technological factors. This white paper reviews the current challenges in
deploying such systems, outlines the relevant legal and regulatory landscape,
and explores future research for fair, robust, and ethical age estimation
technologies.
\\ ( https://arxiv.org/abs/2505.22401 ,  268kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22526
Date: Wed, 28 May 2025 16:13:27 GMT   (610kb)

Title: AI instructional agent improves student's perceived learner control and
  learning outcome: empirical evidence from a randomized controlled trial
Authors: Fei Qin, Zhanxin Hao, Jifan Yu, Zhiyuan Liu, Yu Zhang
Categories: cs.CY
\\
  This study examines the impact of an AI instructional agent on students'
perceived learner control and academic performance in a medium demanding course
with lecturing as the main teaching strategy. Based on a randomized controlled
trial, three instructional conditions were compared: a traditional human
teacher, a self-paced MOOC with chatbot support, and an AI instructional agent
capable of delivering lectures and responding to questions in real time.
Students in the AI instructional agent group reported significantly higher
levels of perceived learner control compared to the other groups. They also
completed the learning task more efficiently and engaged in more frequent
interactions with the instructional system. Regression analyzes showed that
perceived learner control positively predicted post-test performance, with
behavioral indicators such as reduced learning time and higher interaction
frequency supporting this relationship. These findings suggest that AI
instructional agents, when designed to support personalized pace and responsive
interaction, can enhance both students' learning experience and learning
outcomes.
\\ ( https://arxiv.org/abs/2505.22526 ,  610kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22639
Date: Wed, 28 May 2025 17:54:30 GMT   (428kb)

Title: Navigating the AI-Energy Nexus with Geopolitical Insight
Authors: Nidhi Kalra, Robin Wang, and Ismael Arciniegas Rueda
Categories: cs.CY
\\
  This working paper examines how geopolitical strategies and energy resource
management intersect with Artificial Intelligence (AI) development, delineating
the AI-energy nexus as critical to sustaining U.S. AI leadership. By analyzing
the centralized approaches of authoritarian regimes like China and Gulf
nations, alongside market-driven approaches in the U.S., the paper explores
divergent strategies to allocate resources for AI energy needs. It underscores
the role of energy infrastructure, market dynamics, and state-led initiatives
in shaping global AI competition. Recommendations include adopting
geopolitically informed analyses and leveraging both market and non-market
strengths to enhance U.S. competitiveness. This research aims to inform
policymakers, technologists, and researchers about the strategic implications
of the AI-energy nexus and offers insights into advancing U.S. global
leadership in AI amidst evolving technological paradigms.
\\ ( https://arxiv.org/abs/2505.22639 ,  428kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21627
Date: Tue, 27 May 2025 18:02:12 GMT   (1769kb)

Title: Is Your LLM Overcharging You? Tokenization, Transparency, and Incentives
Authors: Ander Artola Velasco, Stratis Tsirtsis, Nastaran Okati, Manuel
  Gomez-Rodriguez
Categories: cs.GT cs.AI cs.CY cs.LG
\\
  State-of-the-art large language models require specialized hardware and
substantial energy to operate. As a consequence, cloud-based services that
provide access to large language models have become very popular. In these
services, the price users pay for an output provided by a model depends on the
number of tokens the model uses to generate it -- they pay a fixed price per
token. In this work, we show that this pricing mechanism creates a financial
incentive for providers to strategize and misreport the (number of) tokens a
model used to generate an output, and users cannot prove, or even know, whether
a provider is overcharging them. However, we also show that, if an unfaithful
provider is obliged to be transparent about the generative process used by the
model, misreporting optimally without raising suspicion is hard. Nevertheless,
as a proof-of-concept, we introduce an efficient heuristic algorithm that
allows providers to significantly overcharge users without raising suspicion,
highlighting the vulnerability of users under the current pay-per-token pricing
mechanism. Further, to completely eliminate the financial incentive to
strategize, we introduce a simple incentive-compatible token pricing mechanism.
Under this mechanism, the price users pay for an output provided by a model
depends on the number of characters of the output -- they pay a fixed price per
character. Along the way, to illustrate and complement our theoretical results,
we conduct experiments with several large language models from the
$\texttt{Llama}$, $\texttt{Gemma}$ and $\texttt{Ministral}$ families, and input
prompts from the LMSYS Chatbot Arena platform.
\\ ( https://arxiv.org/abs/2505.21627 ,  1769kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21685
Date: Tue, 27 May 2025 19:14:07 GMT   (380kb)

Title: Proof of Work With External Utilities
Authors: Yogev Bar-On, Ilan Komargodski, Omri Weinstein
Categories: cs.GT
\\
  Proof-of-Work (PoW) consensus is traditionally analyzed under the assumption
that all miners incur similar costs per unit of computational effort. In
reality, costs vary due to factors such as regional electricity cost
differences and access to specialized hardware. These variations in mining
costs become even more pronounced in the emerging paradigm of
\emph{Proof-of-Useful-Work} (PoUW), where miners can earn additional
\emph{external} rewards by performing beneficial computations, such as
Artificial Intelligence (AI) training and inference workloads.
  Continuing the work of Fiat et al., who investigate equilibrium dynamics of
PoW consensus under heterogeneous cost structures due to varying energy costs,
we expand their model to also consider external rewards. We develop a
theoretical framework to model miner behavior in such conditions and analyze
the resulting equilibrium. Our findings suggest that in some cases, miners with
access to external incentives will optimize profitability by concentrating
their useful tasks in a single block. We also explore the implications of
external rewards for decentralization, modeling it as the Shannon entropy of
computational effort distribution among participants.
  Empirical evidence supports many of our assumptions, indicating that AI
training and inference workloads, when reused for consensus, can retain
security comparable to Bitcoin while dramatically reducing computational costs
and environmental waste.
\\ ( https://arxiv.org/abs/2505.21685 ,  380kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22174
Date: Wed, 28 May 2025 09:48:16 GMT   (58kb)

Title: Online Fair Division for Personalized $2$-Value Instances
Authors: Georgios Amanatidis, Alexandros Lolos, Evangelos Markakis, Victor
  Turmel
Categories: cs.GT cs.AI cs.MA
\\
  We study an online fair division setting, where goods arrive one at a time
and there is a fixed set of $n$ agents, each of whom has an additive valuation
function over the goods. Once a good appears, the value each agent has for it
is revealed and it must be allocated immediately and irrevocably to one of the
agents. It is known that without any assumptions about the values being
severely restricted or coming from a distribution, very strong impossibility
results hold in this setting. To bypass the latter, we turn our attention to
instances where the valuation functions are restricted. In particular, we study
personalized $2$-value instances, where there are only two possible values each
agent may have for each good, possibly different across agents, and we show how
to obtain worst case guarantees with respect to well-known fairness notions,
such as maximin share fairness and envy-freeness up to one (or two) good(s). We
suggest a deterministic algorithm that maintains a $1/(2n-1)$-MMS allocation at
every time step and show that this is the best possible any deterministic
algorithm can achieve if one cares about every single time step; nevertheless,
eventually the allocation constructed by our algorithm becomes a $1/4$-MMS
allocation. To achieve this, the algorithm implicitly maintains a fragile
system of priority levels for all agents. Further, we show that, by allowing
some limited access to future information, it is possible to have stronger
results with less involved approaches. By knowing the values of goods for $n-1$
time steps into the future, we design a matching-based algorithm that achieves
an EF$1$ allocation every $n$ time steps, while always maintaining an EF$2$
allocation. Finally, we show that our results allow us to get the first
nontrivial guarantees for additive instances in which the ratio of the maximum
over the minimum value an agent has for a good is bounded.
\\ ( https://arxiv.org/abs/2505.22174 ,  58kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22513
Date: Wed, 28 May 2025 16:02:52 GMT   (56kb)

Title: Strengthening Proportionality in Temporal Voting
Authors: Bradley Phillips, Edith Elkind, Nicholas Teh, Tomasz W\k{a}s
Categories: cs.GT cs.AI
\\
  We study proportional representation in the framework of temporal voting with
approval ballots. Prior work adapted basic proportional representation concepts
-- justified representation (JR), proportional JR (PJR), and extended JR (EJR)
-- from the multiwinner setting to the temporal setting. Our work introduces
and examines ways of going beyond EJR. Specifically, we consider stronger
variants of JR, PJR, and EJR, and introduce temporal adaptations of more
demanding multiwinner axioms, such as EJR+, full JR (FJR), full proportional JR
(FPJR), and the Core. For each of these concepts, we investigate its existence
and study its relationship to existing notions, thereby establishing a rich
hierarchy of proportionality concepts. Notably, we show that two of our
proposed axioms -- EJR+ and FJR -- strengthen EJR while remaining satisfiable
in every temporal election.
\\ ( https://arxiv.org/abs/2505.22513 ,  56kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21875
Date: Wed, 28 May 2025 01:40:55 GMT   (5675kb)

Title: Broadening Our View: Assistive Technology for Cerebral Visual Impairment
Authors: Bhanuka Gamage, Leona Holloway, Nicola McDowell, Thanh-Toan Do,
  Nicholas Seow Chiang Price, Arthur James Lowery, and Kim Marriott
Categories: cs.HC
Comments: Author's accepted version of a LBW paper published in Extended
  Abstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA
  '24)
DOI: 10.1145/3613905.3650740
\\
  Over the past decade, considerable research has been directed towards
assistive technologies to support people with vision impairments using machine
learning, computer vision, image enhancement, and/or augmented/virtual reality.
However, this has almost totally overlooked a growing demographic: people with
Cerebral Visual Impairment (CVI). Unlike Ocular Vision Impairments (OVI), CVI
arises from damage to the brain's visual processing centres. This paper
introduces CVI and reveals a wide research gap in addressing the needs of this
demographic. Through a scoping review, we identified 14 papers at the
intersection of these technologies and CVI. Of these, only three papers
described assistive technologies focused on people living with CVI, with the
others focusing on diagnosis, understanding, simulation or rehabilitation. Our
findings highlight the opportunity for the Human-Computer Interaction and
Assistive Technologies research community to explore and address this
underrepresented domain, thereby enhancing the quality of life for people with
CVI.
\\ ( https://arxiv.org/abs/2505.21875 ,  5675kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21891
Date: Wed, 28 May 2025 02:10:03 GMT   (20950kb)

Title: TIEboard: A Digital Educational Tool for Kids Geometric Learning
Authors: Arooj Zaidi, Giulia Barbareschi, Kai Kunze, Yun Suen Pai, Junichi
  Yamaoka
Categories: cs.HC
Journal-ref: Proc. ACM Interactive Mobile Wearable Ubiquitous Technology, Vol.
  9, No. 2, Article 62. Publication date: June 2025
DOI: 10.1145/3729478
\\
  Tangible User Interfaces have shown potential in supporting the acquisition
of key concepts in computing and mathematics while fostering engagement in
young learners, but these approaches are less commonly utilised in the context
of geometry. In this paper we introduce TIEboard, an interactive device to
promote early learning of basic geometry concepts. TIEboard draws inspiration
from traditional geoboards and lacing toys to leverage children's familiarity
with these traditional tools. It employs instructional lights to guide children
in creating shapes using colourful threads of optical fiber. The use of
conductive materials allows the system to detect lacing activity and provide
feedback in real-time. TIEboard incorporates six interaction modes of varying
difficulty based on an incremental learning framework. The study evaluated
TIEboard's effectiveness in supporting early geometric learning, facilitating
creativity and promoting collaboration among 16 children aged 5-9.
\\ ( https://arxiv.org/abs/2505.21891 ,  20950kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21964
Date: Wed, 28 May 2025 04:32:05 GMT   (942kb)

Title: UI-Evol: Automatic Knowledge Evolving for Computer Use Agents
Authors: Ziyun Zhang, Xinyi Liu, Xiaoyi Zhang, Jun Wang, Gang Chen, Yan Lu
Categories: cs.HC cs.CL
\\
  External knowledge has played a crucial role in the recent development of
computer use agents. We identify a critical knowledge-execution gap: retrieved
knowledge often fails to translate into effective real-world task execution.
Our analysis shows even 90\% correct knowledge yields only 41\% execution
success rate. To bridge this gap, we propose UI-Evol, a plug-and-play module
for autonomous GUI knowledge evolution. UI-Evol consists of two stages: a
Retrace Stage that extracts faithful objective action sequences from actual
agent-environment interactions, and a Critique Stage that refines existing
knowledge by comparing these sequences against external references. We conduct
comprehensive experiments on the OSWorld benchmark with the state-of-the-art
Agent S2. Our results demonstrate that UI-Evol not only significantly boosts
task performance but also addresses a previously overlooked issue of high
behavioral standard deviation in computer use agents, leading to superior
performance on computer use tasks and substantially improved agent reliability.
\\ ( https://arxiv.org/abs/2505.21964 ,  942kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21966
Date: Wed, 28 May 2025 04:36:08 GMT   (44187kb)

Title: MapStory: LLM-Powered Text-Driven Map Animation Prototyping with
  Human-in-the-Loop Editing
Authors: Aditya Gunturu, Ben Pearman, Keiichi Ihara, Morteza Faraji, Bryan
  Wang, Rubaiat Habib Kazi, and Ryo Suzuki
Categories: cs.HC cs.AI cs.CL cs.MM
Comments: 16 pages and 15 figures
MSC-class: H.5.2, H.5.1
\\
  We introduce MapStory, an LLM-powered animation authoring tool that generates
editable map animation sequences directly from natural language text. Given a
user-written script, MapStory leverages an agentic architecture to
automatically produce a scene breakdown, which decomposes the script into key
animation building blocks such as camera movements, visual highlights, and
animated elements. Our system includes a researcher component that accurately
queries geospatial information by leveraging an LLM with web search, enabling
the automatic extraction of relevant regions, paths, and coordinates while
allowing users to edit and query for changes or additional information to
refine the results. Additionally, users can fine-tune parameters of these
blocks through an interactive timeline editor. We detail the system's design
and architecture, informed by formative interviews with professional animators
and an analysis of 200 existing map animation videos. Our evaluation, which
includes expert interviews (N=5) and a usability study (N=12), demonstrates
that MapStory enables users to create map animations with ease, facilitates
faster iteration, encourages creative exploration, and lowers barriers to
creating map-centric stories.
\\ ( https://arxiv.org/abs/2505.21966 ,  44187kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21982
Date: Wed, 28 May 2025 05:21:29 GMT   (303kb)

Title: Eye-Tracking and Biometric Feedback in UX Research: Measuring User
  Engagement and Cognitive Load
Authors: Aaditya Shankar Majumder
Categories: cs.HC
Comments: 4 pages
\\
  User experience research often uses surveys and interviews, which may miss
subconscious user interactions. This study explores eye-tracking and biometric
feedback as tools to assess user engagement and cognitive load in digital
interfaces. These methods measure gaze behavior and bodily responses, providing
an objective complement to qualitative insights. Using empirical evidence,
practical applications, and advancements from 2023-2025, we present
experimental data, describe our methodology, and place our work within
foundational and recent literature. We address challenges like data
interpretation, ethical issues, and technological integration. These tools are
key for advancing UX design in complex digital environments.
\\ ( https://arxiv.org/abs/2505.21982 ,  303kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22303
Date: Wed, 28 May 2025 12:40:37 GMT   (2577kb)

Title: Voice CMS: updating the knowledge base of a digital assistant through
  conversation
Authors: Grzegorz Wolny and Micha{\l} Szczerbak
Categories: cs.HC cs.AI cs.MA
\\
  In this study, we propose a solution based on a multi-agent LLM architecture
and a voice user interface (VUI) designed to update the knowledge base of a
digital assistant. Its usability is evaluated in comparison to a more
traditional graphical content management system (CMS), with a focus on
understanding the relationship between user preferences and the complexity of
the information being provided. The findings demonstrate that, while the
overall usability of the VUI is rated lower than the graphical interface, it is
already preferred by users for less complex tasks. Furthermore, the quality of
content entered through the VUI is comparable to that achieved with the
graphical interface, even for highly complex tasks. Obtained qualitative
results suggest that a hybrid interface combining the strengths of both
approaches could address the key challenges identified during the experiment,
such as reducing cognitive load through graphical feedback while maintaining
the intuitive nature of voice-based interactions. This work highlights the
potential of conversational interfaces as a viable and effective method for
knowledge management in specific business contexts.
\\ ( https://arxiv.org/abs/2505.22303 ,  2577kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22414
Date: Wed, 28 May 2025 14:41:29 GMT   (851kb)

Title: ToPSen: Task-Oriented Priming and Sensory Alignment for Comparing Coding
  Strategies Between Sighted and Blind Programmers
Authors: Md Ehtesham-Ul-Haque, Syed Masum Billah
Categories: cs.HC
Comments: Accepted at DIS'25
\\
  This paper examines how the coding strategies of sighted and blind
programmers differ when working with audio feedback alone. The goal is to
identify challenges in mixed-ability collaboration, particularly when sighted
programmers work with blind peers or teach programming to blind students. To
overcome limitations of traditional blindness simulation studies, we proposed
Task-Oriented Priming and Sensory Alignment (ToPSen), a design framework that
reframes sensory constraints as technical requirements rather than as a
disability. Through a study of 12 blind and 12 sighted participants coding
non-visually, we found that expert blind programmers maintain more accurate
mental models and process more information in working memory than sighted
programmers using ToPSen. Our analysis revealed that blind and sighted
programmers process structural information differently, exposing gaps in
current IDE designs. These insights inform our guidelines for improving the
accessibility of programming tools and fostering effective mixed-ability
collaboration.
\\ ( https://arxiv.org/abs/2505.22414 ,  851kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22418
Date: Wed, 28 May 2025 14:43:55 GMT   (1368kb)

Title: AI Trust Reshaping Administrative Burdens: Understanding Trust-Burden
  Dynamics in LLM-Assisted Benefits Systems
Authors: Jeongwon Jo, He Zhang, Jie Cai, Nitesh Goyal
Categories: cs.HC
Comments: FAccT 2025
DOI: 10.1145/3715275.3732077
\\
  Supplemental Nutrition Assistance Program (SNAP) is an essential benefit
support system provided by the US administration to 41 million federally
determined low-income applicants. Through interviews with such applicants
across a diverse set of experiences with the SNAP system, our findings reveal
that new AI technologies like LLMs can alleviate traditional burdens but also
introduce new burdens. We introduce new types of learning, compliance, and
psychological costs that transform the administrative burden on applicants. We
also identify how trust in AI across three dimensions--competence, integrity,
and benevolence--is perceived to reduce administrative burdens, which may stem
from unintended and untoward overt trust in the system. We discuss calibrating
appropriate levels of user trust in LLM-based administrative systems,
mitigating newly introduced burdens. In particular, our findings suggest that
evidence-based information disclosure is necessary in benefits administration
and propose directions for future research on trust-burden dynamics in
AI-assisted administration systems.
\\ ( https://arxiv.org/abs/2505.22418 ,  1368kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22428
Date: Wed, 28 May 2025 14:53:08 GMT   (167kb)

Title: Parental Collaboration and Closeness: Envisioning with New Couple
  Parents
Authors: Ya-Fang Lin, Xiaotian Li, Wan-Hsuan Huang, Charan Pushpanathan
  Prabavathi, Jie Cai, John M. Carroll
Categories: cs.HC cs.CY
Comments: DIS 2025
DOI: 10.1145/3715336.3735837
\\
  Couples often experience a decrease in closeness as they cope with the
demands of parenthood. Existing technologies have supported parenting and
parental collaboration. However, these technologies do not adequately support
closeness in co-parenting. We use scenarios and design probes to brainstorm
with 10 new parent couples to explore and envision possibilities for
technologies to support closeness. We reported parents' current technology use
for co-parenting and how participants considered and envisioned co-parenting
technology for closeness, including information and task sharing, emotion
awareness and disclosure, and fostering fun interaction. We discuss the
potential technology has for fostering closeness in co-parenting by (1)
fostering interdependence by supporting parental competence and (2) integrating
positive emotions and experiences, such as validation and fun, in parenting.
Based on our findings, we expand the design space of technology for closeness
to include interdependence. We also expand the design space for co-parenting
technology by integrating more positive emotions.
\\ ( https://arxiv.org/abs/2505.22428 ,  167kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22477
Date: Wed, 28 May 2025 15:27:52 GMT   (993kb)

Title: Human-Centered Human-AI Collaboration (HCHAC)
Authors: Qi Gao, Wei Xu, Hanxi Pan, Mowei Shen, Zaifeng Gao
Categories: cs.HC cs.AI cs.CY
Comments: This article is a chapter from the upcoming book Handbook of
  Human-Centered Artificial Intelligence
\\
  In the intelligent era, the interaction between humans and intelligent
systems fundamentally involves collaboration with autonomous intelligent
agents. Human-AI Collaboration (HAC) represents a novel type of human-machine
relationship facilitated by autonomous intelligent machines equipped with AI
technologies. In this paradigm, AI agents serve not only as auxiliary tools but
also as active teammates, partnering with humans to accomplish tasks
collaboratively. Human-centered AI (HCAI) emphasizes that humans play critical
leadership roles in the collaboration. This human-led collaboration imparts new
dimensions to the human-machine relationship, necessitating innovative research
perspectives, paradigms, and agenda to address the unique challenges posed by
HAC. This chapter delves into the essence of HAC from the human-centered
perspective, outlining its core concepts and distinguishing features. It
reviews the current research methodologies and research agenda within the HAC
field from the HCAI perspective, highlighting advancements and ongoing studies.
Furthermore, a framework for human-centered HAC (HCHAC) is proposed by
integrating these reviews and analyses. A case study of HAC in the context of
autonomous vehicles is provided, illustrating practical applications and the
synergistic interactions between humans and AI agents. Finally, it identifies
potential future research directions aimed at enhancing the effectiveness,
reliability, and ethical integration of human-centered HAC systems in diverse
domains.
\\ ( https://arxiv.org/abs/2505.22477 ,  993kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22539
Date: Wed, 28 May 2025 16:23:28 GMT   (30718kb,D)

Title: Spot-On: A Mixed Reality Interface for Multi-Robot Cooperation
Authors: Tim Engelbracht, Petar Lukovic, Tjark Behrens, Kai Lascheit, Ren\'e
  Zurbr\"ugg, Marc Pollefeys, Hermann Blum and Zuria Bauer
Categories: cs.HC cs.RO
\\
  Recent progress in mixed reality (MR) and robotics is enabling increasingly
sophisticated forms of human-robot collaboration. Building on these
developments, we introduce a novel MR framework that allows multiple quadruped
robots to operate in semantically diverse environments via a MR interface. Our
system supports collaborative tasks involving drawers, swing doors, and
higher-level infrastructure such as light switches. A comprehensive user study
verifies both the design and usability of our app, with participants giving a
"good" or "very good" rating in almost all cases. Overall, our approach
provides an effective and intuitive framework for MR-based multi-robot
collaboration in complex, real-world scenarios.
\\ ( https://arxiv.org/abs/2505.22539 ,  30718kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21681
Date: Tue, 27 May 2025 19:06:48 GMT   (375kb)

Title: Residual Diffusion Models for Variable-Rate Joint Source Channel Coding
  of MIMO CSI
Authors: Sravan Kumar Ankireddy, Heasung Kim, Hyeji Kim
Categories: cs.IT math.IT
Comments: 13 pages, 11 figures
\\
  Despite significant advancements in deep learning-based CSI compression, some
key limitations remain unaddressed. Current approaches predominantly treat CSI
compression as a source coding problem, neglecting transmission errors. In
finite block length regimes, separate source and channel coding proves
suboptimal, with reconstruction performance deteriorating significantly under
challenging channel conditions. While existing autoencoder-based compression
schemes can be readily extended to support joint source-channel coding, they
struggle to capture complex channel distributions and exhibit poor scalability
with increasing parameter count. To overcome these inherent limitations of
autoencoder-based approaches, we propose Residual-Diffusion Joint
Source-Channel Coding (RD-JSCC), a novel framework that integrates a
lightweight autoencoder with a residual diffusion module to iteratively refine
CSI reconstruction. Our flexible decoding strategy balances computational
efficiency and performance by dynamically switching between low-complexity
autoencoder decoding and sophisticated diffusion-based refinement based on
channel conditions. Comprehensive simulations demonstrate that RD-JSCC
significantly outperforms existing autoencoder-based approaches in challenging
wireless environments. Furthermore, RD-JSCC offers several practical features,
including a low-latency 2-step diffusion during inference, support for multiple
compression rates with a single model, robustness to fixed-bit quantization,
and adaptability to imperfect channel estimation.
\\ ( https://arxiv.org/abs/2505.21681 ,  375kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21751
Date: Tue, 27 May 2025 20:40:13 GMT   (7647kb)

Title: Ambient-aware continuous aid for mountain rescue activities
Authors: Radoslaw Klimek
Categories: cs.IT math.IT
Comments: This document is a pre-print copy of the accepted article for
  Information Sciences
Report-no: 2024, vol. 653, no. 119772, pp. 1-31
DOI: 10.1016/j.ins.2023.119772
\\
  Ambient-awareness in conjunction with pervasive computing is a significant
challenge for system designers. It follows the necessity of gathering raw,
massive and heterogeneous environmental data \newrrr{which we} obtained, while
middleware processes must merge context modelling and reasoning seamlessly. We
proposed a system supporting mountain rescuers which is demanding due to the
large number of environmental objects interacting, as well as high data
variability. We presented complex context processing embedded in the proposed
context life cycle and implemented it \erarrr{following a proposed workflow for
a demanding}\newrrr{in a difficult} mountain environment. We introduced five
weather scenarios which are a basis for contextual and perceptual processing
during the validation of our model. The system \erarrr{binds together}
\newrrr{merges} a message streaming broker for massive data transport, low and
high-level processing algorithms, repositories and a logical SAT solver. It
constitutes a Context-Aware-as-a-Service (CAaaS) system, offering advanced
support for mountain rescue operations. The provided software model defines
middleware components which act on a predicted context and transform in situ
sensor data into smart decisions, and which could operate as a platform-based
cloud computing model. It is an enabler yielding a synergy effect with
different software components orchestration when providing pro-activeness and
non-intrusiveness concerning smart decisions.
\\ ( https://arxiv.org/abs/2505.21751 ,  7647kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21951
Date: Wed, 28 May 2025 04:04:24 GMT   (143kb)

Title: When Feedback Empowers the Uplink: Integrating Adaptive Coding with
  Wireless Power Transfer
Authors: Zijian Yang, Yulin Shao, Shaodan Ma
Categories: cs.IT eess.SP math.IT
\\
  Energy consumption and device lifetime are critical concerns for
battery-constrained IoT devices. This paper introduces the Feedback-Aided
Coding and Energy Transfer (FACET) framework, which synergistically combines
adaptive feedback channel coding with wireless power transfer. FACET leverages
the saturation effect of feedback coding, where increasing downlink power
yields diminishing returns, to design a dual-purpose feedback mechanism that
simultaneously guides uplink coding and replenishes device energy. We
characterize the inherent tradeoff between feedback precision and harvested
power, and formulate a fairness-constrained min-max optimization problem to
minimize worst-case net energy consumption. An efficient algorithm based on
alternating optimization and Lagrangian duality is developed, with each
subproblem admitting a closed-form solution. Simulations show that FACET nearly
triples device lifetime compared to conventional feedback coding architectures,
and remains robust across a wide range of power regimes. These results suggest
that FACET not only improves communication efficiency but also redefines the
role of feedback in energy-constrained IoT systems.
\\ ( https://arxiv.org/abs/2505.21951 ,  143kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21971
Date: Wed, 28 May 2025 04:50:03 GMT   (1104kb)

Title: The Tri-Hybrid MIMO Architecture
Authors: Robert W. Heath, Jr., Joseph Carlson, Nitish Vikas Deshpande, Miguel
  Rodrigo Castellanos, Mohamed Akrout, Chan-Byoung Chae
Categories: cs.IT cs.NI math.IT
Comments: 7 pages, 6 figures
\\
  We present an evolution of multiple-input multiple-output (MIMO) wireless
communications known as the tri-hybrid MIMO architecture. In this framework,
the traditional operations of linear precoding at the transmitter are
distributed across digital beamforming, analog beamforming, and reconfigurable
antennas. Compared with the hybrid MIMO architecture, which combines digital
and analog beamforming, the tri-hybrid approach introduces a third layer of
electromagnetic beamforming through antenna reconfigurability. This added layer
offers a pathway to scale MIMO spatial dimensions, important for 6G systems
operating in centimeter-wave bands, where the tension between larger bandwidths
and infrastructure reuse necessitates ultra-large antenna arrays. We introduce
the key features of the tri-hybrid architecture by (i)~reviewing the benefits
and challenges of communicating with reconfigurable antennas, (ii)~examining
tradeoffs between spectral and energy efficiency enabled by reconfigurability,
and (iii)~exploring configuration challenges across the three layers. Overall,
the tri-hybrid MIMO architecture offers a new approach for integrating emerging
antenna technologies in the MIMO precoding framework.
\\ ( https://arxiv.org/abs/2505.21971 ,  1104kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22286
Date: Wed, 28 May 2025 12:22:09 GMT   (501kb)

Title: Wireless Communication for Low-Altitude Economy with UAV Swarm Enabled
  Two-Level Movable Antenna System
Authors: Haiquan Lu, Yong Zeng, Shaodan Ma, Bin Li, Shi Jin, Rui Zhang
Categories: cs.IT eess.SP math.IT
Comments: 13 pages, 10 figures
\\
  Unmanned aerial vehicle (UAV) is regarded as a key enabling platform for
low-altitude economy, due to its advantages such as 3D maneuverability,
flexible deployment, and LoS air-to-air/ground communication links. In
particular, the intrinsic high mobility renders UAV especially suitable for
operating as a movable antenna (MA) from the sky. In this paper, by exploiting
the flexible mobility of UAV swarm and antenna position adjustment of MA, we
propose a novel UAV swarm enabled two-level MA system, where UAVs not only
individually deploy a local MA array, but also form a larger-scale MA system
with their individual MA arrays via swarm coordination. We formulate a general
optimization problem to maximize the minimum achievable rate over all ground
UEs, by jointly optimizing the 3D UAV swarm placement positions, their
individual MAs' positions, and receive beamforming for different UEs. We first
consider the special case where each UAV has only one antenna, under different
scenarios of one single UE, two UEs, and arbitrary number of UEs. In
particular, for the two-UE case, we derive the optimal UAV swarm placement
positions in closed-form that achieves IUI-free communication, where the UAV
swarm forms a uniform sparse array (USA) satisfying collision avoidance
constraint. While for the general case with arbitrary number of UEs, we propose
an efficient alternating optimization algorithm to solve the formulated
non-convex optimization problem. Then, we extend the results to the case where
each UAV is equipped with multiple antennas. Numerical results verify that the
proposed low-altitude UAV swarm enabled MA system significantly outperforms
various benchmark schemes, thanks to the exploitation of two-level mobility to
create more favorable channel conditions for multi-UE communications.
\\ ( https://arxiv.org/abs/2505.22286 ,  501kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22438
Date: Wed, 28 May 2025 15:03:27 GMT   (5093kb)

Title: Synonymous Variational Inference for Perceptual Image Compression
Authors: Zijian Liang, Kai Niu, Changshuo Wang, Jin Xu, Ping Zhang
Categories: cs.IT cs.AI cs.CV cs.LG eess.IV math.IT
Comments: 31 pages, 20 figures. This paper is accepted by Proceedings of the
  42nd International Conference on Machine Learning (ICML 2025) Poster
\\
  Recent contributions of semantic information theory reveal the set-element
relationship between semantic and syntactic information, represented as
synonymous relationships. In this paper, we propose a synonymous variational
inference (SVI) method based on this synonymity viewpoint to re-analyze the
perceptual image compression problem. It takes perceptual similarity as a
typical synonymous criterion to build an ideal synonymous set (Synset), and
approximate the posterior of its latent synonymous representation with a
parametric density by minimizing a partial semantic KL divergence. This
analysis theoretically proves that the optimization direction of perception
image compression follows a triple tradeoff that can cover the existing
rate-distortion-perception schemes. Additionally, we introduce synonymous image
compression (SIC), a new image compression scheme that corresponds to the
analytical process of SVI, and implement a progressive SIC codec to fully
leverage the model's capabilities. Experimental results demonstrate comparable
rate-distortion-perception performance using a single progressive SIC codec,
thus verifying the effectiveness of our proposed analysis method.
\\ ( https://arxiv.org/abs/2505.22438 ,  5093kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21512
Date: Tue, 20 May 2025 18:54:59 GMT   (1312kb)

Title: The Role of Visualization in LLM-Assisted Knowledge Graph Systems:
  Effects on User Trust, Exploration, and Workflows
Authors: Harry Li, Gabriel Appleby, Kenneth Alperin, Steven R Gomez, and Ashley
  Suh
Categories: cs.LG cs.HC
\\
  Knowledge graphs (KGs) are powerful data structures, but exploring them
effectively remains difficult for even expert users. Large language models
(LLMs) are increasingly used to address this gap, yet little is known
empirically about how their usage with KGs shapes user trust, exploration
strategies, or downstream decision-making - raising key design challenges for
LLM-based KG visual analysis systems. To study these effects, we developed
LinkQ, a KG exploration system that converts natural language questions into
structured queries with an LLM. We collaborated with KG experts to design five
visual mechanisms that help users assess the accuracy of both KG queries and
LLM responses: an LLM-KG state diagram that illustrates which stage of the
exploration pipeline LinkQ is in, a query editor displaying the generated query
paired with an LLM explanation, an entity-relation ID table showing extracted
KG entities and relations with semantic descriptions, a query structure graph
that depicts the path traversed in the KG, and an interactive graph
visualization of query results. From a qualitative evaluation with 14
practitioners, we found that users - even KG experts - tended to overtrust
LinkQ's outputs due to its "helpful" visualizations, even when the LLM was
incorrect. Users exhibited distinct workflows depending on their prior
familiarity with KGs and LLMs, challenging the assumption that these systems
are one-size-fits-all - despite often being designed as if they are. Our
findings highlight the risks of false trust in LLM-assisted data analysis tools
and the need for further investigation into the role of visualization as a
mitigation technique.
\\ ( https://arxiv.org/abs/2505.21512 ,  1312kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21514
Date: Wed, 21 May 2025 04:59:44 GMT   (25828kb,D)

Title: SIMCOPILOT: Evaluating Large Language Models for Copilot-Style Code
  Generation
Authors: Mingchao Jiang, Abhinav Jain, Sophia Zorek, Chris Jermaine
Categories: cs.LG cs.PL cs.SE
Comments: Keywords: Benchmark Dataset, LLM Evaluation, Gen-AI, Program
  Synthesis; TLDR: SimCoPilot is a benchmark for evaluating LLMs as
  "copilot"-style interactive coding assistants, testing their ability to
  integrate and complete code within complex real-world software environments
\\
  We introduce SIMCOPILOT, a benchmark that simulates the role of large
language models (LLMs) as interactive, "copilot"-style coding assistants.
Targeting both completion (finishing incomplete methods or code blocks) and
infill tasks (filling missing segments within existing code), SIMCOPILOT
provides a comprehensive framework for evaluating LLM coding capabilities. The
benchmark comprises dedicated sub-benchmarks for Java (SIMCOPILOTJ) and Python
(SIMCOPILOTP), covering diverse codebases varying in size and complexity. Our
key contributions include: (a) establishing a realistic, detailed evaluation
environment to assess LLM utility in practical coding scenarios, and (b)
providing fine-grained analyses that address critical factors frequently
overlooked by existing benchmarks, such as task-specific performance nuances,
contextual understanding across code segments, and sensitivity to variable
scope. Evaluations conducted across domains-including algorithms, databases,
computer vision, and neural networks-offer insights into model strengths and
highlight persistent challenges in maintaining logical consistency within
complex dependency structures. Beyond benchmarking, our study sheds light on
the current limitations of LLM-driven code generation and underscores the
ongoing transition of LLMs from merely syntax-aware generators toward reliable,
intelligent software development partners.
\\ ( https://arxiv.org/abs/2505.21514 ,  25828kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21525
Date: Fri, 23 May 2025 11:44:13 GMT   (7623kb,D)

Title: Temporal Restoration and Spatial Rewiring for Source-Free Multivariate
  Time Series Domain Adaptation
Authors: Peiliang Gong, Yucheng Wang, Min Wu, Zhenghua Chen, Xiaoli Li,
  Daoqiang Zhang
Categories: cs.LG cs.AI cs.CV
\\
  Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained model from
an annotated source domain to an unlabelled target domain without accessing the
source data, thereby preserving data privacy. While existing SFDA methods have
proven effective in reducing reliance on source data, they struggle to perform
well on multivariate time series (MTS) due to their failure to consider the
intrinsic spatial correlations inherent in MTS data. These spatial correlations
are crucial for accurately representing MTS data and preserving invariant
information across domains. To address this challenge, we propose Temporal
Restoration and Spatial Rewiring (TERSE), a novel and concise SFDA method
tailored for MTS data. Specifically, TERSE comprises a customized
spatial-temporal feature encoder designed to capture the underlying
spatial-temporal characteristics, coupled with both temporal restoration and
spatial rewiring tasks to reinstate latent representations of the temporally
masked time series and the spatially masked correlated structures. During the
target adaptation phase, the target encoder is guided to produce spatially and
temporally consistent features with the source domain by leveraging the source
pre-trained temporal restoration and spatial rewiring networks. Therefore,
TERSE can effectively model and transfer spatial-temporal dependencies across
domains, facilitating implicit feature alignment. In addition, as the first
approach to simultaneously consider spatial-temporal consistency in MTS-SFDA,
TERSE can also be integrated as a versatile plug-and-play module into
established SFDA methods. Extensive experiments on three real-world time series
datasets demonstrate the effectiveness and versatility of our approach.
\\ ( https://arxiv.org/abs/2505.21525 ,  7623kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21569
Date: Tue, 27 May 2025 06:22:57 GMT   (1172kb)

Title: ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools
Authors: Zhucong Li, Bowei Zhang, Jin Xiao, Zhijian Zhou, Fenglei Cao, Jiaqing
  Liang, Yuan Qi
Categories: cs.LG cs.AI cs.CL
Comments: 9 pages
\\
  Large Language Model (LLM)-based agents have demonstrated the ability to
improve performance in chemistry-related tasks by selecting appropriate tools.
However, their effectiveness remains limited by the inherent prediction errors
of chemistry tools. In this paper, we take a step further by exploring how
LLMbased agents can, in turn, be leveraged to reduce prediction errors of the
tools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking),
a simple yet effective method that enhances chemistry tools through optimizing
agent-stacking structures from limited data. ChemHAS achieves state-of-the-art
performance across four fundamental chemistry tasks, demonstrating that our
method can effectively compensate for prediction errors of the tools.
Furthermore, we identify and characterize four distinct agent-stacking
behaviors, potentially improving interpretability and revealing new
possibilities for AI agent applications in scientific research. Our code and
dataset are publicly available at https:
//anonymous.4open.science/r/ChemHAS-01E4/README.md.
\\ ( https://arxiv.org/abs/2505.21569 ,  1172kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21571
Date: Tue, 27 May 2025 07:12:09 GMT   (1200kb,D)

Title: FCOS: A Two-Stage Recoverable Model Pruning Framework for Automatic
  Modulation Recognition
Authors: Yao Lu, Tengfei Ma, Zeyu Wang, Zhuangzhi Chen, Dongwei Xu, Yun Lin, Qi
  Xuan, Guan Gui
Categories: cs.LG cs.AI
\\
  With the rapid development of wireless communications and the growing
complexity of digital modulation schemes, traditional manual modulation
recognition methods struggle to extract reliable signal features and meet
real-time requirements in modern scenarios. Recently, deep learning based
Automatic Modulation Recognition (AMR) approaches have greatly improved
classification accuracy. However, their large model sizes and high
computational demands hinder deployment on resource-constrained devices. Model
pruning provides a general approach to reduce model complexity, but existing
weight, channel, and layer pruning techniques each present a trade-off between
compression rate, hardware acceleration, and accuracy preservation. To this
end, in this paper, we introduce FCOS, a novel Fine-to-COarse two-Stage pruning
framework that combines channel-level pruning with layer-level collapse
diagnosis to achieve extreme compression, high performance and efficient
inference. In the first stage of FCOS, hierarchical clustering and parameter
fusion are applied to channel weights to achieve channel-level pruning. Then a
Layer Collapse Diagnosis (LaCD) module uses linear probing to identify layer
collapse and removes the collapsed layers due to high channel compression
ratio. Experiments on multiple AMR benchmarks demonstrate that FCOS outperforms
existing channel and layer pruning methods. Specifically, FCOS achieves 95.51%
FLOPs reduction and 95.31% parameter reduction while still maintaining
performance close to the original ResNet56, with only a 0.46% drop in accuracy
on Sig2019-12. Code is available at https://github.com/yaolu-zjut/FCOS.
\\ ( https://arxiv.org/abs/2505.21571 ,  1200kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21573
Date: Tue, 27 May 2025 07:25:13 GMT   (20398kb,D)

Title: Spectral-inspired Neural Operator for Data-efficient PDE Simulation in
  Physics-agnostic Regimes
Authors: Han Wan and Rui Zhang and Hao Sun
Categories: cs.LG cs.AI
\\
  Partial differential equations (PDEs) govern the spatiotemporal evolution of
various physical systems. Classical numerical solvers, while accurate, require
fine discretization and full knowledge of the governing PDEs, limiting their
applicability when the physics is unknown or fast inference is required.
Data-driven neural PDE solvers alleviate these constraints by learning from
data but demand large training datasets and perform poorly in data-scarce
regimes. Physics-aware methods mitigate data requirements by incorporating
physical knowledge yet rely on known PDE terms or local numerical schemes,
restricting their ability to handle unknown or globally coupled systems. In
this work, we propose the Spectral-inspired Neural Operator (SINO), a novel
framework that learns PDE operators from limited trajectories (as few as 2-5),
without any known PDE terms. SINO operates in the frequency domain and
introduces a Frequency-to-Vector module to learn spectral representations
analogous to derivative multipliers. To model nonlinear physical interactions,
we design a nonlinear operator block that includes a $\Pi$-Block with low-pass
filtering to prevent aliasing. Finally, we introduce an operator distillation
technique to distill the trained model for efficient inference. SINO achieves
state-of-the-art results across multiple PDE benchmarks, demonstrating strong
discretization invariance and robust generalization to out-of-distribution
initial conditions. To our knowledge, SINO is the first physics-aware method
capable of accurately simulating globally coupled systems (e.g., the
Navier-Stokes equations) from limited data without any explicit PDE terms.
\\ ( https://arxiv.org/abs/2505.21573 ,  20398kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21576
Date: Tue, 27 May 2025 07:53:27 GMT   (1503kb,D)

Title: Concentration Distribution Learning from Label Distributions
Authors: Jiawei Tang, Yuheng Jia
Categories: cs.LG cs.AI
\\
  Label distribution learning (LDL) is an effective method to predict the
relative label description degree (a.k.a. label distribution) of a sample.
However, the label distribution is not a complete representation of an instance
because it overlooks the absolute intensity of each label. Specifically, it's
impossible to obtain the total description degree of hidden labels that not in
the label space, which leads to the loss of information and confusion in
instances. To solve the above problem, we come up with a new concept named
background concentration to serve as the absolute description degree term of
the label distribution and introduce it into the LDL process, forming the
improved paradigm of concentration distribution learning. Moreover, we propose
a novel model by probabilistic methods and neural networks to learn label
distributions and background concentrations from existing LDL datasets.
Extensive experiments prove that the proposed approach is able to extract
background concentrations from label distributions while producing more
accurate prediction results than the state-of-the-art LDL methods. The code is
available in https://github.com/seutjw/CDL-LD.
\\ ( https://arxiv.org/abs/2505.21576 ,  1503kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21584
Date: Tue, 27 May 2025 10:41:19 GMT   (241kb)

Title: Fairness in Federated Learning: Fairness for Whom?
Authors: Afaf Taik, Khaoula Chehbouni, Golnoosh Farnadi
Categories: cs.LG cs.AI cs.CY
\\
  Fairness in federated learning has emerged as a rapidly growing area of
research, with numerous works proposing formal definitions and algorithmic
interventions. Yet, despite this technical progress, fairness in FL is often
defined and evaluated in ways that abstract away from the sociotechnical
contexts in which these systems are deployed. In this paper, we argue that
existing approaches tend to optimize narrow system level metrics, such as
performance parity or contribution-based rewards, while overlooking how harms
arise throughout the FL lifecycle and how they impact diverse stakeholders. We
support this claim through a critical analysis of the literature, based on a
systematic annotation of papers for their fairness definitions, design
decisions, evaluation practices, and motivating use cases. Our analysis reveals
five recurring pitfalls: 1) fairness framed solely through the lens of server
client architecture, 2) a mismatch between simulations and motivating use-cases
and contexts, 3) definitions that conflate protecting the system with
protecting its users, 4) interventions that target isolated stages of the
lifecycle while neglecting upstream and downstream effects, 5) and a lack of
multi-stakeholder alignment where multiple fairness definitions can be relevant
at once. Building on these insights, we propose a harm centered framework that
links fairness definitions to concrete risks and stakeholder vulnerabilities.
We conclude with recommendations for more holistic, context-aware, and
accountable fairness research in FL.
\\ ( https://arxiv.org/abs/2505.21584 ,  241kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21587
Date: Tue, 27 May 2025 11:16:49 GMT   (2589kb,D)

Title: CellCLAT: Preserving Topology and Trimming Redundancy in Self-Supervised
  Cellular Contrastive Learning
Authors: Bin Qin, Qirui Ji, Jiangmeng Li, Yupeng Wang, Xuesong Wu, Jianwen Cao,
  Fanjiang Xu
Categories: cs.LG cs.AI
\\
  Self-supervised topological deep learning (TDL) represents a nascent but
underexplored area with significant potential for modeling higher-order
interactions in simplicial complexes and cellular complexes to derive
representations of unlabeled graphs. Compared to simplicial complexes, cellular
complexes exhibit greater expressive power. However, the advancement in
self-supervised learning for cellular TDL is largely hindered by two core
challenges: \textit{extrinsic structural constraints} inherent to cellular
complexes, and intrinsic semantic redundancy in cellular representations. The
first challenge highlights that traditional graph augmentation techniques may
compromise the integrity of higher-order cellular interactions, while the
second underscores that topological redundancy in cellular complexes
potentially diminish task-relevant information. To address these issues, we
introduce Cellular Complex Contrastive Learning with Adaptive Trimming
(CellCLAT), a twofold framework designed to adhere to the combinatorial
constraints of cellular complexes while mitigating informational redundancy.
Specifically, we propose a parameter perturbation-based augmentation method
that injects controlled noise into cellular interactions without altering the
underlying cellular structures, thereby preserving cellular topology during
contrastive learning. Additionally, a cellular trimming scheduler is employed
to mask gradient contributions from task-irrelevant cells through a bi-level
meta-learning approach, effectively removing redundant topological elements
while maintaining critical higher-order semantics. We provide theoretical
justification and empirical validation to demonstrate that CellCLAT achieves
substantial improvements over existing self-supervised graph learning methods,
marking a significant attempt in this domain.
\\ ( https://arxiv.org/abs/2505.21587 ,  2589kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21591
Date: Tue, 27 May 2025 13:40:47 GMT   (6712kb,D)

Title: Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign
  Quantization and Timestep-Aware Fine-Tuning
Authors: Maosen Zhao, Pengtao Chen, Chong Yu, Yan Wen, Xudong Tan, Tao Chen
Categories: cs.LG cs.AI
\\
  Model quantization reduces the bit-width of weights and activations,
improving memory efficiency and inference speed in diffusion models. However,
achieving 4-bit quantization remains challenging. Existing methods, primarily
based on integer quantization and post-training quantization fine-tuning,
struggle with inconsistent performance. Inspired by the success of
floating-point (FP) quantization in large language models, we explore low-bit
FP quantization for diffusion models and identify key challenges: the failure
of signed FP quantization to handle asymmetric activation distributions, the
insufficient consideration of temporal complexity in the denoising process
during fine-tuning, and the misalignment between fine-tuning loss and
quantization error. To address these challenges, we propose the mixup-sign
floating-point quantization (MSFP) framework, first introducing unsigned FP
quantization in model quantization, along with timestep-aware LoRA (TALoRA) and
denoising-factor loss alignment (DFA), which ensure precise and stable
fine-tuning. Extensive experiments show that we are the first to achieve
superior performance in 4-bit FP quantization for diffusion models,
outperforming existing PTQ fine-tuning methods in 4-bit INT quantization.
\\ ( https://arxiv.org/abs/2505.21591 ,  6712kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21595
Date: Tue, 27 May 2025 16:52:29 GMT   (1636kb)

Title: Relevance-driven Input Dropout: an Explanation-guided Regularization
  Technique
Authors: Shreyas Gururaj, Lars Gr\"une, Wojciech Samek, Sebastian Lapuschkin,
  Leander Weber
Categories: cs.LG cs.AI
ACM-class: I.2.6
\\
  Overfitting is a well-known issue extending even to state-of-the-art (SOTA)
Machine Learning (ML) models, resulting in reduced generalization, and a
significant train-test performance gap. Mitigation measures include a
combination of dropout, data augmentation, weight decay, and other
regularization techniques. Among the various data augmentation strategies,
occlusion is a prominent technique that typically focuses on randomly masking
regions of the input during training. Most of the existing literature
emphasizes randomness in selecting and modifying the input features instead of
regions that strongly influence model decisions. We propose Relevance-driven
Input Dropout (RelDrop), a novel data augmentation method which selectively
occludes the most relevant regions of the input, nudging the model to use other
important features in the prediction process, thus improving model
generalization through informed regularization. We further conduct qualitative
and quantitative analyses to study how Relevance-driven Input Dropout (RelDrop)
affects model decision-making. Through a series of experiments on benchmark
datasets, we demonstrate that our approach improves robustness towards
occlusion, results in models utilizing more features within the region of
interest, and boosts inference time generalization performance. Our code is
available at https://github.com/Shreyas-Gururaj/LRP_Relevance_Dropout.
\\ ( https://arxiv.org/abs/2505.21595 ,  1636kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21605
Date: Tue, 27 May 2025 17:47:08 GMT   (1286kb,D)

Title: SOSBENCH: Benchmarking Safety Alignment on Scientific Knowledge
Authors: Fengqing Jiang, Fengbo Ma, Zhangchen Xu, Yuetai Li, Bhaskar
  Ramasubramanian, Luyao Niu, Bo Li, Xianyan Chen, Zhen Xiang, Radha Poovendran
Categories: cs.LG cs.AI cs.CR
\\
  Large language models (LLMs) exhibit advancing capabilities in complex tasks,
such as reasoning and graduate-level question answering, yet their resilience
against misuse, particularly involving scientifically sophisticated risks,
remains underexplored. Existing safety benchmarks typically focus either on
instructions requiring minimal knowledge comprehension (e.g., ``tell me how to
build a bomb") or utilize prompts that are relatively low-risk (e.g.,
multiple-choice or classification tasks about hazardous content). Consequently,
they fail to adequately assess model safety when handling knowledge-intensive,
hazardous scenarios.
  To address this critical gap, we introduce SOSBench, a regulation-grounded,
hazard-focused benchmark encompassing six high-risk scientific domains:
chemistry, biology, medicine, pharmacology, physics, and psychology. The
benchmark comprises 3,000 prompts derived from real-world regulations and laws,
systematically expanded via an LLM-assisted evolutionary pipeline that
introduces diverse, realistic misuse scenarios (e.g., detailed explosive
synthesis instructions involving advanced chemical formulas). We evaluate
frontier models within a unified evaluation framework using our SOSBench.
Despite their alignment claims, advanced models consistently disclose
policy-violating content across all domains, demonstrating alarmingly high
rates of harmful responses (e.g., 79.1% for Deepseek-R1 and 47.3% for GPT-4.1).
These results highlight significant safety alignment deficiencies and
underscore urgent concerns regarding the responsible deployment of powerful
LLMs.
\\ ( https://arxiv.org/abs/2505.21605 ,  1286kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21626
Date: Tue, 27 May 2025 18:00:58 GMT   (2216kb)

Title: Learning Where to Learn: Training Distribution Selection for Provable
  OOD Performance
Authors: Nicolas Guerra, Nicholas H. Nelsen, Yunan Yang
Categories: cs.LG math.OC stat.ML
Comments: 32 pages, 8 figures, 2 tables, 3 algorithms
MSC-class: 62K05, 65K10 (Primary) 68T07, 65D15, 62R20, 60G57 (Secondary)
\\
  Out-of-distribution (OOD) generalization remains a fundamental challenge in
machine learning. Models trained on one data distribution often experience
substantial performance degradation when evaluated on shifted or unseen
domains. To address this challenge, the present paper studies the design of
training data distributions that maximize average-case OOD performance. First,
a theoretical analysis establishes a family of generalization bounds that
quantify how the choice of training distribution influences OOD error across a
predefined family of target distributions. These insights motivate the
introduction of two complementary algorithmic strategies: (i) directly
formulating OOD risk minimization as a bilevel optimization problem over the
space of probability measures and (ii) minimizing a theoretical upper bound on
OOD error. Last, the paper evaluates the two approaches across a range of
function approximation and operator learning examples. The proposed methods
significantly improve OOD accuracy over standard empirical risk minimization
with a fixed distribution. These results highlight the potential of
distribution-aware training as a principled and practical framework for robust
OOD generalization.
\\ ( https://arxiv.org/abs/2505.21626 ,  2216kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21639
Date: Tue, 27 May 2025 18:12:04 GMT   (406kb)

Title: Apprenticeship learning with prior beliefs using inverse optimization
Authors: Mauricio Junca and Esteban Leiva
Categories: cs.LG math.OC
\\
  The relationship between inverse reinforcement learning (IRL) and inverse
optimization (IO) for Markov decision processes (MDPs) has been relatively
underexplored in the literature, despite addressing the same problem. In this
work, we revisit the relationship between the IO framework for MDPs, IRL, and
apprenticeship learning (AL). We incorporate prior beliefs on the structure of
the cost function into the IRL and AL problems, and demonstrate that the
convex-analytic view of the AL formalism (Kamoutsi et al., 2021) emerges as a
relaxation of our framework. Notably, the AL formalism is a special case in our
framework when the regularization term is absent. Focusing on the suboptimal
expert setting, we formulate the AL problem as a regularized min-max problem.
The regularizer plays a key role in addressing the ill-posedness of IRL by
guiding the search for plausible cost functions. To solve the resulting
regularized-convex-concave-min-max problem, we use stochastic mirror descent
(SMD) and establish convergence bounds for the proposed method. Numerical
experiments highlight the critical role of regularization in learning cost
vectors and apprentice policies.
\\ ( https://arxiv.org/abs/2505.21639 ,  406kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21640
Date: Tue, 27 May 2025 18:12:29 GMT   (2405kb)

Title: Efficient Diffusion Models for Symmetric Manifolds
Authors: Oren Mangoubi, Neil He, Nisheeth K. Vishnoi
Categories: cs.LG cs.AI cs.DS math.PR stat.ML
Comments: The conference version of this paper appears in ICML 2025
\\
  We introduce a framework for designing efficient diffusion models for
$d$-dimensional symmetric-space Riemannian manifolds, including the torus,
sphere, special orthogonal group and unitary group. Existing manifold diffusion
models often depend on heat kernels, which lack closed-form expressions and
require either $d$ gradient evaluations or exponential-in-$d$ arithmetic
operations per training step. We introduce a new diffusion model for symmetric
manifolds with a spatially-varying covariance, allowing us to leverage a
projection of Euclidean Brownian motion to bypass heat kernel computations. Our
training algorithm minimizes a novel efficient objective derived via Ito's
Lemma, allowing each step to run in $O(1)$ gradient evaluations and
nearly-linear-in-$d$ ($O(d^{1.19})$) arithmetic operations, reducing the gap
between diffusions on symmetric manifolds and Euclidean space. Manifold
symmetries ensure the diffusion satisfies an "average-case" Lipschitz
condition, enabling accurate and efficient sample generation. Empirically, our
model outperforms prior methods in training speed and improves sample quality
on synthetic datasets on the torus, special orthogonal group, and unitary
group.
\\ ( https://arxiv.org/abs/2505.21640 ,  2405kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21641
Date: Tue, 27 May 2025 18:13:11 GMT   (493kb)

Title: PrivATE: Differentially Private Confidence Intervals for Average
  Treatment Effects
Authors: Maresa Schr\"oder, Justin Hartenstein, Stefan Feuerriegel
Categories: cs.LG cs.CR stat.ME
\\
  The average treatment effect (ATE) is widely used to evaluate the
effectiveness of drugs and other medical interventions. In safety-critical
applications like medicine, reliable inferences about the ATE typically require
valid uncertainty quantification, such as through confidence intervals (CIs).
However, estimating treatment effects in these settings often involves
sensitive data that must be kept private. In this work, we present PrivATE, a
novel machine learning framework for computing CIs for the ATE under
differential privacy. Specifically, we focus on deriving valid
privacy-preserving CIs for the ATE from observational data. Our PrivATE
framework consists of three steps: (i) estimating a differentially private ATE
through output perturbation; (ii) estimating the differentially private
variance through a truncated output perturbation mechanism; and (iii)
constructing the CIs while accounting for the uncertainty from both the
estimation and privatization steps. Our PrivATE framework is model agnostic,
doubly robust, and ensures valid CIs. We demonstrate the effectiveness of our
framework using synthetic and real-world medical datasets. To the best of our
knowledge, we are the first to derive a general, doubly robust framework for
valid CIs of the ATE under ($\varepsilon$, $\delta$)-differential privacy.
\\ ( https://arxiv.org/abs/2505.21641 ,  493kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21651
Date: Tue, 27 May 2025 18:25:21 GMT   (16959kb)

Title: AutoSGD: Automatic Learning Rate Selection for Stochastic Gradient
  Descent
Authors: Nikola Surjanovic, Alexandre Bouchard-C\^ot\'e, Trevor Campbell
Categories: cs.LG math.OC stat.CO stat.ML
\\
  The learning rate is an important tuning parameter for stochastic gradient
descent (SGD) and can greatly influence its performance. However, appropriate
selection of a learning rate schedule across all iterations typically requires
a non-trivial amount of user tuning effort. To address this, we introduce
AutoSGD: an SGD method that automatically determines whether to increase or
decrease the learning rate at a given iteration and then takes appropriate
action. We introduce theory supporting the convergence of AutoSGD, along with
its deterministic counterpart for standard gradient descent. Empirical results
suggest strong performance of the method on a variety of traditional
optimization problems and machine learning tasks.
\\ ( https://arxiv.org/abs/2505.21651 ,  16959kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21660
Date: Tue, 27 May 2025 18:36:19 GMT   (2342kb)

Title: PreGenie: An Agentic Framework for High-quality Visual Presentation
  Generation
Authors: Xiaojie Xu, Xinli Xu, Sirui Chen, Haoyu Chen, Fan Zhang, Ying-Cong
  Chen
Categories: cs.LG
Comments: 11 pages, 9 figures
\\
  Visual presentations are vital for effective communication. Early attempts to
automate their creation using deep learning often faced issues such as poorly
organized layouts, inaccurate text summarization, and a lack of image
understanding, leading to mismatched visuals and text. These limitations
restrict their application in formal contexts like business and scientific
research. To address these challenges, we propose PreGenie, an agentic and
modular framework powered by multimodal large language models (MLLMs) for
generating high-quality visual presentations.
  PreGenie is built on the Slidev presentation framework, where slides are
rendered from Markdown code. It operates in two stages: (1) Analysis and
Initial Generation, which summarizes multimodal input and generates initial
code, and (2) Review and Re-generation, which iteratively reviews intermediate
code and rendered slides to produce final, high-quality presentations. Each
stage leverages multiple MLLMs that collaborate and share information.
Comprehensive experiments demonstrate that PreGenie excels in multimodal
understanding, outperforming existing models in both aesthetics and content
consistency, while aligning more closely with human design preferences.
\\ ( https://arxiv.org/abs/2505.21660 ,  2342kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21666
Date: Tue, 27 May 2025 18:46:21 GMT   (11156kb)

Title: Efficient Controllable Diffusion via Optimal Classifier Guidance
Authors: Owen Oertell, Shikun Sun, Yiding Chen, Jin Peng Zhou, Zhiyong Wang,
  Wen Sun
Categories: cs.LG cs.AI
Comments: 28 pages, 9 figures, 3 tables
\\
  The controllable generation of diffusion models aims to steer the model to
generate samples that optimize some given objective functions. It is desirable
for a variety of applications including image generation, molecule generation,
and DNA/sequence generation. Reinforcement Learning (RL) based fine-tuning of
the base model is a popular approach but it can overfit the reward function
while requiring significant resources. We frame controllable generation as a
problem of finding a distribution that optimizes a KL-regularized objective
function. We present SLCD -- Supervised Learning based Controllable Diffusion,
which iteratively generates online data and trains a small classifier to guide
the generation of the diffusion model. Similar to the standard
classifier-guided diffusion, SLCD's key computation primitive is classification
and does not involve any complex concepts from RL or control. Via a reduction
to no-regret online learning analysis, we show that under KL divergence, the
output from SLCD provably converges to the optimal solution of the
KL-regularized objective. Further, we empirically demonstrate that SLCD can
generate high quality samples with nearly the same inference time as the base
model in both image generation with continuous diffusion and biological
sequence generation with discrete diffusion. Our code is available at
https://github.com/Owen-Oertell/slcd
\\ ( https://arxiv.org/abs/2505.21666 ,  11156kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21677
Date: Tue, 27 May 2025 18:52:34 GMT   (238kb)

Title: What happens when generative AI models train recursively on each others'
  generated outputs?
Authors: Hung Ahn Vu and Galen Reeves and Emily Wenger
Categories: cs.LG cs.AI cs.CY
Comments: 9 pages
\\
  The internet is full of AI-generated content while also serving as a common
source of training data for generative AI (genAI) models. This duality raises
the possibility that future genAI models may be trained on other models'
generated outputs. Prior work has studied consequences of models training on
their own generated outputs, but limited work has considered what happens if
models ingest content produced by other models. Given society's increasing
dependence on genAI tools, understanding downstream effects of such
data-mediated model interactions is critical. To this end, we provide empirical
evidence for how data-mediated interactions might unfold in practice, develop a
theoretical model for this interactive training process, and show
experimentally possible long-term results of such interactions. We find that
data-mediated interactions can benefit models by exposing them to novel
concepts perhaps missed in original training data, but also can homogenize
their performance on shared tasks.
\\ ( https://arxiv.org/abs/2505.21677 ,  238kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21680
Date: Tue, 27 May 2025 18:58:37 GMT   (1236kb)

Title: multivariateGPT: a decoder-only transformer for multivariate categorical
  and numeric data
Authors: Andrew J. Loza, Jun Yup Kim, Shangzheng Song, Yihang Liu, Joseph J. Y.
  Sung, R Andrew Taylor, Dennis L. Shung
Categories: cs.LG cs.AI
Comments: 15 pates, 5 figures
MSC-class: 68T07
ACM-class: I.2.6; I.5.1
\\
  Real-world processes often generate data that are a mix of categorical and
numeric values that are recorded at irregular and informative intervals.
Discrete token-based approaches are limited in numeric representation capacity
while methods like neural ordinary differential equations are not well suited
for categorical data or informative sampling and require augmentation to handle
certain classes of trajectories. Here, we present multivariateGPT, a single
architecture for modeling sequences of mixed categorical (including tokenized
text) and numeric data. This is accomplished with an autoregressive sequence
decomposition, embedding scheme, and loss function that extend the next token
prediction task to likelihood estimation of the joint distribution of next
token class and value. We demonstrate how this approach can efficiently learn
to generalize patterns in simple physical systems and model complex time series
including electrocardiograms and multivariate electronic health record data.
This work extends the utility of transformer based models to additional classes
of data.
\\ ( https://arxiv.org/abs/2505.21680 ,  1236kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21684
Date: Tue, 27 May 2025 19:11:22 GMT   (225kb)

Title: Incentivizing Permissionless Distributed Learning of LLMs
Authors: Joel Lidin, Amir Sarfi, Evangelos Pappas, Samuel Dare, Eugene
  Belilovsky, Jacob Steeves
Categories: cs.LG cs.DC
\\
  We describe an incentive system for distributed deep learning of foundational
models where peers are rewarded for contributions. The incentive system,
\textit{Gauntlet}, has been deployed on the bittensor blockchain and used to
train a 1.2B LLM with completely permissionless contributions of
pseudo-gradients: no control over the users that can register or their
hardware. \textit{Gauntlet} can be applied to any synchronous distributed
training scheme that relies on aggregating updates or pseudo-gradients. We rely
on a two-stage mechanism for fast filtering of peer uptime, reliability, and
synchronization, combined with the core component that estimates the loss
before and after individual pseudo-gradient contributions. We utilized an
OpenSkill rating system to track competitiveness of pseudo-gradient scores
across time. Finally, we introduce a novel mechanism to ensure peers on the
network perform unique computations. Our live 1.2B run, which has paid out
real-valued tokens to participants based on the value of their contributions,
yielded a competitive (on a per-iteration basis) 1.2B model that demonstrates
the utility of our incentive system.
\\ ( https://arxiv.org/abs/2505.21684 ,  225kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21695
Date: Tue, 27 May 2025 19:32:00 GMT   (168kb,D)

Title: AMSFL: Adaptive Multi-Step Federated Learning via Gradient
  Difference-Based Error Modeling
Authors: Ganglou Xu
Categories: cs.LG cs.DC
\\
  Federated learning faces critical challenges in balancing communication
efficiency and model accuracy. One key issue lies in the approximation of
update errors without incurring high computational costs. In this paper, we
propose a lightweight yet effective method called Gradient Difference
Approximation (GDA), which leverages first-order information to estimate local
error trends without computing the full Hessian matrix. The proposed method
forms a key component of the Adaptive Multi-Step Federated Learning (AMSFL)
framework and provides a unified error modeling strategy for large-scale
multi-step adaptive training environments.
\\ ( https://arxiv.org/abs/2505.21695 ,  168kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21717
Date: Tue, 27 May 2025 20:02:59 GMT   (85kb)

Title: Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient
  Sequence Modeling
Authors: M\'onika Farsang, Ramin Hasani, Radu Grosu
Categories: cs.LG cs.AI cs.NE
\\
  We present LrcSSM, a \textit{nonlinear} recurrent model that processes long
sequences as fast as today's linear state-space layers. By forcing the
state-transition matrix to be diagonal and learned at every step, the full
sequence can be solved in parallel with a single prefix-scan, giving
$\mathcal{O}(TD)$ time and memory and only $\mathcal{O}(\log T)$ sequential
depth, for input-sequence length $T$ and a state dimension $D$. Moreover,
LrcSSM offers a formal gradient-stability guarantee that other input-varying
systems such as Liquid-S4 and Mamba do not provide. Lastly, for network depth
$L$, as the forward and backward passes cost $\Theta(T\,D\,L)$ FLOPs, with its
low sequential depth and parameter count $\Theta(D\,L)$, the model follows the
compute-optimal scaling law regime ($\beta \approx 0.42$) recently observed for
Mamba, outperforming quadratic-attention Transformers at equal compute while
avoiding the memory overhead of FFT-based long convolutions. We show that on a
series of long-range forecasting tasks, LrcSSM outperforms LRU, S5 and Mamba.
\\ ( https://arxiv.org/abs/2505.21717 ,  85kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21722
Date: Tue, 27 May 2025 20:09:36 GMT   (3225kb,D)

Title: Saddle-To-Saddle Dynamics in Deep ReLU Networks: Low-Rank Bias in the
  First Saddle Escape
Authors: Ioannis Bantzis, James B. Simon, Arthur Jacot
Categories: cs.LG cs.AI stat.ML
\\
  When a deep ReLU network is initialized with small weights, GD is at first
dominated by the saddle at the origin in parameter space. We study the
so-called escape directions, which play a similar role as the eigenvectors of
the Hessian for strict saddles. We show that the optimal escape direction
features a low-rank bias in its deeper layers: the first singular value of the
$\ell$-th layer weight matrix is at least $\ell^{\frac{1}{4}}$ larger than any
other singular value. We also prove a number of related results about these
escape directions. We argue that this result is a first step in proving
Saddle-to-Saddle dynamics in deep ReLU networks, where GD visits a sequence of
saddles with increasing bottleneck rank.
\\ ( https://arxiv.org/abs/2505.21722 ,  3225kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21731
Date: Tue, 27 May 2025 20:21:46 GMT   (8860kb)

Title: Deep Reinforcement Learning Agents are not even close to Human
  Intelligence
Authors: Quentin Delfosse, Jannis Bl\"uml, Fabian Tatai, Th\'eo Vincent, Bjarne
  Gregori, Elisabeth Dillies, Jan Peters, Constantin Rothkopf, Kristian
  Kersting
Categories: cs.LG cs.AI
Comments: 49 pages in total, 5 main figures, 14 figures total
\\
  Deep reinforcement learning (RL) agents achieve impressive results in a wide
variety of tasks, but they lack zero-shot adaptation capabilities. While most
robustness evaluations focus on tasks complexifications, for which human also
struggle to maintain performances, no evaluation has been performed on tasks
simplifications. To tackle this issue, we introduce HackAtari, a set of task
variations of the Arcade Learning Environments. We use it to demonstrate that,
contrary to humans, RL agents systematically exhibit huge performance drops on
simpler versions of their training tasks, uncovering agents' consistent
reliance on shortcuts. Our analysis across multiple algorithms and
architectures highlights the persistent gap between RL agents and human
behavioral intelligence, underscoring the need for new benchmarks and
methodologies that enforce systematic generalization testing beyond static
evaluation protocols. Training and testing in the same environment is not
enough to obtain agents equipped with human-like intelligence.
\\ ( https://arxiv.org/abs/2505.21731 ,  8860kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21732
Date: Tue, 27 May 2025 20:22:44 GMT   (1994kb)

Title: LaX: Boosting Low-Rank Training of Foundation Models via Latent Crossing
Authors: Ruijie Zhang, Ziyue Liu, Zhengyang Wang, Zheng Zhang
Categories: cs.LG
\\
  Training foundation models such as ViTs and LLMs requires tremendous
computing cost. Low-rank matrix or tensor factorization offers a
parameter-efficient alternative, but often downgrades performance due to the
restricted parameter space. In this work, we introduce {\textbf{Latent Crossing
(LaX)}} -- a simple yet effective plug-and-play module that enhances the
capacity of low-rank models by enabling information flow across low-rank
subspaces. We extensively validate the benefits of LaX on pre-training tasks
with ViT-Base/Large and LLaMA-like models ranging from 60M to 1B parameters.
LaX boosts low-rank model performance to match or exceed the full-rank
baselines while using 2-3\(\times\) fewer parameters. When equipped with
low-rank adapters (i.e., LoRA) for fine-tuning LLaMA-7/13B, LaX consistently
improves performance on arithmetic and common sense reasoning tasks with
negligible cost.
\\ ( https://arxiv.org/abs/2505.21732 ,  1994kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21743
Date: Tue, 27 May 2025 20:33:07 GMT   (1719kb,D)

Title: Simulating the Unseen: Crash Prediction Must Learn from What Did Not
  Happen
Authors: Zihao Li, Xinyuan Cao, Xiangbo Gao, Kexin Tian, Keshu Wu, Mohammad
  Anis, Hao Zhang, Keke Long, Jiwan Jiang, Xiaopeng Li, Yunlong Zhang, Tianbao
  Yang, Dominique Lord, Zhengzhong Tu, Yang Zhou
Categories: cs.LG cs.AI
\\
  Traffic safety science has long been hindered by a fundamental data paradox:
the crashes we most wish to prevent are precisely those events we rarely
observe. Existing crash-frequency models and surrogate safety metrics rely
heavily on sparse, noisy, and under-reported records, while even sophisticated,
high-fidelity simulations undersample the long-tailed situations that trigger
catastrophic outcomes such as fatalities. We argue that the path to achieving
Vision Zero, i.e., the complete elimination of traffic fatalities and severe
injuries, requires a paradigm shift from traditional crash-only learning to a
new form of counterfactual safety learning: reasoning not only about what
happened, but also about the vast set of plausible yet perilous scenarios that
could have happened under slightly different circumstances. To operationalize
this shift, our proposed agenda bridges macro to micro. Guided by crash-rate
priors, generative scene engines, diverse driver models, and causal learning,
near-miss events are synthesized and explained. A crash-focused digital twin
testbed links micro scenes to macro patterns, while a multi-objective validator
ensures that simulations maintain statistical realism. This pipeline transforms
sparse crash data into rich signals for crash prediction, enabling the
stress-testing of vehicles, roads, and policies before deployment. By learning
from crashes that almost happened, we can shift traffic safety from reactive
forensics to proactive prevention, advancing Vision Zero.
\\ ( https://arxiv.org/abs/2505.21743 ,  1719kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21749
Date: Tue, 27 May 2025 20:38:19 GMT   (327kb)

Title: Revisiting Bi-Linear State Transitions in Recurrent Neural Networks
Authors: M.Reza Ebrahimi, Roland Memisevic
Categories: cs.LG cs.CL
\\
  The role of hidden units in recurrent neural networks is typically seen as
modeling memory, with research focusing on enhancing information retention
through gating mechanisms. A less explored perspective views hidden units as
active participants in the computation performed by the network, rather than
passive memory stores. In this work, we revisit bi-linear operations, which
involve multiplicative interactions between hidden units and input embeddings.
We demonstrate theoretically and empirically that they constitute a natural
inductive bias for representing the evolution of hidden states in state
tracking tasks. These are the simplest type of task that require hidden units
to actively contribute to the behavior of the network. We also show that
bi-linear state updates form a natural hierarchy corresponding to state
tracking tasks of increasing complexity, with popular linear recurrent networks
such as Mamba residing at the lowest-complexity center of that hierarchy.
\\ ( https://arxiv.org/abs/2505.21749 ,  327kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21750
Date: Tue, 27 May 2025 20:38:44 GMT   (3743kb)

Title: Hierarchical Reinforcement Learning with Uncertainty-Guided Diffusional
  Subgoals
Authors: Vivienne Huiling Wang, Tinghuai Wang, Joni Pajarinen
Categories: cs.LG
Comments: ICML 2025
\\
  Hierarchical reinforcement learning (HRL) learns to make decisions on
multiple levels of temporal abstraction. A key challenge in HRL is that the
low-level policy changes over time, making it difficult for the high-level
policy to generate effective subgoals. To address this issue, the high-level
policy must capture a complex subgoal distribution while also accounting for
uncertainty in its estimates. We propose an approach that trains a conditional
diffusion model regularized by a Gaussian Process (GP) prior to generate a
complex variety of subgoals while leveraging principled GP uncertainty
quantification. Building on this framework, we develop a strategy that selects
subgoals from both the diffusion policy and GP's predictive mean. Our approach
outperforms prior HRL methods in both sample efficiency and performance on
challenging continuous control benchmarks.
\\ ( https://arxiv.org/abs/2505.21750 ,  3743kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21775
Date: Tue, 27 May 2025 21:19:33 GMT   (1044kb,D)

Title: DualSchool: How Reliable are LLMs for Optimization Education?
Authors: Michael Klamkin, Arnaud Deza, Sikai Cheng, Haoruo Zhao, Pascal Van
  Hentenryck
Categories: cs.LG cs.AI math.OC
\\
  Consider the following task taught in introductory optimization courses which
addresses challenges articulated by the community at the intersection of
(generative) AI and OR: generate the dual of a linear program. LLMs, being
trained at web-scale, have the conversion process and many instances of Primal
to Dual Conversion (P2DC) at their disposal. Students may thus reasonably
expect that LLMs would perform well on the P2DC task. To assess this
expectation, this paper introduces DualSchool, a comprehensive framework for
generating and verifying P2DC instances. The verification procedure of
DualSchool uses the Canonical Graph Edit Distance, going well beyond existing
evaluation methods for optimization models, which exhibit many false positives
and negatives when applied to P2DC. Experiments performed by DualSchool reveal
interesting findings. Although LLMs can recite the conversion procedure
accurately, state-of-the-art open LLMs fail to consistently produce correct
duals. This finding holds even for the smallest two-variable instances and for
derivative tasks, such as correctness, verification, and error classification.
The paper also discusses the implications for educators, students, and the
development of large reasoning systems.
\\ ( https://arxiv.org/abs/2505.21775 ,  1044kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21777
Date: Tue, 27 May 2025 21:20:57 GMT   (17957kb,D)

Title: Memorization to Generalization: Emergence of Diffusion Models from
  Associative Memory
Authors: Bao Pham, Gabriel Raya, Matteo Negri, Mohammed J. Zaki, Luca
  Ambrogioni, Dmitry Krotov
Categories: cs.LG cond-mat.dis-nn
\\
  Hopfield networks are associative memory (AM) systems, designed for storing
and retrieving patterns as local minima of an energy landscape. In the
classical Hopfield model, an interesting phenomenon occurs when the amount of
training data reaches its critical memory load $- spurious\,\,states$, or
unintended stable points, emerge at the end of the retrieval dynamics, leading
to incorrect recall. In this work, we examine diffusion models, commonly used
in generative modeling, from the perspective of AMs. The training phase of
diffusion model is conceptualized as memory encoding (training data is stored
in the memory). The generation phase is viewed as an attempt of memory
retrieval. In the small data regime the diffusion model exhibits a strong
memorization phase, where the network creates distinct basins of attraction
around each sample in the training set, akin to the Hopfield model below the
critical memory load. In the large data regime, a different phase appears where
an increase in the size of the training set fosters the creation of new
attractor states that correspond to manifolds of the generated samples.
Spurious states appear at the boundary of this transition and correspond to
emergent attractor states, which are absent in the training set, but, at the
same time, have distinct basins of attraction around them. Our findings
provide: a novel perspective on the memorization-generalization phenomenon in
diffusion models via the lens of AMs, theoretical prediction of existence of
spurious states, empirical validation of this prediction in commonly-used
diffusion models.
\\ ( https://arxiv.org/abs/2505.21777 ,  17957kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21783
Date: Tue, 27 May 2025 21:31:14 GMT   (3169kb,D)

Title: P-DROP: Poisson-Based Dropout for Graph Neural Networks
Authors: Hyunsik Yun
Categories: cs.LG
Comments: 10 pages, 9 figures
\\
  Over-smoothing remains a major challenge in Graph Neural Networks (GNNs),
where repeated message passing causes node representations to converge and lose
discriminative power. To address this, we propose a novel node selection
strategy based on Poisson processes, introducing stochastic but structure-aware
updates. Specifically, we equip each node with an independent Poisson clock,
enabling asynchronous and localized updates that preserve structural diversity.
We explore two applications of this strategy: as a replacement for
dropout-based regularization and as a dynamic subgraph training scheme.
Experimental results on standard benchmarks (Cora, Citeseer, Pubmed)
demonstrate that our Poisson-based method yields competitive or improved
accuracy compared to traditional Dropout, DropEdge, and DropNode approaches,
particularly in later training stages.
\\ ( https://arxiv.org/abs/2505.21783 ,  3169kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21785
Date: Tue, 27 May 2025 21:36:50 GMT   (426kb)

Title: Born a Transformer -- Always a Transformer?
Authors: Yana Veitsman, Mayank Jobanputra, Yash Sarrof, Aleksandra Bakalova,
  Vera Demberg, Ellie Pavlick, Michael Hahn
Categories: cs.LG cs.CL
\\
  Transformers have theoretical limitations in modeling certain
sequence-to-sequence tasks, yet it remains largely unclear if these limitations
play a role in large-scale pretrained LLMs, or whether LLMs might effectively
overcome these constraints in practice due to the scale of both the models
themselves and their pretraining data. We explore how these architectural
constraints manifest after pretraining, by studying a family of
$\textit{retrieval}$ and $\textit{copying}$ tasks inspired by Liu et al.
[2024]. We use the recently proposed C-RASP framework for studying length
generalization [Huang et al., 2025b] to provide guarantees for each of our
settings. Empirically, we observe an $\textit{induction-versus-anti-induction}$
asymmetry, where pretrained models are better at retrieving tokens to the right
(induction) rather than the left (anti-induction) of a query token. This
asymmetry disappears upon targeted fine-tuning if length-generalization is
guaranteed by theory. Mechanistic analysis reveals that this asymmetry is
connected to the differences in the strength of induction versus anti-induction
circuits within pretrained Transformers. We validate our findings through
practical experiments on real-world tasks demonstrating reliability risks. Our
results highlight that pretraining selectively enhances certain Transformer
capabilities, but does not overcome fundamental length-generalization limits.
\\ ( https://arxiv.org/abs/2505.21785 ,  426kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21790
Date: Tue, 27 May 2025 21:42:10 GMT   (84kb,D)

Title: Faster Rates for Private Adversarial Bandits
Authors: Hilal Asi, Vinod Raman, Kunal Talwar
Categories: cs.LG stat.ML
Comments: Accepted to ICML 2025
\\
  We design new differentially private algorithms for the problems of
adversarial bandits and bandits with expert advice. For adversarial bandits, we
give a simple and efficient conversion of any non-private bandit algorithm to a
private bandit algorithm. Instantiating our conversion with existing
non-private bandit algorithms gives a regret upper bound of
$O\left(\frac{\sqrt{KT}}{\sqrt{\epsilon}}\right)$, improving upon the existing
upper bound $O\left(\frac{\sqrt{KT \log(KT)}}{\epsilon}\right)$ for all
$\epsilon \leq 1$. In particular, our algorithms allow for sublinear expected
regret even when $\epsilon \leq \frac{1}{\sqrt{T}}$, establishing the first
known separation between central and local differential privacy for this
problem. For bandits with expert advice, we give the first differentially
private algorithms, with expected regret
$O\left(\frac{\sqrt{NT}}{\sqrt{\epsilon}}\right),
O\left(\frac{\sqrt{KT\log(N)}\log(KT)}{\epsilon}\right)$, and
$\tilde{O}\left(\frac{N^{1/6}K^{1/2}T^{2/3}\log(NT)}{\epsilon ^{1/3}} +
\frac{N^{1/2}\log(NT)}{\epsilon}\right)$, where $K$ and $N$ are the number of
actions and experts respectively. These rates allow us to get sublinear regret
for different combinations of small and large $K, N$ and $\epsilon.$
\\ ( https://arxiv.org/abs/2505.21790 ,  84kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21792
Date: Tue, 27 May 2025 21:47:20 GMT   (3348kb,D)

Title: Multimodal Federated Learning: A Survey through the Lens of Different FL
  Paradigms
Authors: Yuanzhe Peng, Jieming Bian, Lei Wang, Yin Huang, Jie Xu
Categories: cs.LG cs.AI
\\
  Multimodal Federated Learning (MFL) lies at the intersection of two pivotal
research areas: leveraging complementary information from multiple modalities
to improve downstream inference performance and enabling distributed training
to enhance efficiency and preserve privacy. Despite the growing interest in
MFL, there is currently no comprehensive taxonomy that organizes MFL through
the lens of different Federated Learning (FL) paradigms. This perspective is
important because multimodal data introduces distinct challenges across various
FL settings. These challenges, including modality heterogeneity, privacy
heterogeneity, and communication inefficiency, are fundamentally different from
those encountered in traditional unimodal or non-FL scenarios. In this paper,
we systematically examine MFL within the context of three major FL paradigms:
horizontal FL (HFL), vertical FL (VFL), and hybrid FL. For each paradigm, we
present the problem formulation, review representative training algorithms, and
highlight the most prominent challenge introduced by multimodal data in
distributed settings. We also discuss open challenges and provide insights for
future research. By establishing this taxonomy, we aim to uncover the novel
challenges posed by multimodal data from the perspective of different FL
paradigms and to offer a new lens through which to understand and advance the
development of MFL.
\\ ( https://arxiv.org/abs/2505.21792 ,  3348kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21800
Date: Tue, 27 May 2025 22:14:54 GMT   (6876kb,D)

Title: From Directions to Cones: Exploring Multidimensional Representations of
  Propositional Facts in LLMs
Authors: Stanley Yu, Vaidehi Bulusu, Oscar Yasunaga, Clayton Lau, Cole Blondin,
  Sean O'Brien, Kevin Zhu, Vasu Sharma
Categories: cs.LG cs.CL
\\
  Large Language Models (LLMs) exhibit strong conversational abilities but
often generate falsehoods. Prior work suggests that the truthfulness of simple
propositions can be represented as a single linear direction in a model's
internal activations, but this may not fully capture its underlying geometry.
In this work, we extend the concept cone framework, recently introduced for
modeling refusal, to the domain of truth. We identify multi-dimensional cones
that causally mediate truth-related behavior across multiple LLM families. Our
results are supported by three lines of evidence: (i) causal interventions
reliably flip model responses to factual statements, (ii) learned cones
generalize across model architectures, and (iii) cone-based interventions
preserve unrelated model behavior. These findings reveal the richer,
multidirectional structure governing simple true/false propositions in LLMs and
highlight concept cones as a promising tool for probing abstract behaviors.
\\ ( https://arxiv.org/abs/2505.21800 ,  6876kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21806
Date: Tue, 27 May 2025 22:22:54 GMT   (32595kb)

Title: Towards Operational Automated Greenhouse Gas Plume Detection
Authors: Brian D. Bue, Jake H. Lee, Andrew K. Thorpe, Philip G. Brodrick,
  Daniel Cusworth, Alana Ayasse, Vassiliki Mancoridis, Anagha Satish, Shujun
  Xiong, Riley Duren
Categories: cs.LG
Comments: Main 19 pages 14 figures. Supplemental 19 pages 16 figures. In review
\\
  Operational deployment of a fully automated greenhouse gas (GHG) plume
detection system remains an elusive goal for imaging spectroscopy missions,
despite recent advances in deep learning approaches. With the dramatic increase
in data availability, however, automation continues to increase in importance
for natural and anthropogenic emissions monitoring. This work reviews and
addresses several key obstacles in the field: data and label quality control,
prevention of spatiotemporal biases, and correctly aligned modeling objectives.
We demonstrate through rigorous experiments using multicampaign data from
airborne and spaceborne instruments that convolutional neural networks (CNNs)
are able to achieve operational detection performance when these obstacles are
alleviated. We demonstrate that a multitask model that learns both instance
detection and pixelwise segmentation simultaneously can successfully lead
towards an operational pathway. We evaluate the model's plume detectability
across emission source types and regions, identifying thresholds for
operational deployment. Finally, we provide analysis-ready data, models, and
source code for reproducibility, and work to define a set of best practices and
validation standards to facilitate future contributions to the field.
\\ ( https://arxiv.org/abs/2505.21806 ,  32595kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21807
Date: Tue, 27 May 2025 22:23:11 GMT   (166kb)

Title: TabReason: A Reinforcement Learning-Enhanced Reasoning LLM for
  Explainable Tabular Data Prediction
Authors: Tommy Xu, Zhitian Zhang, Xiangyu Sun, Lauren Kelly Zung, Hossein
  Hajimirsadeghi, Greg Mori
Categories: cs.LG
\\
  Predictive modeling on tabular data is the cornerstone of many real-world
applications. Although gradient boosting machines and some recent deep models
achieve strong performance on tabular data, they often lack interpretability.
On the other hand, large language models (LLMs) have demonstrated powerful
capabilities to generate human-like reasoning and explanations, but remain
under-performed for tabular data prediction. In this paper, we propose a new
approach that leverages reasoning-based LLMs, trained using reinforcement
learning, to perform more accurate and explainable predictions on tabular data.
Our method introduces custom reward functions that guide the model not only
toward high prediction accuracy but also toward human-understandable reasons
for its predictions. Experimental results show that our model achieves
promising performance on financial benchmark datasets, outperforming most
existing LLMs.
\\ ( https://arxiv.org/abs/2505.21807 ,  166kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21813
Date: Tue, 27 May 2025 22:44:36 GMT   (124kb,D)

Title: Optimizing Data Augmentation through Bayesian Model Selection
Authors: Madi Matymov (1), Ba-Hien Tran (2), Michael Kampffmeyer (3 and 4),
  Markus Heinonen (5), Maurizio Filippone (1) ((1) KAUST, (2) Huawei Paris
  Research Center, (3) UiT The Arctic University of Norway, (4) Norwegian
  Computing Center, (5) Aalto University)
Categories: cs.LG stat.ML
Comments: 26 pages, 3 figures
MSC-class: 62F15, 68T07 (Primary) 62M45, 62C10, 65C60 (Secondary)
\\
  Data Augmentation (DA) has become an essential tool to improve robustness and
generalization of modern machine learning. However, when deciding on DA
strategies it is critical to choose parameters carefully, and this can be a
daunting task which is traditionally left to trial-and-error or expensive
optimization based on validation performance. In this paper, we counter these
limitations by proposing a novel framework for optimizing DA. In particular, we
take a probabilistic view of DA, which leads to the interpretation of
augmentation parameters as model (hyper)-parameters, and the optimization of
the marginal likelihood with respect to these parameters as a Bayesian model
selection problem. Due to its intractability, we derive a tractable Evidence
Lower BOund (ELBO), which allows us to optimize augmentation parameters jointly
with model parameters. We provide extensive theoretical results on variational
approximation quality, generalization guarantees, invariance properties, and
connections to empirical Bayes. Through experiments on computer vision tasks,
we show that our approach improves calibration and yields robust performance
over fixed or no augmentation. Our work provides a rigorous foundation for
optimizing DA through Bayesian principles with significant potential for robust
machine learning.
\\ ( https://arxiv.org/abs/2505.21813 ,  124kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21824
Date: Tue, 27 May 2025 23:22:39 GMT   (1025kb)

Title: Unsupervised Latent Pattern Analysis for Estimating Type 2 Diabetes Risk
  in Undiagnosed Populations
Authors: Praveen Kumar, Vincent T. Metzger, Scott A. Malec
Categories: cs.LG stat.AP
\\
  The global prevalence of diabetes, particularly type 2 diabetes mellitus
(T2DM), is rapidly increasing, posing significant health and economic
challenges. T2DM not only disrupts blood glucose regulation but also damages
vital organs such as the heart, kidneys, eyes, nerves, and blood vessels,
leading to substantial morbidity and mortality. In the US alone, the economic
burden of diagnosed diabetes exceeded \$400 billion in 2022. Early detection of
individuals at risk is critical to mitigating these impacts. While machine
learning approaches for T2DM prediction are increasingly adopted, many rely on
supervised learning, which is often limited by the lack of confirmed negative
cases. To address this limitation, we propose a novel unsupervised framework
that integrates Non-negative Matrix Factorization (NMF) with statistical
techniques to identify individuals at risk of developing T2DM. Our method
identifies latent patterns of multimorbidity and polypharmacy among diagnosed
T2DM patients and applies these patterns to estimate the T2DM risk in
undiagnosed individuals. By leveraging data-driven insights from comorbidity
and medication usage, our approach provides an interpretable and scalable
solution that can assist healthcare providers in implementing timely
interventions, ultimately improving patient outcomes and potentially reducing
the future health and economic burden of T2DM.
\\ ( https://arxiv.org/abs/2505.21824 ,  1025kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21825
Date: Tue, 27 May 2025 23:23:34 GMT   (767kb)

Title: Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many
  Short Ones
Authors: Parsa Mirtaheri, Ezra Edelman, Samy Jelassi, Eran Malach, Enric
  Boix-Adsera
Categories: cs.LG cs.AI cs.CL
\\
  Inference-time computation has emerged as a promising scaling axis for
improving large language model reasoning. However, despite yielding impressive
performance, the optimal allocation of inference-time computation remains
poorly understood. A central question is whether to prioritize sequential
scaling (e.g., longer chains of thought) or parallel scaling (e.g., majority
voting across multiple short chains of thought). In this work, we seek to
illuminate the landscape of test-time scaling by demonstrating the existence of
reasoning settings where sequential scaling offers an exponential advantage
over parallel scaling. These settings are based on graph connectivity problems
in challenging distributions of graphs. We validate our theoretical findings
with comprehensive experiments across a range of language models, including
models trained from scratch for graph connectivity with different chain of
thought strategies as well as large reasoning models.
\\ ( https://arxiv.org/abs/2505.21825 ,  767kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21829
Date: Tue, 27 May 2025 23:30:18 GMT   (1203kb)

Title: In Search of Adam's Secret Sauce
Authors: Antonio Orvieto, Robert Gower
Categories: cs.LG
\\
  Understanding the remarkable efficacy of Adam when training transformer-based
language models has become a central research topic within the optimization
community. To gain deeper insights, several simplifications of Adam have been
proposed, such as the signed gradient and signed momentum methods. In this
work, we conduct an extensive empirical study - training over 1,300 language
models across different data configurations and scales - comparing Adam to
several known simplified variants. We find that signed momentum methods are
faster than SGD, but consistently underperform relative to Adam, even after
careful tuning of momentum, clipping setting and learning rates. However, our
analysis reveals a compelling option that preserves near-optimal performance
while allowing for new insightful reformulations: constraining the Adam
momentum parameters to be equal. Beyond robust performance, this choice affords
new theoretical insights, highlights the "secret sauce" on top of signed
momentum, and grants a precise statistical interpretation: we show that Adam in
this setting implements a natural online algorithm for estimating the mean and
variance of gradients-one that arises from a mean-field Gaussian variational
inference perspective.
\\ ( https://arxiv.org/abs/2505.21829 ,  1203kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21835
Date: Tue, 27 May 2025 23:49:35 GMT   (199kb,D)

Title: TuneComp: Joint Fine-tuning and Compression for Large Foundation Models
Authors: Xiangyu Chen, Jing Liu, Ye Wang, Matthew Brand, Pu (Perry) Wang,
  Toshiaki Koike-Akino
Categories: cs.LG cs.AI
Comments: Preliminary Work
\\
  To reduce model size during post-training, compression methods, including
knowledge distillation, low-rank approximation, and pruning, are often applied
after fine-tuning the model. However, sequential fine-tuning and compression
sacrifices performance, while creating a larger than necessary model as an
intermediate step. In this work, we aim to reduce this gap, by directly
constructing a smaller model while guided by the downstream task. We propose to
jointly fine-tune and compress the model by gradually distilling it to a pruned
low-rank structure. Experiments demonstrate that joint fine-tuning and
compression significantly outperforms other sequential compression methods.
\\ ( https://arxiv.org/abs/2505.21835 ,  199kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21841
Date: Wed, 28 May 2025 00:16:34 GMT   (749kb)

Title: An Optimistic Algorithm for online CMDPS with Anytime Adversarial
  Constraints
Authors: Jiahui Zhu, Kihyun Yu, Dabeen Lee, Xin Liu, Honghao Wei
Categories: cs.LG cs.AI
Comments: Proceedings of the 41 st International Conference on Machine Learning
\\
  Online safe reinforcement learning (RL) plays a key role in dynamic
environments, with applications in autonomous driving, robotics, and
cybersecurity. The objective is to learn optimal policies that maximize rewards
while satisfying safety constraints modeled by constrained Markov decision
processes (CMDPs). Existing methods achieve sublinear regret under stochastic
constraints but often fail in adversarial settings, where constraints are
unknown, time-varying, and potentially adversarially designed. In this paper,
we propose the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, the
first to address online CMDPs with anytime adversarial constraints. OMDPD
achieves optimal regret O(sqrt(K)) and strong constraint violation O(sqrt(K))
without relying on Slater's condition or the existence of a strictly known safe
policy. We further show that access to accurate estimates of rewards and
transitions can further improve these bounds. Our results offer practical
guarantees for safe decision-making in adversarial environments.
\\ ( https://arxiv.org/abs/2505.21841 ,  749kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21852
Date: Wed, 28 May 2025 00:48:20 GMT   (265kb)

Title: A Provable Approach for End-to-End Safe Reinforcement Learning
Authors: Akifumi Wachi, Kohei Miyaguchi, Takumi Tanabe, Rei Sato, Youhei
  Akimoto
Categories: cs.LG cs.AI cs.IT cs.RO math.IT
Comments: 27 pages
\\
  A longstanding goal in safe reinforcement learning (RL) is a method to ensure
the safety of a policy throughout the entire process, from learning to
operation. However, existing safe RL paradigms inherently struggle to achieve
this objective. We propose a method, called Provably Lifetime Safe RL (PLS),
that integrates offline safe RL with safe policy deployment to address this
challenge. Our proposed method learns a policy offline using return-conditioned
supervised learning and then deploys the resulting policy while cautiously
optimizing a limited set of parameters, known as target returns, using Gaussian
processes (GPs). Theoretically, we justify the use of GPs by analyzing the
mathematical relationship between target and actual returns. We then prove that
PLS finds near-optimal target returns while guaranteeing safety with high
probability. Empirically, we demonstrate that PLS outperforms baselines both in
safety and reward performance, thereby achieving the longstanding goal to
obtain high rewards while ensuring the safety of a policy throughout the
lifetime from learning to operation.
\\ ( https://arxiv.org/abs/2505.21852 ,  265kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21857
Date: Wed, 28 May 2025 01:03:28 GMT   (270kb)

Title: Revisiting Bayesian Model Averaging in the Era of Foundation Models
Authors: Mijung Park
Categories: cs.LG stat.ML
\\
  We revisit the classical, full-fledged Bayesian model averaging (BMA)
paradigm to ensemble pre-trained and/or lightly-finetuned foundation models to
enhance the classification performance on image and text data. To make BMA
tractable under foundation models, we introduce trainable linear classifiers
that take frozen features from the pre-trained foundation models as inputs. The
model posteriors over the linear classifiers tell us which linear heads and
frozen features are better suited for a given dataset, resulting in a
principled model ensembling method. Furthermore, we propose a computationally
cheaper, optimizable model averaging scheme (OMA). In OMA, we directly optimize
the model ensemble weights, just like those weights based on model posterior
distributions in BMA, by reducing the amount of surprise (expected entropy of
the predictions) we get from predictions of ensembled models. With the rapid
development of foundation models, these approaches will enable the
incorporation of future, possibly significantly better foundation models to
enhance the performance of challenging classification tasks.
\\ ( https://arxiv.org/abs/2505.21857 ,  270kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21877
Date: Wed, 28 May 2025 01:46:34 GMT   (3270kb)

Title: Hybrid Batch Normalisation: Resolving the Dilemma of Batch Normalisation
  in Federated Learning
Authors: Hongyao Chen, Tianyang Xu, Xiaojun Wu, Josef Kittler
Categories: cs.LG cs.DC
\\
  Batch Normalisation (BN) is widely used in conventional deep neural network
training to harmonise the input-output distributions for each batch of data.
However, federated learning, a distributed learning paradigm, faces the
challenge of dealing with non-independent and identically distributed data
among the client nodes. Due to the lack of a coherent methodology for updating
BN statistical parameters, standard BN degrades the federated learning
performance. To this end, it is urgent to explore an alternative normalisation
solution for federated learning. In this work, we resolve the dilemma of the BN
layer in federated learning by developing a customised normalisation approach,
Hybrid Batch Normalisation (HBN). HBN separates the update of statistical
parameters (i.e. , means and variances used for evaluation) from that of
learnable parameters (i.e. , parameters that require gradient updates),
obtaining unbiased estimates of global statistical parameters in distributed
scenarios. In contrast with the existing solutions, we emphasise the supportive
power of global statistics for federated learning. The HBN layer introduces a
learnable hybrid distribution factor, allowing each computing node to
adaptively mix the statistical parameters of the current batch with the global
statistics. Our HBN can serve as a powerful plugin to advance federated
learning performance. It reflects promising merits across a wide range of
federated learning settings, especially for small batch sizes and heterogeneous
data.
\\ ( https://arxiv.org/abs/2505.21877 ,  3270kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21882
Date: Wed, 28 May 2025 01:58:44 GMT   (40168kb)

Title: HydraNet: Momentum-Driven State Space Duality for Multi-Granularity
  Tennis Tournaments Analysis
Authors: Ruijie Li, Xiang Zhao, Qiao Ning, Shikai Guo
Categories: cs.LG
Comments: 14 pages, 9 figures (including subfigures), 5 tables. This is the
  first work to explore and effectively model momentum across multiple
  granularities in professional tennis tournaments
\\
  In tennis tournaments, momentum, a critical yet elusive phenomenon, reflects
the dynamic shifts in performance of athletes that can decisively influence
match outcomes. Despite its significance, momentum in terms of effective
modeling and multi-granularity analysis across points, games, sets, and matches
in tennis tournaments remains underexplored. In this study, we define a novel
Momentum Score (MS) metric to quantify a player's momentum level in
multi-granularity tennis tournaments, and design HydraNet, a momentum-driven
state-space duality-based framework, to model MS by integrating thirty-two
heterogeneous dimensions of athletes performance in serve, return, psychology
and fatigue. HydraNet integrates a Hydra module, which builds upon a
state-space duality (SSD) framework, capturing explicit momentum with a
sliding-window mechanism and implicit momentum through cross-game state
propagation. It also introduces a novel Versus Learning method to better
enhance the adversarial nature of momentum between the two athletes at a macro
level, along with a Collaborative-Adversarial Attention Mechanism (CAAM) for
capturing and integrating intra-player and inter-player dynamic momentum at a
micro level. Additionally, we construct a million-level tennis cross-tournament
dataset spanning from 2012-2023 Wimbledon and 2013-2023 US Open, and validate
the multi-granularity modeling capability of HydraNet for the MS metric on this
dataset. Extensive experimental evaluations demonstrate that the MS metric
constructed by the HydraNet framework provides actionable insights into how
momentum impacts outcomes at different granularities, establishing a new
foundation for momentum modeling and sports analysis. To the best of our
knowledge. The source code and datasets are available at
https://github.com/ReyJerry/HydraNet.
\\ ( https://arxiv.org/abs/2505.21882 ,  40168kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21893
Date: Wed, 28 May 2025 02:11:56 GMT   (45459kb,D)

Title: SDPO: Importance-Sampled Direct Preference Optimization for Stable
  Diffusion Training
Authors: Xiaomeng Yang, Zhiyu Tan, Junyan Wang, Zhijian Zhou, Hao Li
Categories: cs.LG cs.AI
\\
  Preference learning has become a central technique for aligning generative
models with human expectations. Recently, it has been extended to diffusion
models through methods like Direct Preference Optimization (DPO). However,
existing approaches such as Diffusion-DPO suffer from two key challenges:
timestep-dependent instability, caused by a mismatch between the reverse and
forward diffusion processes and by high gradient variance in early noisy
timesteps, and off-policy bias arising from the mismatch between optimization
and data collection policies. We begin by analyzing the reverse diffusion
trajectory and observe that instability primarily occurs at early timesteps
with low importance weights. To address these issues, we first propose
DPO-C\&M, a practical strategy that improves stability by clipping and masking
uninformative timesteps while partially mitigating off-policy bias. Building on
this, we introduce SDPO (Importance-Sampled Direct Preference Optimization), a
principled framework that incorporates importance sampling into the objective
to fully correct for off-policy bias and emphasize informative updates during
the diffusion process. Experiments on CogVideoX-2B, CogVideoX-5B, and
Wan2.1-1.3B demonstrate that both methods outperform standard Diffusion-DPO,
with SDPO achieving superior VBench scores, human preference alignment, and
training robustness. These results highlight the importance of timestep-aware,
distribution-corrected optimization in diffusion-based preference learning.
\\ ( https://arxiv.org/abs/2505.21893 ,  45459kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21895
Date: Wed, 28 May 2025 02:15:15 GMT   (8126kb)

Title: Compressing Sine-Activated Low-Rank Adapters through Post-Training
  Quantization
Authors: Cameron Gordon, Yiping Ji, Hemanth Saratchandran, Paul Albert, Simon
  Lucey
Categories: cs.LG cs.AI
Comments: 23 pages, 9 figures
\\
  Low-Rank Adaptation (LoRA) has become a standard approach for
parameter-efficient fine-tuning, offering substantial reductions in trainable
parameters by modeling updates as the product of two low-rank matrices. While
effective, the low-rank constraint inherently limits representational capacity,
often resulting in reduced performance compared to full-rank fine-tuning.
Recent work by Ji et al. (2025) has addressed this limitation by applying a
fixed-frequency sinusoidal transformation to low-rank adapters, increasing
their stable rank without introducing additional parameters. This raises a
crucial question: can the same sine-activated technique be successfully applied
within the context of Post-Training Quantization to retain benefits even after
model compression? In this paper, we investigate this question by extending the
sinusoidal transformation framework to quantized LoRA adapters. We develop a
theoretical analysis showing that the stable rank of a quantized adapter is
tightly linked to that of its full-precision counterpart, motivating the use of
such rank-enhancing functions even under quantization. Our results demonstrate
that the expressivity gains from a sinusoidal non-linearity persist after
quantization, yielding highly compressed adapters with negligible loss in
performance. We validate our approach across a range of fine-tuning tasks for
language, vision and text-to-image generation achieving significant memory
savings while maintaining competitive accuracy.
\\ ( https://arxiv.org/abs/2505.21895 ,  8126kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21908
Date: Wed, 28 May 2025 02:54:07 GMT   (464kb)

Title: Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An
  Empirical Study on Diagnosis-Related Group Coding
Authors: Hanyin Wang, Zhenbang Wu, Gururaj Kolar, Hariprasad Korsapati, Brian
  Bartlett, Bryan Hull, Jimeng Sun
Categories: cs.LG cs.AI
\\
  Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement
and operations but require labor-intensive assignment. Large Language Models
(LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of
the task: pretraining corpora rarely contain private clinical or billing data.
We introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL)
for automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained
with Group Relative Policy Optimization (GRPO) using rule-based rewards,
DRG-Sapphire introduces a series of RL enhancements to address domain-specific
challenges not seen in previous mathematical tasks. Our model achieves
state-of-the-art accuracy on the MIMIC-IV benchmark and generates
physician-validated reasoning for DRG assignments, significantly enhancing
explainability. Our study further sheds light on broader challenges of applying
RL to knowledge-intensive, OOD tasks. We observe that RL performance scales
approximately linearly with the logarithm of the number of supervised
fine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally
constrained by the domain knowledge encoded in the base model. For OOD tasks
like DRG coding, strong RL performance requires sufficient knowledge infusion
prior to RL. Consequently, scaling SFT may be more effective and
computationally efficient than scaling RL alone for such tasks.
\\ ( https://arxiv.org/abs/2505.21908 ,  464kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21910
Date: Wed, 28 May 2025 02:55:28 GMT   (8422kb,D)

Title: Taming Transformer Without Using Learning Rate Warmup
Authors: Xianbiao Qi, Yelin He, Jiaquan Ye, Chun-Guang Li, Bojia Zi, Xili Dai,
  Qin Zou, Rong Xiao
Categories: cs.LG cs.CV
Comments: This paper is published as a conference paper at ICLR 2025
\\
  Scaling Transformer to a large scale without using some technical tricks such
as learning rate warump and using an obviously lower learning rate is an
extremely challenging task, and is increasingly gaining more attention. In this
paper, we provide a theoretical analysis for the process of training
Transformer and reveal the rationale behind the model crash phenomenon in the
training process, termed \textit{spectral energy concentration} of
${\bW_q}^{\top} \bW_k$, which is the reason for a malignant entropy collapse,
where ${\bW_q}$ and $\bW_k$ are the projection matrices for the query and the
key in Transformer, respectively. To remedy this problem, motivated by
\textit{Weyl's Inequality}, we present a novel optimization strategy, \ie,
making the weight updating in successive steps smooth -- if the ratio
$\frac{\sigma_{1}(\nabla \bW_t)}{\sigma_{1}(\bW_{t-1})}$ is larger than a
threshold, we will automatically bound the learning rate to a weighted multiple
of $\frac{\sigma_{1}(\bW_{t-1})}{\sigma_{1}(\nabla \bW_t)}$, where $\nabla
\bW_t$ is the updating quantity in step $t$. Such an optimization strategy can
prevent spectral energy concentration to only a few directions, and thus can
avoid malignant entropy collapse which will trigger the model crash. We conduct
extensive experiments using ViT, Swin-Transformer and GPT, showing that our
optimization strategy can effectively and stably train these Transformers
without using learning rate warmup.
\\ ( https://arxiv.org/abs/2505.21910 ,  8422kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21918
Date: Wed, 28 May 2025 03:04:13 GMT   (527kb)

Title: Self-supervised Learning Method Using Transformer for Multi-dimensional
  Sensor Data Processing
Authors: Haruki Kai and Tsuyoshi Okita
Categories: cs.LG cs.AI
Comments: 25 pages, 4 figures
\\
  We developed a deep learning algorithm for human activity recognition using
sensor signals as input. In this study, we built a pretrained language model
based on the Transformer architecture, which is widely used in natural language
processing. By leveraging this pretrained model, we aimed to improve
performance on the downstream task of human activity recognition. While this
task can be addressed using a vanilla Transformer, we propose an enhanced
n-dimensional numerical processing Transformer that incorporates three key
features: embedding n-dimensional numerical data through a linear layer,
binning-based pre-processing, and a linear transformation in the output layer.
We evaluated the effectiveness of our proposed model across five different
datasets. Compared to the vanilla Transformer, our model demonstrated 10%-15%
improvements in accuracy.
\\ ( https://arxiv.org/abs/2505.21918 ,  527kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21923
Date: Wed, 28 May 2025 03:16:08 GMT   (20824kb,D)

Title: FALCON: An ML Framework for Fully Automated Layout-Constrained Analog
  Circuit Design
Authors: Asal Mehradfar, Xuzhe Zhao, Yilun Huang, Emir Ceyani, Yankai Yang,
  Shihao Han, Hamidreza Aghasi, Salman Avestimehr
Categories: cs.LG cs.AI cs.AR cs.CE
\\
  Designing analog circuits from performance specifications is a complex,
multi-stage process encompassing topology selection, parameter inference, and
layout feasibility. We introduce FALCON, a unified machine learning framework
that enables fully automated, specification-driven analog circuit synthesis
through topology selection and layout-constrained optimization. Given a target
performance, FALCON first selects an appropriate circuit topology using a
performance-driven classifier guided by human design heuristics. Next, it
employs a custom, edge-centric graph neural network trained to map circuit
topology and parameters to performance, enabling gradient-based parameter
inference through the learned forward model. This inference is guided by a
differentiable layout cost, derived from analytical equations capturing
parasitic and frequency-dependent effects, and constrained by design rules. We
train and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave
circuits, generated and simulated using Cadence Spectre across 20
expert-designed topologies. Through this evaluation, FALCON demonstrates >99\%
accuracy in topology inference, <10\% relative error in performance prediction,
and efficient layout-aware design that completes in under 1 second per
instance. Together, these results position FALCON as a practical and extensible
foundation model for end-to-end analog circuit design automation.
\\ ( https://arxiv.org/abs/2505.21923 ,  20824kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21930
Date: Wed, 28 May 2025 03:27:08 GMT   (3332kb,D)

Title: Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets
Authors: Dongyue Li, Ziniu Zhang, Lu Wang, and Hongyang R. Zhang
Categories: cs.LG cs.CL
Comments: 17 pages. To appear in ACL'25
\\
  This paper develops an ensemble method for fine-tuning a language model to
multiple datasets. Existing methods, such as quantized LoRA (QLoRA), are
efficient when adapting to a single dataset. When training on multiple datasets
of different tasks, a common setup in practice, it remains unclear how to
design an efficient adaptation for fine-tuning language models. We propose to
use an ensemble of multiple smaller adapters instead of a single adapter per
task. We design an efficient algorithm that partitions $n$ datasets into $m$
groups, where $m$ is typically much smaller than $n$ in practice, and train one
adapter for each group before taking a weighted combination to form the
ensemble. The algorithm leverages a first-order approximation property of
low-rank adaptation to quickly obtain the fine-tuning performances of dataset
combinations since methods like LoRA stay close to the base model. Hence, we
use the gradients of the base model to estimate its behavior during
fine-tuning. Empirically, this approximation holds with less than $1\%$ error
on models with up to $34$ billion parameters, leading to an estimation of true
fine-tuning performances under $5\%$ error while speeding up computation
compared to base fine-tuning by $105$ times. When applied to fine-tune Llama
and GPT models on ten text classification tasks, our approach provides up to
$10\%$ higher average test accuracy over QLoRA, with only $9\%$ more FLOPs. On
a Llama model with $34$ billion parameters, an ensemble of QLoRA increases test
accuracy by $3\%$ compared to QLoRA, with only $8\%$ more FLOPs.
\\ ( https://arxiv.org/abs/2505.21930 ,  3332kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21938
Date: Wed, 28 May 2025 03:47:13 GMT   (584kb)

Title: Practical Adversarial Attacks on Stochastic Bandits via Fake Data
  Injection
Authors: Qirun Zeng, Eric He, Richard Hoffmann, Xuchuang Wang, Jinhang Zuo
Categories: cs.LG cs.AI cs.CR
\\
  Adversarial attacks on stochastic bandits have traditionally relied on some
unrealistic assumptions, such as per-round reward manipulation and unbounded
perturbations, limiting their relevance to real-world systems. We propose a
more practical threat model, Fake Data Injection, which reflects realistic
adversarial constraints: the attacker can inject only a limited number of
bounded fake feedback samples into the learner's history, simulating legitimate
interactions. We design efficient attack strategies under this model,
explicitly addressing both magnitude constraints (on reward values) and
temporal constraints (on when and how often data can be injected). Our
theoretical analysis shows that these attacks can mislead both Upper Confidence
Bound (UCB) and Thompson Sampling algorithms into selecting a target arm in
nearly all rounds while incurring only sublinear attack cost. Experiments on
synthetic and real-world datasets validate the effectiveness of our strategies,
revealing significant vulnerabilities in widely used stochastic bandit
algorithms under practical adversarial scenarios.
\\ ( https://arxiv.org/abs/2505.21938 ,  584kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21942
Date: Wed, 28 May 2025 03:52:34 GMT   (712kb)

Title: Continual Learning Beyond Experience Rehearsal and Full Model Surrogates
Authors: Prashant Bhat, Laurens Niesten, Elahe Arani, Bahram Zonooz
Categories: cs.LG stat.ML
Comments: 23 pages, 9 figures
\\
  Continual learning (CL) has remained a significant challenge for deep neural
networks as learning new tasks erases previously acquired knowledge, either
partially or completely. Existing solutions often rely on experience rehearsal
or full model surrogates to mitigate CF. While effective, these approaches
introduce substantial memory and computational overhead, limiting their
scalability and applicability in real-world scenarios. To address this, we
propose SPARC, a scalable CL approach that eliminates the need for experience
rehearsal and full-model surrogates. By effectively combining task-specific
working memories and task-agnostic semantic memory for cross-task knowledge
consolidation, SPARC results in a remarkable parameter efficiency, using only
6% of the parameters required by full-model surrogates. Despite its lightweight
design, SPARC achieves superior performance on Seq-TinyImageNet and matches
rehearsal-based methods on various CL benchmarks. Additionally, weight
re-normalization in the classification layer mitigates task-specific biases,
establishing SPARC as a practical and scalable solution for CL under stringent
efficiency constraints.
\\ ( https://arxiv.org/abs/2505.21942 ,  712kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21944
Date: Wed, 28 May 2025 03:55:05 GMT   (3579kb)

Title: Stochastic Primal-Dual Double Block-Coordinate for Two-way Partial AUC
  Maximization
Authors: Linli Zhou, Bokun Wang, My T. Thai, Tianbao Yang
Categories: cs.LG
\\
  Two-way partial AUC (TPAUC) is a critical performance metric for binary
classification with imbalanced data, as it focuses on specific ranges of the
true positive rate (TPR) and false positive rate (FPR). However, stochastic
algorithms for TPAUC optimization remain under-explored, with existing methods
either limited to approximated TPAUC loss functions or burdened by sub-optimal
complexities. To overcome these limitations, we introduce two innovative
stochastic primal-dual double block-coordinate algorithms for TPAUC
maximization. These algorithms utilize stochastic block-coordinate updates for
both the primal and dual variables, catering to both convex and non-convex
settings. We provide theoretical convergence rate analyses, demonstrating
significant improvements over prior approaches. Our experimental results, based
on multiple benchmark datasets, validate the superior performance of our
algorithms, showcasing faster convergence and better generalization. This work
advances the state of the art in TPAUC optimization and offers practical tools
for real-world machine learning applications.
\\ ( https://arxiv.org/abs/2505.21944 ,  3579kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21959
Date: Wed, 28 May 2025 04:23:12 GMT   (8132kb,D)

Title: EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language
  Model Ensembles
Authors: Aakriti Agrawal, Mucong Ding, Zora Che, Chenghao Deng, Anirudh
  Satheesh, Bang An, Bayan Bruss, John Langford, Furong Huang
Categories: cs.LG cs.CL
Comments: Superalignment. arXiv admin note: substantial text overlap with
  arXiv:2410.04571
\\
  With Large Language Models (LLMs) rapidly approaching and potentially
surpassing human-level performance, it has become imperative to develop
approaches capable of effectively supervising and enhancing these powerful
models using smaller, human-level models exposed to only human-level data. We
address this critical weak-to-strong (W2S) generalization challenge by
proposing a novel method aimed at improving weak experts, by training on the
same limited human-level data, enabling them to generalize to complex,
super-human-level tasks. Our approach, called \textbf{EnsemW2S}, employs a
token-level ensemble strategy that iteratively combines multiple weak experts,
systematically addressing the shortcomings identified in preceding iterations.
By continuously refining these weak models, we significantly enhance their
collective ability to supervise stronger student models. We extensively
evaluate the generalization performance of both the ensemble of weak experts
and the subsequent strong student model across in-distribution (ID) and
out-of-distribution (OOD) datasets. For OOD, we specifically introduce question
difficulty as an additional dimension for defining distributional shifts. Our
empirical results demonstrate notable improvements, achieving 4\%, and 3.2\%
improvements on ID datasets and, upto 6\% and 2.28\% on OOD datasets for
experts and student models respectively, underscoring the effectiveness of our
proposed method in advancing W2S generalization.
\\ ( https://arxiv.org/abs/2505.21959 ,  8132kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21972
Date: Wed, 28 May 2025 04:50:41 GMT   (1250kb)

Title: Judging LLMs on a Simplex
Authors: Patrick Vossler, Fan Xia, Yifan Mai, Jean Feng
Categories: cs.LG cs.AI stat.ML
Comments: 28 pages, 7 figures
\\
  Automated evaluation of free-form outputs from large language models (LLMs)
is challenging because many distinct answers can be equally valid. A common
practice is to use LLMs themselves as judges, but the theoretical properties of
this approach are not yet well understood. We show that a geometric framework
that represents both judges and candidates as points on a probability simplex
can provide helpful insight on what is or is not identifiable using LLM judges.
Our theoretical analysis uncovers a "phase transition" in ranking
identifiability: for binary scoring systems, true rankings are identifiable
even with weak judges under mild assumptions, while rankings become
non-identifiable for three or more scoring levels even with infinite data,
absent additional prior knowledge. This non-identifiability highlights how
uncertainty in rankings stems from not only aleatoric uncertainty (i.e.,
inherent stochasticity in the data) but also epistemic uncertainty regarding
which assumptions hold, an aspect that has received limited attention until
now. To integrate both types of uncertainty, we use Bayesian inference to
encode assumptions as priors and conduct sensitivity analysis of ranking
estimates and credible intervals. Empirical evaluations across multiple
benchmarks demonstrate that Bayesian inference yields more accurate rankings
and substantially improves coverage rates. These results underscore the
importance of taking a more holistic approach to uncertainty quantification
when using LLMs as judges.
\\ ( https://arxiv.org/abs/2505.21972 ,  1250kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21974
Date: Wed, 28 May 2025 05:00:50 GMT   (13734kb,D)

Title: BOFormer: Learning to Solve Multi-Objective Bayesian Optimization via
  Non-Markovian RL
Authors: Yu-Heng Hung, Kai-Jie Lin, Yu-Heng Lin, Chien-YiWang, Cheng Sun,
  Ping-Chun Hsieh
Categories: cs.LG
\\
  Bayesian optimization (BO) offers an efficient pipeline for optimizing
black-box functions with the help of a Gaussian process prior and an
acquisition function (AF). Recently, in the context of single-objective BO,
learning-based AFs witnessed promising empirical results given its favorable
non-myopic nature. Despite this, the direct extension of these approaches to
multi-objective Bayesian optimization (MOBO) suffer from the
\textit{hypervolume identifiability issue}, which results from the
non-Markovian nature of MOBO problems. To tackle this, inspired by the
non-Markovian RL literature and the success of Transformers in language
modeling, we present a generalized deep Q-learning framework and propose
\textit{BOFormer}, which substantiates this framework for MOBO via sequence
modeling. Through extensive evaluation, we demonstrate that BOFormer constantly
outperforms the benchmark rule-based and learning-based algorithms in various
synthetic MOBO and real-world multi-objective hyperparameter optimization
problems. We have made the source code publicly available to encourage further
research in this direction.
\\ ( https://arxiv.org/abs/2505.21974 ,  13734kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21978
Date: Wed, 28 May 2025 05:11:59 GMT   (983kb)

Title: Two-Stage Feature Generation with Transformer and Reinforcement Learning
Authors: Wanfu Gao, Zengyao Man, Zebin He, Yuhao Tang, Jun Gao and Kunpeng Liu
Categories: cs.LG
\\
  Feature generation is a critical step in machine learning, aiming to enhance
model performance by capturing complex relationships within the data and
generating meaningful new features. Traditional feature generation methods
heavily rely on domain expertise and manual intervention, making the process
labor-intensive and challenging to adapt to different scenarios. Although
automated feature generation techniques address these issues to some extent,
they often face challenges such as feature redundancy, inefficiency in feature
space exploration, and limited adaptability to diverse datasets and tasks. To
address these problems, we propose a Two-Stage Feature Generation (TSFG)
framework, which integrates a Transformer-based encoder-decoder architecture
with Proximal Policy Optimization (PPO). The encoder-decoder model in TSFG
leverages the Transformer's self-attention mechanism to efficiently represent
and transform features, capturing complex dependencies within the data. PPO
further enhances TSFG by dynamically adjusting the feature generation strategy
based on task-specific feedback, optimizing the process for improved
performance and adaptability. TSFG dynamically generates high-quality feature
sets, significantly improving the predictive performance of machine learning
models. Experimental results demonstrate that TSFG outperforms existing
state-of-the-art methods in terms of feature quality and adaptability.
\\ ( https://arxiv.org/abs/2505.21978 ,  983kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21987
Date: Wed, 28 May 2025 05:25:16 GMT   (619kb,D)

Title: ACE: Exploring Activation Cosine Similarity and Variance for Accurate
  and Calibration-Efficient LLM Pruning
Authors: Zhendong Mi, Zhenglun Kong, Geng Yuan, Shaoyi Huang
Categories: cs.LG
Comments: 9 pages, 2 figures, 13 tables
ACM-class: I.2.6; I.2.7
\\
  With the rapid expansion of large language models (LLMs), the demand for
memory and computational resources has grown significantly. Recent advances in
LLM pruning aim to reduce the size and computational cost of these models.
However, existing methods often suffer from either suboptimal pruning
performance or low time efficiency during the pruning process. In this work, we
propose an efficient and effective pruning method that simultaneously achieves
high pruning performance and fast pruning speed with improved calibration
efficiency. Our approach introduces two key innovations: (1) An activation
cosine similarity loss-guided pruning metric, which considers the angular
deviation of the output activation between the dense and pruned models. (2) An
activation variance-guided pruning metric, which helps preserve semantic
distinctions in output activations after pruning, enabling effective pruning
with shorter input sequences. These two components can be readily combined to
enhance LLM pruning in both accuracy and efficiency. Experimental results show
that our method achieves up to an 18% reduction in perplexity and up to 63%
decrease in pruning time on prevalent LLMs such as LLaMA, LLaMA-2, and OPT.
\\ ( https://arxiv.org/abs/2505.21987 ,  619kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22014
Date: Wed, 28 May 2025 06:23:19 GMT   (2040kb)

Title: Learning in Compact Spaces with Approximately Normalized Transformers
Authors: J\"org K.H. Franke, Urs Spiegelhalter, Marianna Nezhurina, Jenia
  Jitsev, Frank Hutter, Michael Hefenbrock
Categories: cs.LG
Comments: Preprint
\\
  In deep learning, regularization and normalization are common solutions for
challenges such as overfitting, numerical instabilities, and the increasing
variance in the residual stream. An alternative approach is to force all
parameters and representations to lie on a hypersphere. This removes the need
for regularization and increases convergence speed, but comes with additional
costs. In this work, we propose a more holistic but approximate normalization
(anTransformer). Our approach constrains the norm of parameters and normalizes
all representations via scalar multiplications motivated by the tight
concentration of the norms of high-dimensional random vectors. When applied to
GPT training, we observe a 40% faster convergence compared to models with QK
normalization, with less than 3% additional runtime. Deriving scaling laws for
anGPT, we found our method enables training with larger batch sizes and fewer
hyperparameters, while matching the favorable scaling characteristics of
classic GPT architectures.
\\ ( https://arxiv.org/abs/2505.22014 ,  2040kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22028
Date: Wed, 28 May 2025 06:50:40 GMT   (1920kb)

Title: Weakly-Supervised Contrastive Learning for Imprecise Class Labels
Authors: Zi-Hao Zhou, Jun-Jie Wang, Tong Wei and Min-Ling Zhang
Categories: cs.LG
Comments: 38 pages, 2 figures, 11 tables
\\
  Contrastive learning has achieved remarkable success in learning effective
representations, with supervised contrastive learning often outperforming
self-supervised approaches. However, in real-world scenarios, data annotations
are often ambiguous or inaccurate, meaning that class labels may not reliably
indicate whether two examples belong to the same class. This limitation
restricts the applicability of supervised contrastive learning. To address this
challenge, we introduce the concept of ``continuous semantic similarity'' to
define positive and negative pairs. Instead of directly relying on imprecise
class labels, we measure the semantic similarity between example pairs, which
quantifies how closely they belong to the same category by iteratively refining
weak supervisory signals. Based on this concept, we propose a graph-theoretic
framework for weakly-supervised contrastive learning, where semantic similarity
serves as the graph weights. Our framework is highly versatile and can be
applied to many weakly-supervised learning scenarios. We demonstrate its
effectiveness through experiments in two common settings, i.e., noisy label and
partial label learning, where existing methods can be easily integrated to
significantly improve performance. Theoretically, we establish an error bound
for our approach, showing that it can approximate supervised contrastive
learning under mild conditions. The implementation code is available at
https://github.com/Speechless-10308/WSC.
\\ ( https://arxiv.org/abs/2505.22028 ,  1920kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22041
Date: Wed, 28 May 2025 07:03:46 GMT   (294kb)

Title: Detecting Undesired Process Behavior by Means of Retrieval Augmented
  Generation
Authors: Michael Grohs, Adrian Rebmann, Jana-Rebecca Rehse
Categories: cs.LG
Comments: Accepted at the BPM Forum, located at the International Conference on
  Business Process Management (BPM) 2025
\\
  Conformance checking techniques detect undesired process behavior by
comparing process executions that are recorded in event logs to desired
behavior that is captured in a dedicated process model. If such models are not
available, conformance checking techniques are not applicable, but
organizations might still be interested in detecting undesired behavior in
their processes. To enable this, existing approaches use Large Language Models
(LLMs), assuming that they can learn to distinguish desired from undesired
behavior through fine-tuning. However, fine-tuning is highly resource-intensive
and the fine-tuned LLMs often do not generalize well. To address these
limitations, we propose an approach that requires neither a dedicated process
model nor resource-intensive fine-tuning to detect undesired process behavior.
Instead, we use Retrieval Augmented Generation (RAG) to provide an LLM with
direct access to a knowledge base that contains both desired and undesired
process behavior from other processes, assuming that the LLM can transfer this
knowledge to the process at hand. Our evaluation shows that our approach
outperforms fine-tuned LLMs in detecting undesired behavior, demonstrating that
RAG is a viable alternative to resource-intensive fine-tuning, particularly
when enriched with relevant context from the event log, such as frequent traces
and activities.
\\ ( https://arxiv.org/abs/2505.22041 ,  294kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22042
Date: Wed, 28 May 2025 07:07:02 GMT   (592kb)

Title: Estimating the Effects of Sample Training Orders for Large Language
  Models without Retraining
Authors: Hao Yang, Haoxuan Li, Mengyue Yang, Xu Chen, Mingming Gong
Categories: cs.LG cs.AI
\\
  The order of training samples plays a crucial role in large language models
(LLMs), significantly impacting both their external performance and internal
learning dynamics. Traditional methods for investigating this effect generally
require retraining the model with various sample orders, which is
computationally infeasible for LLMs. In this work, we improve traditional
methods by designing a retraining-free framework. By approximating Adam
optimizer updates with first- and second-order Taylor expansions and utilizing
random projection methods to store intermediate checkpoints, our framework can
efficiently estimate model parameters for arbitrary training sample orders.
Next, we apply our framework to two downstream research problems: (1) Training
curriculum design for LLMs -- we base our retraining-free framework to propose
a novel curriculum learning strategy that augments curriculum proposals with
estimated model performances, enabling more informed sample scheduling. (2)
LLMs' memorization and generalization effect analysis -- we use our
retraining-free framework to estimate how the positions of training samples
influence LLMs' capacity for memorization and generalization. We conduct
extensive experiments to validate the effectiveness of our retraining-free
framework in reproducing the true model performances, and further demonstrate
its potential in optimizing LLM training curricula and analyzing the
memorization and generalization effects of LLMs.
\\ ( https://arxiv.org/abs/2505.22042 ,  592kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22049
Date: Wed, 28 May 2025 07:18:08 GMT   (2716kb)

Title: Differentiable Generalized Sliced Wasserstein Plans
Authors: Laetitia Chapel and Romain Tavenard and Samuel Vaiter
Categories: cs.LG
\\
  Optimal Transport (OT) has attracted significant interest in the machine
learning community, not only for its ability to define meaningful distances
between probability distributions -- such as the Wasserstein distance -- but
also for its formulation of OT plans. Its computational complexity remains a
bottleneck, though, and slicing techniques have been developed to scale OT to
large datasets. Recently, a novel slicing scheme, dubbed min-SWGG, lifts a
single one-dimensional plan back to the original multidimensional space,
finally selecting the slice that yields the lowest Wasserstein distance as an
approximation of the full OT plan. Despite its computational and theoretical
advantages, min-SWGG inherits typical limitations of slicing methods: (i) the
number of required slices grows exponentially with the data dimension, and (ii)
it is constrained to linear projections. Here, we reformulate min-SWGG as a
bilevel optimization problem and propose a differentiable approximation scheme
to efficiently identify the optimal slice, even in high-dimensional settings.
We furthermore define its generalized extension for accommodating to data
living on manifolds. Finally, we demonstrate the practical value of our
approach in various applications, including gradient flows on manifolds and
high-dimensional spaces, as well as a novel sliced OT-based conditional flow
matching for image generation -- where fast computation of transport plans is
essential.
\\ ( https://arxiv.org/abs/2505.22049 ,  2716kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22074
Date: Wed, 28 May 2025 07:55:51 GMT   (1687kb)

Title: The Resurrection of the ReLU
Authors: Co\c{s}ku Can Horuz, Geoffrey Kasenbacher, Saya Higuchi, Sebastian
  Kairat, Jendrik Stoltz, Moritz Pesl, Bernhard A. Moser, Christoph Linse,
  Thomas Martinetz, Sebastian Otte
Categories: cs.LG cs.AI
\\
  Modeling sophisticated activation functions within deep learning
architectures has evolved into a distinct research direction. Functions such as
GELU, SELU, and SiLU offer smooth gradients and improved convergence
properties, making them popular choices in state-of-the-art models. Despite
this trend, the classical ReLU remains appealing due to its simplicity,
inherent sparsity, and other advantageous topological characteristics. However,
ReLU units are prone to becoming irreversibly inactive - a phenomenon known as
the dying ReLU problem - which limits their overall effectiveness. In this
work, we introduce surrogate gradient learning for ReLU (SUGAR) as a novel,
plug-and-play regularizer for deep architectures. SUGAR preserves the standard
ReLU function during the forward pass but replaces its derivative in the
backward pass with a smooth surrogate that avoids zeroing out gradients. We
demonstrate that SUGAR, when paired with a well-chosen surrogate function,
substantially enhances generalization performance over convolutional network
architectures such as VGG-16 and ResNet-18, providing sparser activations while
effectively resurrecting dead ReLUs. Moreover, we show that even in modern
architectures like Conv2NeXt and Swin Transformer - which typically employ GELU
- substituting these with SUGAR yields competitive and even slightly superior
performance. These findings challenge the prevailing notion that advanced
activation functions are necessary for optimal performance. Instead, they
suggest that the conventional ReLU, particularly with appropriate gradient
handling, can serve as a strong, versatile revived classic across a broad range
of deep learning vision models.
\\ ( https://arxiv.org/abs/2505.22074 ,  1687kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22081
Date: Wed, 28 May 2025 08:01:25 GMT   (802kb)

Title: Can Test-time Computation Mitigate Memorization Bias in Neural Symbolic
  Regression?
Authors: Shun Sato, Issei Sato
Categories: cs.LG
\\
  Symbolic regression aims to discover mathematical equations that fit given
numerical data. It has been applied in various fields of scientific research,
such as producing human-readable expressions that explain physical phenomena.
Recently, Neural symbolic regression (NSR) methods that involve Transformers
pre-trained on large-scale synthetic datasets have gained attention. While
these methods offer advantages such as short inference time, they suffer from
low performance, particularly when the number of input variables is large. In
this study, we hypothesized that this limitation stems from the memorization
bias of Transformers in symbolic regression. We conducted a quantitative
evaluation of this bias in Transformers using a synthetic dataset and found
that Transformers rarely generate expressions not present in the training data.
Additional theoretical analysis reveals that this bias arises from the
Transformer's inability to construct expressions compositionally while
verifying their numerical validity. We finally examined if tailoring test-time
strategies can lead to reduced memorization bias and better performance. We
empirically demonstrate that providing additional information to the model at
test time can significantly mitigate memorization bias. On the other hand, we
also find that reducing memorization bias does not necessarily correlate with
improved performance. These findings contribute to a deeper understanding of
the limitations of NSR approaches and offer a foundation for designing more
robust, generalizable symbolic regression methods. Code is available at
https://github.com/Shun-0922/Mem-Bias-NSR .
\\ ( https://arxiv.org/abs/2505.22081 ,  802kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22108
Date: Wed, 28 May 2025 08:36:21 GMT   (774kb)

Title: Inclusive, Differentially Private Federated Learning for Clinical Data
Authors: Santhosh Parampottupadam, Melih Co\c{s}\u{g}un, Sarthak Pati,
  Maximilian Zenk, Saikat Roy, Dimitrios Bounias, Benjamin Hamm, Sinem Sav,
  Ralf Floca, Klaus Maier-Hein
Categories: cs.LG cs.AI cs.CR cs.DC
\\
  Federated Learning (FL) offers a promising approach for training clinical AI
models without centralizing sensitive patient data. However, its real-world
adoption is hindered by challenges related to privacy, resource constraints,
and compliance. Existing Differential Privacy (DP) approaches often apply
uniform noise, which disproportionately degrades model performance, even among
well-compliant institutions. In this work, we propose a novel compliance-aware
FL framework that enhances DP by adaptively adjusting noise based on
quantifiable client compliance scores. Additionally, we introduce a compliance
scoring tool based on key healthcare and security standards to promote secure,
inclusive, and equitable participation across diverse clinical settings.
Extensive experiments on public datasets demonstrate that integrating
under-resourced, less compliant clinics with highly regulated institutions
yields accuracy improvements of up to 15% over traditional FL. This work
advances FL by balancing privacy, compliance, and performance, making it a
viable solution for real-world clinical workflows in global healthcare.
\\ ( https://arxiv.org/abs/2505.22108 ,  774kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22109
Date: Wed, 28 May 2025 08:37:33 GMT   (33956kb,D)

Title: The quest for the GRAph Level autoEncoder (GRALE)
Authors: Paul Krzakala, Gabriel Melo, Charlotte Laclau, Florence d'Alch\'e-Buc,
  R\'emi Flamary
Categories: cs.LG cs.AI
\\
  Although graph-based learning has attracted a lot of attention, graph
representation learning is still a challenging task whose resolution may impact
key application fields such as chemistry or biology. To this end, we introduce
GRALE, a novel graph autoencoder that encodes and decodes graphs of varying
sizes into a shared embedding space. GRALE is trained using an Optimal
Transport-inspired loss that compares the original and reconstructed graphs and
leverages a differentiable node matching module, which is trained jointly with
the encoder and decoder. The proposed attention-based architecture relies on
Evoformer, the core component of AlphaFold, which we extend to support both
graph encoding and decoding. We show, in numerical experiments on simulated and
molecular data, that GRALE enables a highly general form of pre-training,
applicable to a wide range of downstream tasks, from classification and
regression to more complex tasks such as graph interpolation, editing,
matching, and prediction.
\\ ( https://arxiv.org/abs/2505.22109 ,  33956kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22114
Date: Wed, 28 May 2025 08:41:35 GMT   (2645kb)

Title: BiMi Sheets: Infosheets for bias mitigation methods
Authors: MaryBeth Defrance, Guillaume Bied, Maarten Buyl, Jefrey Lijffijt, Tijl
  De Bie
Categories: cs.LG
\\
  Over the past 15 years, hundreds of bias mitigation methods have been
proposed in the pursuit of fairness in machine learning (ML). However,
algorithmic biases are domain-, task-, and model-specific, leading to a
`portability trap': bias mitigation solutions in one context may not be
appropriate in another. Thus, a myriad of design choices have to be made when
creating a bias mitigation method, such as the formalization of fairness it
pursues, and where and how it intervenes in the ML pipeline. This creates
challenges in benchmarking and comparing the relative merits of different bias
mitigation methods, and limits their uptake by practitioners.
  We propose BiMi Sheets as a portable, uniform guide to document the design
choices of any bias mitigation method. This enables researchers and
practitioners to quickly learn its main characteristics and to compare with
their desiderata. Furthermore, the sheets' structure allow for the creation of
a structured database of bias mitigation methods. In order to foster the
sheets' adoption, we provide a platform for finding and creating BiMi Sheets at
bimisheet.com.
\\ ( https://arxiv.org/abs/2505.22114 ,  2645kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22151
Date: Wed, 28 May 2025 09:17:44 GMT   (292kb)

Title: Oryx: a Performant and Scalable Algorithm for Many-Agent Coordination in
  Offline MARL
Authors: Claude Formanek, Omayma Mahjoub, Louay Ben Nessir, Sasha Abramowitz,
  Ruan de Kock, Wiem Khlifi, Simon Du Toit, Felix Chalumeau, Daniel
  Rajaonarivonivelomanantsoa, Arnol Fokam, Siddarth Singh, Ulrich Mbou Sob,
  Arnu Pretorius
Categories: cs.LG
\\
  A key challenge in offline multi-agent reinforcement learning (MARL) is
achieving effective many-agent multi-step coordination in complex environments.
In this work, we propose Oryx, a novel algorithm for offline cooperative MARL
to directly address this challenge. Oryx adapts the recently proposed
retention-based architecture Sable and combines it with a sequential form of
implicit constraint Q-learning (ICQ), to develop a novel offline
auto-regressive policy update scheme. This allows Oryx to solve complex
coordination challenges while maintaining temporal coherence over lengthy
trajectories. We evaluate Oryx across a diverse set of benchmarks from prior
works (SMAC, RWARE, and Multi-Agent MuJoCo) covering tasks of both discrete and
continuous control, varying in scale and difficulty. Oryx achieves
state-of-the-art performance on more than 80% of the 65 tested datasets,
outperforming prior offline MARL methods and demonstrating robust
generalisation across domains with many agents and long horizons. Finally, we
introduce new datasets to push the limits of many-agent coordination in offline
MARL, and demonstrate Oryx's superior ability to scale effectively in such
settings. We will make all of our datasets, experimental data, and code
available upon publication.
\\ ( https://arxiv.org/abs/2505.22151 ,  292kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22152
Date: Wed, 28 May 2025 09:18:01 GMT   (446kb)

Title: Uncertainty Estimation for Heterophilic Graphs Through the Lens of
  Information Theory
Authors: Dominik Fuchsgruber, Tom Wollschl\"ager, Johannes Bordne, Stephan
  G\"unnemann
Categories: cs.LG cs.SI
\\
  While uncertainty estimation for graphs recently gained traction, most
methods rely on homophily and deteriorate in heterophilic settings. We address
this by analyzing message passing neural networks from an information-theoretic
perspective and developing a suitable analog to data processing inequality to
quantify information throughout the model's layers. In contrast to non-graph
domains, information about the node-level prediction target can increase with
model depth if a node's features are semantically different from its neighbors.
Therefore, on heterophilic graphs, the latent embeddings of an MPNN each
provide different information about the data distribution - different from
homophilic settings. This reveals that considering all node representations
simultaneously is a key design principle for epistemic uncertainty estimation
on graphs beyond homophily. We empirically confirm this with a simple post-hoc
density estimator on the joint node embedding space that provides
state-of-the-art uncertainty on heterophilic graphs. At the same time, it
matches prior work on homophilic graphs without explicitly exploiting homophily
through post-processing.
\\ ( https://arxiv.org/abs/2505.22152 ,  446kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22158
Date: Wed, 28 May 2025 09:23:37 GMT   (607kb)

Title: The informativeness of the gradient revisited
Authors: Rustem Takhanov
Categories: cs.LG
\\
  In the past decade gradient-based deep learning has revolutionized several
applications. However, this rapid advancement has highlighted the need for a
deeper theoretical understanding of its limitations. Research has shown that,
in many practical learning tasks, the information contained in the gradient is
so minimal that gradient-based methods require an exceedingly large number of
iterations to achieve success. The informativeness of the gradient is typically
measured by its variance with respect to the random selection of a target
function from a hypothesis class.
  We use this framework and give a general bound on the variance in terms of a
parameter related to the pairwise independence of the target function class and
the collision entropy of the input distribution. Our bound scales as $
\tilde{\mathcal{O}}(\varepsilon+e^{-\frac{1}{2}\mathcal{E}_c}) $, where $
\tilde{\mathcal{O}} $ hides factors related to the regularity of the learning
model and the loss function, $ \varepsilon $ measures the pairwise independence
of the target function class and $\mathcal{E}_c$ is the collision entropy of
the input distribution.
  To demonstrate the practical utility of our bound, we apply it to the class
of Learning with Errors (LWE) mappings and high-frequency functions. In
addition to the theoretical analysis, we present experiments to understand
better the nature of recent deep learning-based attacks on LWE.
\\ ( https://arxiv.org/abs/2505.22158 ,  607kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22196
Date: Wed, 28 May 2025 10:18:20 GMT   (489kb)

Title: An Augmentation-Aware Theory for Self-Supervised Contrastive Learning
Authors: Jingyi Cui, Hongwei Wen, Yisen Wang
Categories: cs.LG
Comments: Accepted to ICML2025
\\
  Self-supervised contrastive learning has emerged as a powerful tool in
machine learning and computer vision to learn meaningful representations from
unlabeled data. Meanwhile, its empirical success has encouraged many
theoretical studies to reveal the learning mechanisms. However, in the existing
theoretical research, the role of data augmentation is still under-exploited,
especially the effects of specific augmentation types. To fill in the blank, we
for the first time propose an augmentation-aware error bound for
self-supervised contrastive learning, showing that the supervised risk is
bounded not only by the unsupervised risk, but also explicitly by a trade-off
induced by data augmentation. Then, under a novel semantic label assumption, we
discuss how certain augmentation methods affect the error bound. Lastly, we
conduct both pixel- and representation-level experiments to verify our proposed
theoretical results.
\\ ( https://arxiv.org/abs/2505.22196 ,  489kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22199
Date: Wed, 28 May 2025 10:23:34 GMT   (5380kb,D)

Title: Enhancing Uncertainty Estimation and Interpretability via Bayesian
  Non-negative Decision Layer
Authors: Xinyue Hu, Zhibin Duan, Bo Chen, Mingyuan Zhou
Categories: cs.LG cs.AI
Comments: Accepted by The Thirteenth International Conference on Learning
  Representations (ICLR 2025)
\\
  Although deep neural networks have demonstrated significant success due to
their powerful expressiveness, most models struggle to meet practical
requirements for uncertainty estimation. Concurrently, the entangled nature of
deep neural networks leads to a multifaceted problem, where various localized
explanation techniques reveal that multiple unrelated features influence the
decisions, thereby undermining interpretability. To address these challenges,
we develop a Bayesian Non-negative Decision Layer (BNDL), which reformulates
deep neural networks as a conditional Bayesian non-negative factor analysis. By
leveraging stochastic latent variables, the BNDL can model complex dependencies
and provide robust uncertainty estimation. Moreover, the sparsity and
non-negativity of the latent variables encourage the model to learn
disentangled representations and decision layers, thereby improving
interpretability. We also offer theoretical guarantees that BNDL can achieve
effective disentangled learning. In addition, we developed a corresponding
variational inference method utilizing a Weibull variational inference network
to approximate the posterior distribution of the latent variables. Our
experimental results demonstrate that with enhanced disentanglement
capabilities, BNDL not only improves the model's accuracy but also provides
reliable uncertainty estimation and improved interpretability.
\\ ( https://arxiv.org/abs/2505.22199 ,  5380kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22203
Date: Wed, 28 May 2025 10:28:41 GMT   (203kb)

Title: Pitfalls of Rule- and Model-based Verifiers -- A Case Study on
  Mathematical Reasoning
Authors: Yuzhen Huang, Weihao Zeng, Xingshan Zeng, Qi Zhu, Junxian He
Categories: cs.LG cs.AI cs.CL
\\
  Trustworthy verifiers are essential for the success of reinforcement learning
with verifiable reward (RLVR), which is the core methodology behind various
large reasoning models such as DeepSeek-R1. In complex domains like
mathematical reasoning, rule-based verifiers have been widely adopted in
previous works to train strong reasoning models. However, the reliability of
these verifiers and their impact on the RL training process remain poorly
understood. In this work, we take mathematical reasoning as a case study and
conduct a comprehensive analysis of various verifiers in both static evaluation
and RL training scenarios. First, we find that current open-source rule-based
verifiers often fail to recognize equivalent answers presented in different
formats across multiple commonly used mathematical datasets, resulting in
non-negligible false negative rates. This limitation adversely affects RL
training performance and becomes more pronounced as the policy model gets
stronger. Subsequently, we investigate model-based verifiers as a potential
solution to address these limitations. While the static evaluation shows that
model-based verifiers achieve significantly higher verification accuracy,
further analysis and RL training results imply that they are highly susceptible
to hacking, where they misclassify certain patterns in responses as correct
(i.e., false positives). This vulnerability is exploited during policy model
optimization, leading to artificially inflated rewards. Our findings underscore
the unique risks inherent to both rule-based and model-based verifiers, aiming
to offer valuable insights to develop more robust reward systems in
reinforcement learning.
\\ ( https://arxiv.org/abs/2505.22203 ,  203kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22208
Date: Wed, 28 May 2025 10:36:49 GMT   (1383kb)

Title: LaMM: Semi-Supervised Pre-Training of Large-Scale Materials Models
Authors: Yosuke Oyama, Yusuke Majima, Eiji Ohta, Yasufumi Sakai
Categories: cs.LG
Comments: 24 pages, 9 figures
\\
  Neural network potentials (NNPs) are crucial for accelerating computational
materials science by surrogating density functional theory (DFT) calculations.
Improving their accuracy is possible through pre-training and fine-tuning,
where an NNP model is first pre-trained on a large-scale dataset and then
fine-tuned on a smaller target dataset. However, this approach is
computationally expensive, mainly due to the cost of DFT-based dataset labeling
and load imbalances during large-scale pre-training. To address this, we
propose LaMM, a semi-supervised pre-training method incorporating improved
denoising self-supervised learning and a load-balancing algorithm for efficient
multi-node training. We demonstrate that our approach effectively leverages a
large-scale dataset of $\sim$300 million semi-labeled samples to train a single
NNP model, resulting in improved fine-tuning performance in terms of both speed
and accuracy.
\\ ( https://arxiv.org/abs/2505.22208 ,  1383kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22224
Date: Wed, 28 May 2025 10:55:16 GMT   (105kb)

Title: Solver-Free Decision-Focused Learning for Linear Optimization Problems
Authors: Senne Berden, Ali \.Irfan Mahmuto\u{g}ullar{\i}, Dimos Tsouros, Tias
  Guns
Categories: cs.LG cs.AI
\\
  Mathematical optimization is a fundamental tool for decision-making in a wide
range of applications. However, in many real-world scenarios, the parameters of
the optimization problem are not known a priori and must be predicted from
contextual features. This gives rise to predict-then-optimize problems, where a
machine learning model predicts problem parameters that are then used to make
decisions via optimization. A growing body of work on decision-focused learning
(DFL) addresses this setting by training models specifically to produce
predictions that maximize downstream decision quality, rather than accuracy.
While effective, DFL is computationally expensive, because it requires solving
the optimization problem with the predicted parameters at each loss evaluation.
In this work, we address this computational bottleneck for linear optimization
problems, a common class of problems in both DFL literature and real-world
applications. We propose a solver-free training method that exploits the
geometric structure of linear optimization to enable efficient training with
minimal degradation in solution quality. Our method is based on the insight
that a solution is optimal if and only if it achieves an objective value that
is at least as good as that of its adjacent vertices on the feasible polytope.
Building on this, our method compares the estimated quality of the ground-truth
optimal solution with that of its precomputed adjacent vertices, and uses this
as loss function. Experiments demonstrate that our method significantly reduces
computational cost while maintaining high decision quality.
\\ ( https://arxiv.org/abs/2505.22224 ,  105kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22235
Date: Wed, 28 May 2025 11:11:24 GMT   (642kb)

Title: Optimal kernel regression bounds under energy-bounded noise
Authors: Amon Lahr, Johannes K\"ohler, Anna Scampicchio, Melanie N. Zeilinger
Categories: cs.LG
\\
  Non-conservative uncertainty bounds are key for both assessing an estimation
algorithm's accuracy and in view of downstream tasks, such as its deployment in
safety-critical contexts. In this paper, we derive a tight, non-asymptotic
uncertainty bound for kernel-based estimation, which can also handle correlated
noise sequences. Its computation relies on a mild norm-boundedness assumption
on the unknown function and the noise, returning the worst-case function
realization within the hypothesis class at an arbitrary query input location.
The value of this function is shown to be given in terms of the posterior mean
and covariance of a Gaussian process for an optimal choice of the measurement
noise covariance. By rigorously analyzing the proposed approach and comparing
it with other results in the literature, we show its effectiveness in returning
tight and easy-to-compute bounds for kernel-based estimates.
\\ ( https://arxiv.org/abs/2505.22235 ,  642kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22252
Date: Wed, 28 May 2025 11:40:48 GMT   (15024kb)

Title: B-XAIC Dataset: Benchmarking Explainable AI for Graph Neural Networks
  Using Chemical Data
Authors: Magdalena Proszewska, Tomasz Danel, Dawid Rymarczyk
Categories: cs.LG cs.CE
Comments: 26 pages, 16 figures, 5 tables
\\
  Understanding the reasoning behind deep learning model predictions is crucial
in cheminformatics and drug discovery, where molecular design determines their
properties. However, current evaluation frameworks for Explainable AI (XAI) in
this domain often rely on artificial datasets or simplified tasks, employing
data-derived metrics that fail to capture the complexity of real-world
scenarios and lack a direct link to explanation faithfulness. To address this,
we introduce B-XAIC, a novel benchmark constructed from real-world molecular
data and diverse tasks with known ground-truth rationales for assigned labels.
Through a comprehensive evaluation using B-XAIC, we reveal limitations of
existing XAI methods for Graph Neural Networks (GNNs) in the molecular domain.
This benchmark provides a valuable resource for gaining deeper insights into
the faithfulness of XAI, facilitating the development of more reliable and
interpretable models.
\\ ( https://arxiv.org/abs/2505.22252 ,  15024kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22254
Date: Wed, 28 May 2025 11:41:07 GMT   (1460kb,D)

Title: A Unified Online-Offline Framework for Co-Branding Campaign
  Recommendations
Authors: Xiangxiang Dai, Xiaowei Sun, Jinhang Zuo, Xutong Liu, John C.S. Lui
Categories: cs.LG
Comments: Accepted at the 31st ACM SIGKDD Conference on Knowledge Discovery and
  Data Mining, 2025
DOI: 10.1145/3711896.3736824
\\
  Co-branding has become a vital strategy for businesses aiming to expand
market reach within recommendation systems. However, identifying effective
cross-industry partnerships remains challenging due to resource imbalances,
uncertain brand willingness, and ever-changing market conditions. In this
paper, we provide the first systematic study of this problem and propose a
unified online-offline framework to enable co-branding recommendations. Our
approach begins by constructing a bipartite graph linking ``initiating'' and
``target'' brands to quantify co-branding probabilities and assess market
benefits. During the online learning phase, we dynamically update the graph in
response to market feedback, while striking a balance between exploring new
collaborations for long-term gains and exploiting established partnerships for
immediate benefits. To address the high initial co-branding costs, our
framework mitigates redundant exploration, thereby enhancing short-term
performance while ensuring sustainable strategic growth. In the offline
optimization phase, our framework consolidates the interests of multiple
sub-brands under the same parent brand to maximize overall returns, avoid
excessive investment in single sub-brands, and reduce unnecessary costs
associated with over-prioritizing a single sub-brand. We present a theoretical
analysis of our approach, establishing a highly nontrivial sublinear regret
bound for online learning in the complex co-branding problem, and enhancing the
approximation guarantee for the NP-hard offline budget allocation optimization.
Experiments on both synthetic and real-world co-branding datasets demonstrate
the practical effectiveness of our framework, with at least 12\% improvement.
\\ ( https://arxiv.org/abs/2505.22254 ,  1460kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22255
Date: Wed, 28 May 2025 11:41:11 GMT   (2680kb)

Title: Train Sparse Autoencoders Efficiently by Utilizing Features Correlation
Authors: Vadim Kurochkin, Yaroslav Aksenov, Daniil Laptev, Daniil Gavrilov and
  Nikita Balagansky
Categories: cs.LG cs.CL
\\
  Sparse Autoencoders (SAEs) have demonstrated significant promise in
interpreting the hidden states of language models by decomposing them into
interpretable latent directions. However, training SAEs at scale remains
challenging, especially when large dictionary sizes are used. While decoders
can leverage sparse-aware kernels for efficiency, encoders still require
computationally intensive linear operations with large output dimensions. To
address this, we propose KronSAE, a novel architecture that factorizes the
latent representation via Kronecker product decomposition, drastically reducing
memory and computational overhead. Furthermore, we introduce mAND, a
differentiable activation function approximating the binary AND operation,
which improves interpretability and performance in our factorized framework.
\\ ( https://arxiv.org/abs/2505.22255 ,  2680kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22257
Date: Wed, 28 May 2025 11:42:33 GMT   (441kb)

Title: Revisiting Group Relative Policy Optimization: Insights into On-Policy
  and Off-Policy Training
Authors: Youssef Mroueh, Nicolas Dupuis, Brian Belgodere, Apoorva Nitsure,
  Mattia Rigotti, Kristjan Greenewald, Jiri Navratil, Jerret Ross, Jesus Rios
Categories: cs.LG stat.ML
\\
  We revisit Group Relative Policy Optimization (GRPO) in both on-policy and
off-policy optimization regimes. Our motivation comes from recent work on
off-policy Proximal Policy Optimization (PPO), which improves training
stability, sampling efficiency, and memory usage. In addition, a recent
analysis of GRPO suggests that estimating the advantage function with
off-policy samples could be beneficial. Building on these observations, we
adapt GRPO to the off-policy setting. We show that both on-policy and
off-policy GRPO objectives yield an improvement in the reward. This result
motivates the use of clipped surrogate objectives in the off-policy version of
GRPO. We then compare the empirical performance of reinforcement learning with
verifiable rewards in post-training using both GRPO variants. Our results show
that off-policy GRPO either significantly outperforms or performs on par with
its on-policy counterpart.
\\ ( https://arxiv.org/abs/2505.22257 ,  441kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22275
Date: Wed, 28 May 2025 12:06:48 GMT   (18115kb)

Title: Full Domain Analysis in Fluid Dynamics
Authors: Alexander Hagg, Adam Gaier, Dominik Wilde, Alexander Asteroth, Holger
  Foysi and Dirk Reith
Categories: cs.LG cs.NE
MSC-class: 68U01
ACM-class: I.2.1; I.2.6
\\
  Novel techniques in evolutionary optimization, simulation and machine
learning allow for a broad analysis of domains like fluid dynamics, in which
computation is expensive and flow behavior is complex. Under the term of full
domain analysis we understand the ability to efficiently determine the full
space of solutions in a problem domain, and analyze the behavior of those
solutions in an accessible and interactive manner. The goal of full domain
analysis is to deepen our understanding of domains by generating many examples
of flow, their diversification, optimization and analysis. We define a formal
model for full domain analysis, its current state of the art, and requirements
of subcomponents. Finally, an example is given to show what we can learn by
using full domain analysis. Full domain analysis, rooted in optimization and
machine learning, can be a helpful tool in understanding complex systems in
computational physics and beyond.
\\ ( https://arxiv.org/abs/2505.22275 ,  18115kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22306
Date: Wed, 28 May 2025 12:45:39 GMT   (2506kb)

Title: Versatile Cardiovascular Signal Generation with a Unified Diffusion
  Transformer
Authors: Zehua Chen, Yuyang Miao, Liyuan Wang, Luyun Fan, Danilo P. Mandic, Jun
  Zhu
Categories: cs.LG cs.AI
\\
  Cardiovascular signals such as photoplethysmography (PPG),
electrocardiography (ECG), and blood pressure (BP) are inherently correlated
and complementary, together reflecting the health of cardiovascular system.
However, their joint utilization in real-time monitoring is severely limited by
diverse acquisition challenges from noisy wearable recordings to burdened
invasive procedures. Here we propose UniCardio, a multi-modal diffusion
transformer that reconstructs low-quality signals and synthesizes unrecorded
signals in a unified generative framework. Its key innovations include a
specialized model architecture to manage the signal modalities involved in
generation tasks and a continual learning paradigm to incorporate varying
modality combinations. By exploiting the complementary nature of cardiovascular
signals, UniCardio clearly outperforms recent task-specific baselines in signal
denoising, imputation, and translation. The generated signals match the
performance of ground-truth signals in detecting abnormal health conditions and
estimating vital signs, even in unseen domains, while ensuring interpretability
for human experts. These advantages position UniCardio as a promising avenue
for advancing AI-assisted healthcare.
\\ ( https://arxiv.org/abs/2505.22306 ,  2506kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22308
Date: Wed, 28 May 2025 12:50:09 GMT   (273kb,D)

Title: Transformers Pretrained on Procedural Data Contain Modular Structures
  for Algorithmic Reasoning
Authors: Zachary Shinnick, Liangze Jiang, Hemanth Saratchandran, Anton van den
  Hengel, Damien Teney
Categories: cs.LG
\\
  Pretraining on large, semantically rich datasets is key for developing
language models. Surprisingly, recent studies have shown that even synthetic
data, generated procedurally through simple semantic-free algorithms, can yield
some of the same benefits as natural language pretraining. It is unclear what
specific capabilities such simple synthetic data instils in a model, where
these capabilities reside in the architecture, and how they manifest within its
weights. In this short paper, we identify several beneficial forms of
procedural data, together with specific algorithmic reasoning skills that
improve in small transformers. Our core finding is that different procedural
rules instil distinct but complementary inductive structures in the model. With
extensive ablations and partial-transfer experiments, we discover that these
structures reside in different parts of the model. Attention layers often carry
the most transferable information, but some pretraining rules impart useful
structure to MLP blocks instead. Most interestingly, the structures induced by
multiple rules can be composed to jointly reinforce multiple capabilities.
These results suggest an exciting possibility of disentangling the acquisition
of knowledge from reasoning in language models, with the goal of improving
their robustness and data efficiency.
\\ ( https://arxiv.org/abs/2505.22308 ,  273kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22310
Date: Wed, 28 May 2025 12:53:08 GMT   (565kb)

Title: From Dormant to Deleted: Tamper-Resistant Unlearning Through
  Weight-Space Regularization
Authors: Shoaib Ahmed Siddiqui, Adrian Weller, David Krueger, Gintare Karolina
  Dziugaite, Michael Curtis Mozer, Eleni Triantafillou
Categories: cs.LG cs.AI cs.CV
\\
  Recent unlearning methods for LLMs are vulnerable to relearning attacks:
knowledge believed-to-be-unlearned re-emerges by fine-tuning on a small set of
(even seemingly-unrelated) examples. We study this phenomenon in a controlled
setting for example-level unlearning in vision classifiers. We make the
surprising discovery that forget-set accuracy can recover from around 50%
post-unlearning to nearly 100% with fine-tuning on just the retain set -- i.e.,
zero examples of the forget set. We observe this effect across a wide variety
of unlearning methods, whereas for a model retrained from scratch excluding the
forget set (gold standard), the accuracy remains at 50%. We observe that
resistance to relearning attacks can be predicted by weight-space properties,
specifically, $L_2$-distance and linear mode connectivity between the original
and the unlearned model. Leveraging this insight, we propose a new class of
methods that achieve state-of-the-art resistance to relearning attacks.
\\ ( https://arxiv.org/abs/2505.22310 ,  565kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22312
Date: Wed, 28 May 2025 12:56:04 GMT   (11561kb)

Title: Skywork Open Reasoner 1 Technical Report
Authors: Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng
  Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang
  Zeng, Tianwen Wei, Cheng Cheng, Bo An, Yang Liu, Yahui Zhou
Categories: cs.LG cs.AI cs.CL
\\
  The success of DeepSeek-R1 underscores the significant role of reinforcement
learning (RL) in enhancing the reasoning capabilities of large language models
(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL
implementation for long Chain-of-Thought (CoT) models. Building on the
DeepSeek-R1-Distill model series, our RL approach achieves notable performance
gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench
from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)
for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and
Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable
results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models
demonstrate competitive reasoning capabilities among models of similar size. We
perform comprehensive ablation studies on the core components of our training
pipeline to validate their effectiveness. Additionally, we thoroughly
investigate the phenomenon of entropy collapse, identify key factors affecting
entropy dynamics, and demonstrate that mitigating premature entropy collapse is
critical for improved test performance. To support community research, we fully
open-source our model weights, training code, and training datasets.
\\ ( https://arxiv.org/abs/2505.22312 ,  11561kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22316
Date: Wed, 28 May 2025 13:00:52 GMT   (768kb,D)

Title: Rethinking BPS: A Utility-Based Evaluation Framework
Authors: Konrad \"Ozdemir, Lukas Kirchdorfer, Keyvan Amiri Elyasi, Han van der
  Aa, Heiner Stuckenschmidt
Categories: cs.LG
\\
  Business process simulation (BPS) is a key tool for analyzing and optimizing
organizational workflows, supporting decision-making by estimating the impact
of process changes. The reliability of such estimates depends on the ability of
a BPS model to accurately mimic the process under analysis, making rigorous
accuracy evaluation essential. However, the state-of-the-art approach to
evaluating BPS models has two key limitations. First, it treats simulation as a
forecasting problem, testing whether models can predict unseen future events.
This fails to assess how well a model captures the as-is process, particularly
when process behavior changes from train to test period. Thus, it becomes
difficult to determine whether poor results stem from an inaccurate model or
the inherent complexity of the data, such as unpredictable drift. Second, the
evaluation approach strongly relies on Earth Mover's Distance-based metrics,
which can obscure temporal patterns and thus yield misleading conclusions about
simulation quality. To address these issues, we propose a novel framework that
evaluates simulation quality based on its ability to generate representative
process behavior. Instead of comparing simulated logs to future real-world
executions, we evaluate whether predictive process monitoring models trained on
simulated data perform comparably to those trained on real data for downstream
analysis tasks. Empirical results show that our framework not only helps
identify sources of discrepancies but also distinguishes between model accuracy
and data complexity, offering a more meaningful way to assess BPS quality.
\\ ( https://arxiv.org/abs/2505.22316 ,  768kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22322
Date: Wed, 28 May 2025 13:06:00 GMT   (2336kb)

Title: A Closer Look on Memorization in Tabular Diffusion Model: A Data-Centric
  Perspective
Authors: Zhengyu Fang, Zhimeng Jiang, Huiyuan Chen, Xiaoge Zhang, Kaiyu Tang,
  Xiao Li, Jing Li
Categories: cs.LG
\\
  Diffusion models have shown strong performance in generating high-quality
tabular data, but they carry privacy risks by reproducing exact training
samples. While prior work focuses on dataset-level augmentation to reduce
memorization, little is known about which individual samples contribute most.
We present the first data-centric study of memorization dynamics in tabular
diffusion models. We quantify memorization for each real sample based on how
many generated samples are flagged as replicas, using a relative distance
ratio. Our empirical analysis reveals a heavy-tailed distribution of
memorization counts: a small subset of samples contributes disproportionately
to leakage, confirmed via sample-removal experiments. To understand this, we
divide real samples into top- and non-top-memorized groups and analyze their
training-time behaviors. We track when each sample is first memorized and
monitor per-epoch memorization intensity (AUC). Memorized samples are memorized
slightly earlier and show stronger signals in early training. Based on these
insights, we propose DynamicCut, a two-stage, model-agnostic mitigation method:
(a) rank samples by epoch-wise intensity, (b) prune a tunable top fraction, and
(c) retrain on the filtered dataset. Across multiple tabular datasets and
models, DynamicCut reduces memorization with minimal impact on data diversity
and downstream performance. It also complements augmentation-based defenses.
Furthermore, DynamicCut enables cross-model transferability: high-ranked
samples identified from one model (e.g., a diffusion model) are also effective
for reducing memorization when removed from others, such as GANs and VAEs.
\\ ( https://arxiv.org/abs/2505.22322 ,  2336kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22355
Date: Wed, 28 May 2025 13:35:12 GMT   (1049kb)

Title: Look Within or Look Beyond? A Theoretical Comparison Between
  Parameter-Efficient and Full Fine-Tuning
Authors: Yongkang Liu and Xingle Xu and Ercong Nie and Zijing Wang and Shi Feng
  and Daling Wang and Qian Li and Hinrich Sch\"utze
Categories: cs.LG
\\
  Parameter-Efficient Fine-Tuning (PEFT) methods achieve performance comparable
to Full Fine-Tuning (FFT) while requiring significantly fewer computing
resources, making it the go-to choice for researchers. We find that although
PEFT can achieve competitive results on some benchmarks, its performance falls
short of FFT in complex tasks, such as reasoning and instruction-based
fine-tuning. In this paper, we compare the characteristics of PEFT and FFT in
terms of representational capacity and robustness based on optimization theory.
We theoretically demonstrate that PEFT is a strict subset of FFT. By providing
theoretical upper bounds for PEFT, we show that the limited parameter space
constrains the model's representational ability, making it more susceptible to
perturbations. Experiments on 15 datasets encompassing classification,
generation, reasoning, instruction fine-tuning tasks and 11 adversarial test
sets validate our theories. We hope that these results spark further research
beyond the realms of well established PEFT. The source code is in the anonymous
Github repository\footnote{https://github.com/misonsky/PEFTEval}.
\\ ( https://arxiv.org/abs/2505.22355 ,  1049kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22356
Date: Wed, 28 May 2025 13:37:04 GMT   (157kb)

Title: Suitability Filter: A Statistical Framework for Classifier Evaluation in
  Real-World Deployment Settings
Authors: Ang\'eline Pouget, Mohammad Yaghini, Stephan Rabanser, Nicolas
  Papernot
Categories: cs.LG cs.AI cs.CY stat.ML
Comments: Accepted to ICML 2025
\\
  Deploying machine learning models in safety-critical domains poses a key
challenge: ensuring reliable model performance on downstream user data without
access to ground truth labels for direct validation. We propose the suitability
filter, a novel framework designed to detect performance deterioration by
utilizing suitability signals -- model output features that are sensitive to
covariate shifts and indicative of potential prediction errors. The suitability
filter evaluates whether classifier accuracy on unlabeled user data shows
significant degradation compared to the accuracy measured on the labeled test
dataset. Specifically, it ensures that this degradation does not exceed a
pre-specified margin, which represents the maximum acceptable drop in accuracy.
To achieve reliable performance evaluation, we aggregate suitability signals
for both test and user data and compare these empirical distributions using
statistical hypothesis testing, thus providing insights into decision
uncertainty. Our modular method adapts to various models and domains. Empirical
evaluations across different classification tasks demonstrate that the
suitability filter reliably detects performance deviations due to covariate
shift. This enables proactive mitigation of potential failures in high-stakes
applications.
\\ ( https://arxiv.org/abs/2505.22356 ,  157kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22358
Date: Wed, 28 May 2025 13:38:21 GMT   (548kb,D)

Title: Budget-Adaptive Adapter Tuning in Orthogonal Subspaces for Continual
  Learning in LLMs
Authors: Zhiyi Wan, Wanrou Du, Liang Li, Miao Pan, Xiaoqi Qin
Categories: cs.LG cs.AI
\\
  Large language models (LLMs) often suffer from catastrophic forgetting in
continual learning (CL) scenarios, where performance on previously learned
tasks degrades severely while training on sequentially arriving tasks. Although
pioneering CL approaches using orthogonal subspaces can mitigate task
interference, they typically employ fixed budget allocation, neglecting the
varying complexity across tasks and layers. Besides, recent budget-adaptive
tuning methods for LLMs often adopt multi-stage paradigms that decouple
optimization and budget allocation. Such decoupling results in potential
misalignment, which hinders those approaches' practical application in CL
scenarios. To address these limitations, we propose OA-Adapter, a novel
parameter-efficient approach for continual learning in LLMs that unifies
dynamic budget adaptation with orthogonal subspace learning in a single
end-to-end training stage. Specifically, OA-Adapter introduces a dynamic
bottleneck dimension adaptation mechanism that simultaneously allocates an
efficient parameter budget and optimizes task objectives without misalignment.
To effectively preserve previously acquired knowledge while coordinating with
the dynamic budget allocation, orthogonal constraints are applied specifically
between the parameter subspace of the current task and the dynamically
allocated parameter subspaces of historical tasks. Experimental results on
continual learning benchmarks demonstrate that OA-Adapter outperforms
state-of-the-art methods in both accuracy and parameter efficiency, achieving
higher average accuracy while using 58.5% fewer parameters on the standard CL
benchmark.
\\ ( https://arxiv.org/abs/2505.22358 ,  548kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22359
Date: Wed, 28 May 2025 13:39:14 GMT   (28kb)

Title: Multiclass Loss Geometry Matters for Generalization of Gradient Descent
  in Separable Classification
Authors: Matan Schliserman and Tomer Koren
Categories: cs.LG
\\
  We study the generalization performance of unregularized gradient methods for
separable linear classification. While previous work mostly deal with the
binary case, we focus on the multiclass setting with $k$ classes and establish
novel population risk bounds for Gradient Descent for loss functions that decay
to zero. In this setting, we show risk bounds that reveal that convergence
rates are crucially influenced by the geometry of the loss template, as
formalized by Wang and Scott (2024), rather than of the loss function itself.
Particularly, we establish risk upper bounds that holds for any decay rate of
the loss whose template is smooth with respect to the $p$-norm. In the case of
exponentially decaying losses, our results indicates a contrast between the
$p=\infty$ case, where the risk exhibits a logarithmic dependence on $k$, and
$p=2$ where the risk scales linearly with $k$. To establish this separation
formally, we also prove a lower bound in the latter scenario, demonstrating
that the polynomial dependence on $k$ is unavoidable. Central to our analysis
is a novel bound on the Rademacher complexity of low-noise vector-valued linear
predictors with a loss template smooth w.r.t.~general $p$-norms.
\\ ( https://arxiv.org/abs/2505.22359 ,  28kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22361
Date: Wed, 28 May 2025 13:41:00 GMT   (131kb)

Title: Continuum-armed Bandit Optimization with Batch Pairwise Comparison
  Oracles
Authors: Xiangyu Chang, Xi Chen, Yining Wang, Zhiyi Zeng
Categories: cs.LG stat.ML
\\
  This paper studies a bandit optimization problem where the goal is to
maximize a function $f(x)$ over $T$ periods for some unknown strongly concave
function $f$. We consider a new pairwise comparison oracle, where the
decision-maker chooses a pair of actions $(x, x')$ for a consecutive number of
periods and then obtains an estimate of $f(x)-f(x')$. We show that such a
pairwise comparison oracle finds important applications to joint pricing and
inventory replenishment problems and network revenue management. The challenge
in this bandit optimization is twofold. First, the decision-maker not only
needs to determine a pair of actions $(x, x')$ but also a stopping time $n$
(i.e., the number of queries based on $(x, x')$). Second, motivated by our
inventory application, the estimate of the difference $f(x)-f(x')$ is biased,
which is different from existing oracles in stochastic optimization literature.
To address these challenges, we first introduce a discretization technique and
local polynomial approximation to relate this problem to linear bandits. Then
we developed a tournament successive elimination technique to localize the
discretized cell and run an interactive batched version of LinUCB algorithm on
cells. We establish regret bounds that are optimal up to poly-logarithmic
factors. Furthermore, we apply our proposed algorithm and analytical framework
to the two operations management problems and obtain results that improve
state-of-the-art results in the existing literature.
\\ ( https://arxiv.org/abs/2505.22361 ,  131kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22362
Date: Wed, 28 May 2025 13:41:04 GMT   (639kb)

Title: Directed Homophily-Aware Graph Neural Network
Authors: Aihu Zhang, Jiaxing Xu, Mengcheng Lan, Shili Xiang, Yiping Ke
Categories: cs.LG
\\
  Graph Neural Networks (GNNs) have achieved significant success in various
learning tasks on graph-structured data. Nevertheless, most GNNs struggle to
generalize to heterophilic neighborhoods. Additionally, many GNNs ignore the
directional nature of real-world graphs, resulting in suboptimal performance on
directed graphs with asymmetric structures. In this work, we propose Directed
Homophily-aware Graph Neural Network (DHGNN), a novel framework that addresses
these limitations by incorporating homophily-aware and direction-sensitive
components. DHGNN employs a resettable gating mechanism to adaptively modulate
message contributions based on homophily levels and informativeness, and a
structure-aware noise-tolerant fusion module to effectively integrate node
representations from the original and reverse directions. Extensive experiments
on both homophilic and heterophilic directed graph datasets demonstrate that
DHGNN outperforms state-of-the-art methods in node classification and link
prediction. In particular, DHGNN improves over the best baseline by up to
15.07% in link prediction. Our analysis further shows that the gating mechanism
captures directional homophily gaps and fluctuating homophily across layers,
providing deeper insights into message-passing behavior on complex graph
structures.
\\ ( https://arxiv.org/abs/2505.22362 ,  639kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22370
Date: Wed, 28 May 2025 13:57:56 GMT   (299kb)

Title: SplitLoRA: Balancing Stability and Plasticity in Continual Learning
  Through Gradient Space Splitting
Authors: Haomiao Qiu, Miao Zhang, Ziyue Qiao, Weili Guan, Min Zhang, Liqiang
  Nie
Categories: cs.LG cs.AI
Comments: 18 pages, 4 figures
\\
  Continual Learning requires a model to learn multiple tasks in sequence while
maintaining both stability:preserving knowledge from previously learned tasks,
and plasticity:effectively learning new tasks. Gradient projection has emerged
as an effective and popular paradigm in CL, where it partitions the gradient
space of previously learned tasks into two orthogonal subspaces: a primary
subspace and a minor subspace. New tasks are learned effectively within the
minor subspace, thereby reducing interference with previously acquired
knowledge. However, existing Gradient Projection methods struggle to achieve an
optimal balance between plasticity and stability, as it is hard to
appropriately partition the gradient space. In this work, we consider a
continual learning paradigm based on Low-Rank Adaptation, which has gained
considerable attention due to its efficiency and wide applicability, and
propose a novel approach for continual learning, called SplitLoRA. We first
provide a theoretical analysis of how subspace partitioning affects model
stability and plasticity. Informed by this analysis, we then introduce an
effective method that derives the optimal partition of the gradient space for
previously learned tasks. This approach effectively balances stability and
plasticity in continual learning. Experimental results on multiple datasets
demonstrate that the proposed method achieves state-of-the-art performance.
\\ ( https://arxiv.org/abs/2505.22370 ,  299kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22381
Date: Wed, 28 May 2025 14:09:51 GMT   (749kb)

Title: A Divide-and-Conquer Approach for Modeling Arrival Times in Business
  Process Simulation
Authors: Lukas Kirchdorfer, Konrad \"Ozdemir, Stjepan Kusenic, Han van der Aa,
  Heiner Stuckenschmidt
Categories: cs.LG
\\
  Business Process Simulation (BPS) is a critical tool for analyzing and
improving organizational processes by estimating the impact of process changes.
A key component of BPS is the case-arrival model, which determines the pattern
of new case entries into a process. Although accurate case-arrival modeling is
essential for reliable simulations, as it influences waiting and overall cycle
times, existing approaches often rely on oversimplified static distributions of
inter-arrival times. These approaches fail to capture the dynamic and temporal
complexities inherent in organizational environments, leading to less accurate
and reliable outcomes. To address this limitation, we propose Auto Time Kernel
Density Estimation (AT-KDE), a divide-and-conquer approach that models arrival
times of processes by incorporating global dynamics, day-of-week variations,
and intraday distributional changes, ensuring both precision and scalability.
Experiments conducted across 20 diverse processes demonstrate that AT-KDE is
far more accurate and robust than existing approaches while maintaining
sensible execution time efficiency.
\\ ( https://arxiv.org/abs/2505.22381 ,  749kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22389
Date: Wed, 28 May 2025 14:14:19 GMT   (13808kb)

Title: Train with Perturbation, Infer after Merging: A Two-Stage Framework for
  Continual Learning
Authors: Haomiao Qiu, Miao Zhang, Ziyue Qiao, Liqiang Nie
Categories: cs.LG cs.AI
Comments: 17 pages, 3 figures
\\
  Continual Learning (CL) aims to enable models to continuously acquire new
knowledge from a sequence of tasks with avoiding the forgetting of learned
information. However, existing CL methods only rely on the parameters of the
most recent task for inference, which makes them susceptible to catastrophic
forgetting. Inspired by the recent success of model merging techniques, we
propose \textbf{Perturb-and-Merge (P\&M)}, a novel continual learning framework
that integrates model merging into the CL paradigm to mitigate forgetting.
Specifically, after training on each task, P\&M constructs a new model by
forming a convex combination of the previous model and the newly trained
task-specific model. Through theoretical analysis, we minimize the total loss
increase across all tasks and derive an analytical solution for the optimal
merging coefficient. To further improve the performance of the merged model, we
observe that the degradation introduced during merging can be alleviated by a
regularization term composed of the task vector and the Hessian matrix of the
loss function. Interestingly, we show that this term can be efficiently
approximated using second-order symmetric finite differences, and a stochastic
perturbation strategy along the task vector direction is accordingly devised
which incurs no additional forward or backward passes while providing an
effective approximation of the regularization term. Finally, we combine P\&M
with LoRA, a parameter-efficient fine-tuning method, to reduce memory overhead.
Our proposed approach achieves state-of-the-art performance on several
continual learning benchmark datasets.
\\ ( https://arxiv.org/abs/2505.22389 ,  13808kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22391
Date: Wed, 28 May 2025 14:17:58 GMT   (4268kb)

Title: Physics-Informed Distillation of Diffusion Models for PDE-Constrained
  Generation
Authors: Yi Zhang, Difan Zou
Categories: cs.LG cs.AI cs.CE cs.NA math.NA
Comments: 23 pages, 5 figures, 4 tables
\\
  Modeling physical systems in a generative manner offers several advantages,
including the ability to handle partial observations, generate diverse
solutions, and address both forward and inverse problems. Recently, diffusion
models have gained increasing attention in the modeling of physical systems,
particularly those governed by partial differential equations (PDEs). However,
diffusion models only access noisy data $\boldsymbol{x}_t$ at intermediate
steps, making it infeasible to directly enforce constraints on the clean sample
$\boldsymbol{x}_0$ at each noisy level. As a workaround, constraints are
typically applied to the expectation of clean samples
$\mathbb{E}[\boldsymbol{x}_0|\boldsymbol{x}_t]$, which is estimated using the
learned score network. However, imposing PDE constraints on the expectation
does not strictly represent the one on the true clean data, known as Jensen's
Gap. This gap creates a trade-off: enforcing PDE constraints may come at the
cost of reduced accuracy in generative modeling. To address this, we propose a
simple yet effective post-hoc distillation approach, where PDE constraints are
not injected directly into the diffusion process, but instead enforced during a
post-hoc distillation stage. We term our method as Physics-Informed
Distillation of Diffusion Models (PIDDM). This distillation not only
facilitates single-step generation with improved PDE satisfaction, but also
support both forward and inverse problem solving and reconstruction from
randomly partial observation. Extensive experiments across various PDE
benchmarks demonstrate that PIDDM significantly improves PDE satisfaction over
several recent and competitive baselines, such as PIDM, DiffusionPDE, and
ECI-sampling, with less computation overhead. Our approach can shed light on
more efficient and effective strategies for incorporating physical constraints
into diffusion models.
\\ ( https://arxiv.org/abs/2505.22391 ,  4268kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22411
Date: Wed, 28 May 2025 14:39:26 GMT   (2347kb)

Title: Mitigating Overthinking in Large Reasoning Models via Manifold Steering
Authors: Yao Huang, Huanran Chen, Shouwei Ruan, Yichi Zhang, Xingxing Wei,
  Yinpeng Dong
Categories: cs.LG cs.AI cs.CL
Comments: 17 pages, 7 figures
\\
  Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable
capabilities in solving complex tasks such as mathematics and coding. However,
these models frequently exhibit a phenomenon known as overthinking during
inference, characterized by excessive validation loops and redundant
deliberation, leading to substantial computational overheads. In this paper, we
aim to mitigate overthinking by investigating the underlying mechanisms from
the perspective of mechanistic interpretability. We first showcase that the
tendency of overthinking can be effectively captured by a single direction in
the model's activation space and the issue can be eased by intervening the
activations along this direction. However, this efficacy soon reaches a plateau
and even deteriorates as the intervention strength increases. We therefore
systematically explore the activation space and find that the overthinking
phenomenon is actually tied to a low-dimensional manifold, which indicates that
the limited effect stems from the noises introduced by the high-dimensional
steering direction. Based on this insight, we propose Manifold Steering, a
novel approach that elegantly projects the steering direction onto the
low-dimensional activation manifold given the theoretical approximation of the
interference noise. Extensive experiments on DeepSeek-R1 distilled models
validate that our method reduces output tokens by up to 71% while maintaining
and even improving the accuracy on several mathematical benchmarks. Our method
also exhibits robust cross-domain transferability, delivering consistent token
reduction performance in code generation and knowledge-based QA tasks. Code is
available at: https://github.com/Aries-iai/Manifold_Steering.
\\ ( https://arxiv.org/abs/2505.22411 ,  2347kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22422
Date: Wed, 28 May 2025 14:48:07 GMT   (5245kb)

Title: STaR-Bets: Sequential Target-Recalculating Bets for Tighter Confidence
  Intervals
Authors: V\'aclav Vor\'a\v{c}ek and Francesco Orabona
Categories: cs.LG
Comments: comments are welcome
\\
  The construction of confidence intervals for the mean of a bounded random
variable is a classical problem in statistics with numerous applications in
machine learning and virtually all scientific fields. In particular, obtaining
the tightest possible confidence intervals is vital every time the sampling of
the random variables is expensive. The current state-of-the-art method to
construct confidence intervals is by using betting algorithms. This is a very
successful approach for deriving optimal confidence sequences, even matching
the rate of law of iterated logarithms. However, in the fixed horizon setting,
these approaches are either sub-optimal or based on heuristic solutions with
strong empirical performance but without a finite-time guarantee. Hence, no
betting-based algorithm guaranteeing the optimal
$\mathcal{O}(\sqrt{\frac{\sigma^2\log\frac1\delta}{n}})$ width of the
confidence intervals are known. This work bridges this gap. We propose a
betting-based algorithm to compute confidence intervals that empirically
outperforms the competitors. Our betting strategy uses the optimal strategy in
every step (in a certain sense), whereas the standard betting methods choose a
constant strategy in advance. Leveraging this fact results in strict
improvements even for classical concentration inequalities, such as the ones of
Hoeffding or Bernstein. Moreover, we also prove that the width of our
confidence intervals is optimal up to an $1+o(1)$ factor diminishing with $n$.
The code is available
on~https://github.com/vvoracek/STaR-bets-confidence-interval.
\\ ( https://arxiv.org/abs/2505.22422 ,  5245kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22425
Date: Wed, 28 May 2025 14:52:15 GMT   (325kb,D)

Title: Scaling Reasoning without Attention
Authors: Xueliang Zhao, Wei Wu, Lingpeng Kong
Categories: cs.LG cs.AI cs.CL
Comments: preprint
\\
  Large language models (LLMs) have made significant advances in complex
reasoning tasks, yet they remain bottlenecked by two core challenges:
architectural inefficiency due to reliance on Transformers, and a lack of
structured fine-tuning for high-difficulty domains. We introduce \ourmodel, an
attention-free language model that addresses both issues through architectural
and data-centric innovations. Built on the state space dual (SSD) layers of
Mamba-2, our model eliminates the need for self-attention and key-value
caching, enabling fixed-memory, constant-time inference. To train it for
complex reasoning, we propose a two-phase curriculum fine-tuning strategy based
on the \textsc{PromptCoT} synthesis paradigm, which generates pedagogically
structured problems via abstract concept selection and rationale-guided
generation. On benchmark evaluations, \ourmodel-7B outperforms strong
Transformer and hybrid models of comparable scale, and even surpasses the much
larger Gemma3-27B by 2.6\% on AIME 24, 0.6\% on AIME 25, and 3.0\% on
Livecodebench. These results highlight the potential of state space models as
efficient and scalable alternatives to attention-based architectures for
high-capacity reasoning.
\\ ( https://arxiv.org/abs/2505.22425 ,  325kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22440
Date: Wed, 28 May 2025 15:04:36 GMT   (462kb)

Title: Data-Driven Antenna Miniaturization: A Knowledge-Based System
  Integrating Quantum PSO and Predictive Machine Learning Models
Authors: Khan Masood Parvez, Sk Md Abidar Rahaman, and Ali Shiri Sichani
Categories: cs.LG
\\
  The rapid evolution of wireless technologies necessitates automated design
frameworks to address antenna miniaturization and performance optimization
within constrained development cycles. This study demonstrates a machine
learning enhanced workflow integrating Quantum-Behaved Dynamic Particle Swarm
Optimization (QDPSO) with ANSYS HFSS simulations to accelerate antenna design.
The QDPSO algorithm autonomously optimized loop dimensions in 11.53 seconds,
achieving a resonance frequency of 1.4208 GHz a 12.7 percent reduction compared
to conventional 1.60 GHz designs. Machine learning models (SVM, Random Forest,
XGBoost, and Stacked ensembles) predicted resonance frequencies in 0.75 seconds
using 936 simulation datasets, with stacked models showing superior training
accuracy (R2=0.9825) and SVM demonstrating optimal validation performance
(R2=0.7197). The complete design cycle, encompassing optimization, prediction,
and ANSYS validation, required 12.42 minutes on standard desktop hardware
(Intel i5-8500, 16GB RAM), contrasting sharply with the 50-hour benchmark of
PSADEA-based approaches. This 240 times of acceleration eliminates traditional
trial-and-error methods that often extend beyond seven expert-led days. The
system enables precise specifications of performance targets with automated
generation of fabrication-ready parameters, particularly benefiting compact
consumer devices requiring rapid frequency tuning. By bridging AI-driven
optimization with CAD validation, this framework reduces engineering workloads
while ensuring production-ready designs, establishing a scalable paradigm for
next-generation RF systems in 6G and IoT applications.
\\ ( https://arxiv.org/abs/2505.22440 ,  462kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22442
Date: Wed, 28 May 2025 15:07:24 GMT   (5896kb)

Title: SOReL and TOReL: Two Methods for Fully Offline Reinforcement Learning
Authors: Mattie Fellows, Clarisse Wibault, Uljad Berdica, Johannes Forkel,
  Jakob N. Foerster, Michael A. Osborne
Categories: cs.LG cs.AI
\\
  Sample efficiency remains a major obstacle for real world adoption of
reinforcement learning (RL): success has been limited to settings where
simulators provide access to essentially unlimited environment interactions,
which in reality are typically costly or dangerous to obtain. Offline RL in
principle offers a solution by exploiting offline data to learn a near-optimal
policy before deployment. In practice, however, current offline RL methods rely
on extensive online interactions for hyperparameter tuning, and have no
reliable bound on their initial online performance. To address these two
issues, we introduce two algorithms. Firstly, SOReL: an algorithm for safe
offline reinforcement learning. Using only offline data, our Bayesian approach
infers a posterior over environment dynamics to obtain a reliable estimate of
the online performance via the posterior predictive uncertainty. Crucially, all
hyperparameters are also tuned fully offline. Secondly, we introduce TOReL: a
tuning for offline reinforcement learning algorithm that extends our
information rate based offline hyperparameter tuning methods to general offline
RL approaches. Our empirical evaluation confirms SOReL's ability to accurately
estimate regret in the Bayesian setting whilst TOReL's offline hyperparameter
tuning achieves competitive performance with the best online hyperparameter
tuning methods using only offline data. Thus, SOReL and TOReL make a
significant step towards safe and reliable offline RL, unlocking the potential
for RL in the real world. Our implementations are publicly available:
https://github.com/CWibault/sorel\_torel.
\\ ( https://arxiv.org/abs/2505.22442 ,  5896kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22450
Date: Wed, 28 May 2025 15:10:33 GMT   (919kb)

Title: Position: All Current Generative Fidelity and Diversity Metrics are
  Flawed
Authors: Ossi R\"ais\"a, Boris van Breugel, Mihaela van der Schaar
Categories: cs.LG
Comments: ICML 2025
\\
  Any method's development and practical application is limited by our ability
to measure its reliability. The popularity of generative modeling emphasizes
the importance of good synthetic data metrics. Unfortunately, previous works
have found many failure cases in current metrics, for example lack of outlier
robustness and unclear lower and upper bounds. We propose a list of desiderata
for synthetic data metrics, and a suite of sanity checks: carefully chosen
simple experiments that aim to detect specific and known generative modeling
failure modes. Based on these desiderata and the results of our checks, we
arrive at our position: all current generative fidelity and diversity metrics
are flawed. This significantly hinders practical use of synthetic data. Our aim
is to convince the research community to spend more effort in developing
metrics, instead of models. Additionally, through analyzing how current metrics
fail, we provide practitioners with guidelines on how these metrics should
(not) be used.
\\ ( https://arxiv.org/abs/2505.22450 ,  919kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22473
Date: Wed, 28 May 2025 15:23:36 GMT   (56kb)

Title: Pure Exploration with Infinite Answers
Authors: Riccardo Poiani, Martino Bernasconi, Andrea Celli
Categories: cs.LG
\\
  We study pure exploration problems where the set of correct answers is
possibly infinite, e.g., the regression of any continuous function of the means
of the bandit. We derive an instance-dependent lower bound for these problems.
By analyzing it, we discuss why existing methods (i.e., Sticky Track-and-Stop)
for finite answer problems fail at being asymptotically optimal in this more
general setting. Finally, we present a framework, Sticky-Sequence
Track-and-Stop, which generalizes both Track-and-Stop and Sticky
Track-and-Stop, and that enjoys asymptotic optimality. Due to its generality,
our analysis also highlights special cases where existing methods enjoy
optimality.
\\ ( https://arxiv.org/abs/2505.22473 ,  56kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22474
Date: Wed, 28 May 2025 15:24:04 GMT   (5105kb)

Title: Forecasting Multivariate Urban Data via Decomposition and
  Spatio-Temporal Graph Analysis
Authors: Amirhossein Sohrabbeig, Omid Ardakanian, and Petr Musilek
Categories: cs.LG
\\
  The forecasting of multivariate urban data presents a complex challenge due
to the intricate dependencies between various urban metrics such as weather,
air pollution, carbon intensity, and energy demand. This paper introduces a
novel multivariate time-series forecasting model that utilizes advanced Graph
Neural Networks (GNNs) to capture spatial dependencies among different
time-series variables. The proposed model incorporates a decomposition-based
preprocessing step, isolating trend, seasonal, and residual components to
enhance the accuracy and interpretability of forecasts. By leveraging the
dynamic capabilities of GNNs, the model effectively captures interdependencies
and improves the forecasting performance. Extensive experiments on real-world
datasets, including electricity usage, weather metrics, carbon intensity, and
air pollution data, demonstrate the effectiveness of the proposed approach
across various forecasting scenarios. The results highlight the potential of
the model to optimize smart infrastructure systems, contributing to
energy-efficient urban development and enhanced public well-being.
\\ ( https://arxiv.org/abs/2505.22474 ,  5105kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22475
Date: Wed, 28 May 2025 15:26:55 GMT   (30kb)

Title: Non-Asymptotic Analysis of (Sticky) Track-and-Stop
Authors: Riccardo Poiani, Martino Bernasconi, Andrea Celli
Categories: cs.LG
\\
  In pure exploration problems, a statistician sequentially collects
information to answer a question about some stochastic and unknown environment.
The probability of returning a wrong answer should not exceed a maximum risk
parameter $\delta$ and good algorithms make as few queries to the environment
as possible. The Track-and-Stop algorithm is a pioneering method to solve these
problems. Specifically, it is well-known that it enjoys asymptotic optimality
sample complexity guarantees for $\delta\to 0$ whenever the map from the
environment to its correct answers is single-valued (e.g., best-arm
identification with a unique optimal arm). The Sticky Track-and-Stop algorithm
extends these results to settings where, for each environment, there might
exist multiple correct answers (e.g., $\epsilon$-optimal arm identification).
Although both methods are optimal in the asymptotic regime, their
non-asymptotic guarantees remain unknown. In this work, we fill this gap and
provide non-asymptotic guarantees for both algorithms.
\\ ( https://arxiv.org/abs/2505.22475 ,  30kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22483
Date: Wed, 28 May 2025 15:31:53 GMT   (965kb)

Title: A Closer Look at Multimodal Representation Collapse
Authors: Abhra Chaudhuri and Anjan Dutta and Tu Bui and Serban Georgescu
Categories: cs.LG cs.AI cs.CV
Comments: International Conference on Machine Learning (ICML) 2025 (Spotlight)
\\
  We aim to develop a fundamental understanding of modality collapse, a
recently observed empirical phenomenon wherein models trained for multimodal
fusion tend to rely only on a subset of the modalities, ignoring the rest. We
show that modality collapse happens when noisy features from one modality are
entangled, via a shared set of neurons in the fusion head, with predictive
features from another, effectively masking out positive contributions from the
predictive features of the former modality and leading to its collapse. We
further prove that cross-modal knowledge distillation implicitly disentangles
such representations by freeing up rank bottlenecks in the student encoder,
denoising the fusion-head outputs without negatively impacting the predictive
features from either modality. Based on the above findings, we propose an
algorithm that prevents modality collapse through explicit basis reallocation,
with applications in dealing with missing modalities. Extensive experiments on
multiple multimodal benchmarks validate our theoretical claims. Project page:
https://abhrac.github.io/mmcollapse/.
\\ ( https://arxiv.org/abs/2505.22483 ,  965kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22486
Date: Wed, 28 May 2025 15:36:02 GMT   (8352kb,D)

Title: Understanding Adversarial Training with Energy-based Models
Authors: Mujtaba Hussain Mirza, Maria Rosaria Briglia, Filippo Bartolucci,
  Senad Beadini, Giuseppe Lisanti, Iacopo Masi
Categories: cs.LG cs.CV
Comments: Under review for TPAMI
\\
  We aim at using Energy-based Model (EBM) framework to better understand
adversarial training (AT) in classifiers, and additionally to analyze the
intrinsic generative capabilities of robust classifiers. By viewing standard
classifiers through an energy lens, we begin by analyzing how the energies of
adversarial examples, generated by various attacks, differ from those of the
natural samples. The central focus of our work is to understand the critical
phenomena of Catastrophic Overfitting (CO) and Robust Overfitting (RO) in AT
from an energy perspective. We analyze the impact of existing AT approaches on
the energy of samples during training and observe that the behavior of the
``delta energy' -- change in energy between original sample and its adversarial
counterpart -- diverges significantly when CO or RO occurs. After a thorough
analysis of these energy dynamics and their relationship with overfitting, we
propose a novel regularizer, the Delta Energy Regularizer (DER), designed to
smoothen the energy landscape during training. We demonstrate that DER is
effective in mitigating both CO and RO across multiple benchmarks. We further
show that robust classifiers, when being used as generative models, have limits
in handling trade-off between image quality and variability. We propose an
improved technique based on a local class-wise principal component analysis
(PCA) and energy-based guidance for better class-specific initialization and
adaptive stopping, enhancing sample diversity and generation quality.
Considering that we do not explicitly train for generative modeling, we achieve
a competitive Inception Score (IS) and Fr\'echet inception distance (FID)
compared to hybrid discriminative-generative models.
\\ ( https://arxiv.org/abs/2505.22486 ,  8352kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22491
Date: Wed, 28 May 2025 15:40:48 GMT   (23596kb,D)

Title: On the Surprising Effectiveness of Large Learning Rates under Standard
  Width Scaling
Authors: Moritz Haas, Sebastian Bordt, Ulrike von Luxburg, Leena Chennuru
  Vankadara
Categories: cs.LG cs.AI stat.ML
\\
  The dominant paradigm for training large-scale vision and language models is
He initialization and a single global learning rate (\textit{standard
parameterization}, SP). Despite its practical success, standard parametrization
remains poorly understood from a theoretical perspective: Existing
infinite-width theory would predict instability under large learning rates and
vanishing feature learning under stable learning rates. However, empirically
optimal learning rates consistently decay much slower than theoretically
predicted. By carefully studying neural network training dynamics, we
demonstrate that this discrepancy is not fully explained by finite-width
phenomena such as catapult effects or a lack of alignment between weights and
incoming activations. We instead show that the apparent contradiction can be
fundamentally resolved by taking the loss function into account: In contrast to
Mean Squared Error (MSE) loss, we prove that under cross-entropy (CE) loss, an
intermediate \textit{controlled divergence} regime emerges, where logits
diverge but loss, gradients, and activations remain stable. Stable training
under large learning rates enables persistent feature evolution at scale in all
hidden layers, which is crucial for the practical success of SP. In experiments
across optimizers (SGD, Adam), architectures (MLPs, GPT) and data modalities
(vision, language), we validate that neural networks operate in this controlled
divergence regime under CE loss but not under MSE loss. Our empirical evidence
suggests that width-scaling considerations are surprisingly useful for
predicting empirically optimal learning rate exponents. Finally, our analysis
clarifies the effectiveness and limitations of recently proposed layerwise
learning rate scalings for standard initialization.
\\ ( https://arxiv.org/abs/2505.22491 ,  23596kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22492
Date: Wed, 28 May 2025 15:42:20 GMT   (1866kb,D)

Title: Demystifying the Paradox of Importance Sampling with an Estimated
  History-Dependent Behavior Policy in Off-Policy Evaluation
Authors: Hongyi Zhou, Josiah P. Hanna, Jin Zhu, Ying Yang, Chengchun Shi
Categories: cs.LG cs.AI stat.ML
Comments: Accepted by ICML 2025
\\
  This paper studies off-policy evaluation (OPE) in reinforcement learning with
a focus on behavior policy estimation for importance sampling. Prior work has
shown empirically that estimating a history-dependent behavior policy can lead
to lower mean squared error (MSE) even when the true behavior policy is
Markovian. However, the question of why the use of history should lower MSE
remains open. In this paper, we theoretically demystify this paradox by
deriving a bias-variance decomposition of the MSE of ordinary importance
sampling (IS) estimators, demonstrating that history-dependent behavior policy
estimation decreases their asymptotic variances while increasing their
finite-sample biases. Additionally, as the estimated behavior policy conditions
on a longer history, we show a consistent decrease in variance. We extend these
findings to a range of other OPE estimators, including the sequential IS
estimator, the doubly robust estimator and the marginalized IS estimator, with
the behavior policy estimated either parametrically or non-parametrically.
\\ ( https://arxiv.org/abs/2505.22492 ,  1866kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22494
Date: Wed, 28 May 2025 15:45:43 GMT   (1838kb)

Title: ProSpero: Active Learning for Robust Protein Design Beyond Wild-Type
  Neighborhoods
Authors: Michal Kmicikiewicz, Vincent Fortuin, Ewa Szczurek
Categories: cs.LG
\\
  Designing protein sequences of both high fitness and novelty is a challenging
task in data-efficient protein engineering. Exploration beyond wild-type
neighborhoods often leads to biologically implausible sequences or relies on
surrogate models that lose fidelity in novel regions. Here, we propose
ProSpero, an active learning framework in which a frozen pre-trained generative
model is guided by a surrogate updated from oracle feedback. By integrating
fitness-relevant residue selection with biologically-constrained Sequential
Monte Carlo sampling, our approach enables exploration beyond wild-type
neighborhoods while preserving biological plausibility. We show that our
framework remains effective even when the surrogate is misspecified. ProSpero
consistently outperforms or matches existing methods across diverse protein
engineering tasks, retrieving sequences of both high fitness and novelty.
\\ ( https://arxiv.org/abs/2505.22494 ,  1838kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22504
Date: Wed, 28 May 2025 15:52:22 GMT   (1348kb,D)

Title: Geometric GNNs for Charged Particle Tracking at GlueX
Authors: Ahmed Hossam Mohammed, Kishansingh Rajput, Simon Taylor, Denis
  Furletov, Sergey Furletov, Malachi Schram
Categories: cs.LG
\\
  Nuclear physics experiments are aimed at uncovering the fundamental building
blocks of matter. The experiments involve high-energy collisions that produce
complex events with many particle trajectories. Tracking charged particles
resulting from collisions in the presence of a strong magnetic field is
critical to enable the reconstruction of particle trajectories and precise
determination of interactions. It is traditionally achieved through
combinatorial approaches that scale worse than linearly as the number of hits
grows. Since particle hit data naturally form a 3-dimensional point cloud and
can be structured as graphs, Graph Neural Networks (GNNs) emerge as an
intuitive and effective choice for this task. In this study, we evaluate the
GNN model for track finding on the data from the GlueX experiment at Jefferson
Lab. We use simulation data to train the model and test on both simulation and
real GlueX measurements. We demonstrate that GNN-based track finding
outperforms the currently used traditional method at GlueX in terms of
segment-based efficiency at a fixed purity while providing faster inferences.
We show that the GNN model can achieve significant speedup by processing
multiple events in batches, which exploits the parallel computation capability
of Graphical Processing Units (GPUs). Finally, we compare the GNN
implementation on GPU and FPGA and describe the trade-off.
\\ ( https://arxiv.org/abs/2505.22504 ,  1348kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22506
Date: Wed, 28 May 2025 15:54:33 GMT   (1808kb)

Title: Sparsification and Reconstruction from the Perspective of Representation
  Geometry
Authors: Wenjie Sun, Bingzhe Wu, Zhile Yang, Chengke Wu
Categories: cs.LG
Comments: 24 pages, 5 figures
MSC-class: 22-08
ACM-class: I.2.4; I.2.7
\\
  Sparse Autoencoders (SAEs) have emerged as a predominant tool in mechanistic
interpretability, aiming to identify interpretable monosemantic features.
However, how does sparse encoding organize the representations of activation
vector from language models? What is the relationship between this
organizational paradigm and feature disentanglement as well as reconstruction
performance? To address these questions, we propose the SAEMA, which validates
the stratified structure of the representation by observing the variability of
the rank of the symmetric semipositive definite (SSPD) matrix corresponding to
the modal tensor unfolded along the latent tensor with the level of noise added
to the residual stream. To systematically investigate how sparse encoding
alters representational structures, we define local and global representations,
demonstrating that they amplify inter-feature distinctions by merging similar
semantic features and introducing additional dimensionality. Furthermore, we
intervene the global representation from an optimization perspective, proving a
significant causal relationship between their separability and the
reconstruction performance. This study explains the principles of sparsity from
the perspective of representational geometry and demonstrates the impact of
changes in representational structure on reconstruction performance.
Particularly emphasizes the necessity of understanding representations and
incorporating representational constraints, providing empirical references for
developing new interpretable tools and improving SAEs. The code is available at
\hyperlink{https://github.com/wenjie1835/SAERepGeo}{https://github.com/wenjie1835/SAERepGeo}.
\\ ( https://arxiv.org/abs/2505.22506 ,  1808kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22509
Date: Wed, 28 May 2025 15:59:13 GMT   (1089kb)

Title: Accelerating Optimization via Differentiable Stopping Time
Authors: Zhonglin Xie, Yiman Fong, Haoran Yuan, Zaiwen Wen
Categories: cs.LG math.OC
\\
  Optimization is an important module of modern machine learning applications.
Tremendous efforts have been made to accelerate optimization algorithms. A
common formulation is achieving a lower loss at a given time. This enables a
differentiable framework with respect to the algorithm hyperparameters. In
contrast, its dual, minimizing the time to reach a target loss, is believed to
be non-differentiable, as the time is not differentiable. As a result, it
usually serves as a conceptual framework or is optimized using zeroth-order
methods. To address this limitation, we propose a differentiable stopping time
and theoretically justify it based on differential equations. An efficient
algorithm is designed to backpropagate through it. As a result, the proposed
differentiable stopping time enables a new differentiable formulation for
accelerating algorithms. We further discuss its applications, such as online
hyperparameter tuning and learning to optimize. Our proposed methods show
superior performance in comprehensive experiments across various problems,
which confirms their effectiveness.
\\ ( https://arxiv.org/abs/2505.22509 ,  1089kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22521
Date: Wed, 28 May 2025 16:08:04 GMT   (304kb)

Title: Evaluating Supervised Learning Models for Fraud Detection: A Comparative
  Study of Classical and Deep Architectures on Imbalanced Transaction Data
Authors: Chao Wang, Chuanhao Nie, Yunbo Liu
Categories: cs.LG cs.AI
Comments: 5 pages. Chao Wang, Chuanhao Nie, and Yunbo Liu contributed equally
  to this work. Corresponding author: Yunbo Liu (yunbo.liu954@duke.edu).
  Submitted to the 3rd International Conference on Management Innovation and
  Economy Development (MIED 2025), Chongqing, China
\\
  Fraud detection remains a critical task in high-stakes domains such as
finance and e-commerce, where undetected fraudulent transactions can lead to
significant economic losses. In this study, we systematically compare the
performance of four supervised learning models - Logistic Regression, Random
Forest, Light Gradient Boosting Machine (LightGBM), and a Gated Recurrent Unit
(GRU) network - on a large-scale, highly imbalanced online transaction dataset.
While ensemble methods such as Random Forest and LightGBM demonstrated superior
performance in both overall and class-specific metrics, Logistic Regression
offered a reliable and interpretable baseline. The GRU model showed strong
recall for the minority fraud class, though at the cost of precision,
highlighting a trade-off relevant for real-world deployment. Our evaluation
emphasizes not only weighted averages but also per-class precision, recall, and
F1-scores, providing a nuanced view of each model's effectiveness in detecting
rare but consequential fraudulent activity. The findings underscore the
importance of choosing models based on the specific risk tolerance and
operational needs of fraud detection systems.
\\ ( https://arxiv.org/abs/2505.22521 ,  304kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22524
Date: Wed, 28 May 2025 16:12:03 GMT   (281kb)

Title: Test-Time Alignment of Discrete Diffusion Models with Sequential Monte
  Carlo
Authors: Chinmay Pani, Zijing Ou, Yingzhen Li
Categories: cs.LG
\\
  Discrete diffusion models have become highly effective across various
domains. However, real-world applications often require the generative process
to adhere to certain constraints but without task-specific fine-tuning. To this
end, we propose a training-free method based on Sequential Monte Carlo (SMC) to
sample from the reward-aligned target distribution at the test time. Our
approach leverages twisted SMC with an approximate locally optimal proposal,
obtained via a first-order Taylor expansion of the reward function. To address
the challenge of ill-defined gradients in discrete spaces, we incorporate a
Gumbel-Softmax relaxation, enabling efficient gradient-based approximation
within the discrete generative framework. Empirical results on both synthetic
datasets and image modelling validate the effectiveness of our approach.
\\ ( https://arxiv.org/abs/2505.22524 ,  281kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22531
Date: Wed, 28 May 2025 16:18:21 GMT   (3227kb)

Title: Training RL Agents for Multi-Objective Network Defense Tasks
Authors: Andres Molina-Markham, Luis Robaina, Sean Steinle, Akash Trivedi,
  Derek Tsui, Nicholas Potteiger, Lauren Brandt, Ransom Winder, Ahmed Ridley
Categories: cs.LG cs.AI cs.CR
\\
  Open-ended learning (OEL) -- which emphasizes training agents that achieve
broad capability over narrow competency -- is emerging as a paradigm to develop
artificial intelligence (AI) agents to achieve robustness and generalization.
However, despite promising results that demonstrate the benefits of OEL,
applying OEL to develop autonomous agents for real-world cybersecurity
applications remains a challenge.
  We propose a training approach, inspired by OEL, to develop autonomous
network defenders. Our results demonstrate that like in other domains, OEL
principles can translate into more robust and generalizable agents for cyber
defense. To apply OEL to network defense, it is necessary to address several
technical challenges. Most importantly, it is critical to provide a task
representation approach over a broad universe of tasks that maintains a
consistent interface over goals, rewards and action spaces. This way, the
learning agent can train with varying network conditions, attacker behaviors,
and defender goals while being able to build on previously gained knowledge.
  With our tools and results, we aim to fundamentally impact research that
applies AI to solve cybersecurity problems. Specifically, as researchers
develop gyms and benchmarks for cyber defense, it is paramount that they
consider diverse tasks with consistent representations, such as those we
propose in our work.
\\ ( https://arxiv.org/abs/2505.22531 ,  3227kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22533
Date: Wed, 28 May 2025 16:19:39 GMT   (553kb)

Title: TabularQGAN: A Quantum Generative Model for Tabular Data
Authors: Pallavi Bhardwaj, Caitlin Jones, Lasse Dierich, Aleksandar
  Vu\v{c}kovi\'c
Categories: cs.LG cs.AI quant-ph
Comments: 18 pages,8 figures and 4 tables
\\
  In this paper, we introduce a novel quantum generative model for synthesizing
tabular data. Synthetic data is valuable in scenarios where real-world data is
scarce or private, it can be used to augment or replace existing datasets.
Real-world enterprise data is predominantly tabular and heterogeneous, often
comprising a mixture of categorical and numerical features, making it highly
relevant across various industries such as healthcare, finance, and software.
We propose a quantum generative adversarial network architecture with flexible
data encoding and a novel quantum circuit ansatz to effectively model tabular
data. The proposed approach is tested on the MIMIC III healthcare and Adult
Census datasets, with extensive benchmarking against leading classical models,
CTGAN, and CopulaGAN. Experimental results demonstrate that our quantum model
outperforms classical models by an average of 8.5% with respect to an overall
similarity score from SDMetrics, while using only 0.072% of the parameters of
the classical models. Additionally, we evaluate the generalization capabilities
of the models using two custom-designed metrics that demonstrate the ability of
the proposed quantum model to generate useful and novel samples. To our
knowledge, this is one of the first demonstrations of a successful quantum
generative model for handling tabular data, indicating that this task could be
well-suited to quantum computers.
\\ ( https://arxiv.org/abs/2505.22533 ,  553kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22538
Date: Wed, 28 May 2025 16:22:53 GMT   (244kb)

Title: Uncertainty Quantification with Proper Scoring Rules: Adjusting Measures
  to Prediction Tasks
Authors: Paul Hofman, Yusuf Sale, Eyke H\"ullermeier
Categories: cs.LG stat.ML
\\
  We address the problem of uncertainty quantification and propose measures of
total, aleatoric, and epistemic uncertainty based on a known decomposition of
(strictly) proper scoring rules, a specific type of loss function, into a
divergence and an entropy component. This leads to a flexible framework for
uncertainty quantification that can be instantiated with different losses
(scoring rules), which makes it possible to tailor uncertainty quantification
to the use case at hand. We show that this flexibility is indeed advantageous.
In particular, we analyze the task of selective prediction and show that the
scoring rule should ideally match the task loss. In addition, we perform
experiments on two other common tasks. For out-of-distribution detection, our
results confirm that a widely used measure of epistemic uncertainty, mutual
information, performs best. Moreover, in the setting of active learning, our
measure of epistemic uncertainty based on the zero-one-loss consistently
outperforms other uncertainty measures.
\\ ( https://arxiv.org/abs/2505.22538 ,  244kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22541
Date: Wed, 28 May 2025 16:23:48 GMT   (25603kb)

Title: A Human-Centric Approach to Explainable AI for Personalized Education
Authors: Vinitra Swamy
Categories: cs.LG cs.CY
Comments: PhD Thesis, EPFL (Computer Science)
\\
  Deep neural networks form the backbone of artificial intelligence research,
with potential to transform the human experience in areas ranging from
autonomous driving to personal assistants, healthcare to education. However,
their integration into the daily routines of real-world classrooms remains
limited. It is not yet common for a teacher to assign students individualized
homework targeting their specific weaknesses, provide students with instant
feedback, or simulate student responses to a new exam question. While these
models excel in predictive performance, this lack of adoption can be attributed
to a significant weakness: the lack of explainability of model decisions,
leading to a lack of trust from students, parents, and teachers. This thesis
aims to bring human needs to the forefront of eXplainable AI (XAI) research,
grounded in the concrete use case of personalized learning and teaching. We
frame the contributions along two verticals: technical advances in XAI and
their aligned human studies. We investigate explainability in AI for education,
revealing systematic disagreements between post-hoc explainers and identifying
a need for inherently interpretable model architectures. We propose four novel
technical contributions in interpretability with a multimodal modular
architecture (MultiModN), an interpretable mixture-of-experts model
(InterpretCC), adversarial training for explainer stability, and a
theory-driven LLM-XAI framework to present explanations to students
(iLLuMinaTE), which we evaluate in diverse settings with professors, teachers,
learning scientists, and university students. By combining empirical
evaluations of existing explainers with novel architectural designs and human
studies, our work lays a foundation for human-centric AI systems that balance
state-of-the-art performance with built-in transparency and trust.
\\ ( https://arxiv.org/abs/2505.22541 ,  25603kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22549
Date: Wed, 28 May 2025 16:32:33 GMT   (38093kb,D)

Title: DES-LOC: Desynced Low Communication Adaptive Optimizers for Training
  Foundation Models
Authors: Alex Iacob, Lorenzo Sani, Mher Safaryan, Paris Giampouras, Samuel
  Horv\'ath, Andrej Jovanovic, Meghdad Kurmanji, Preslav Aleksandrov, William
  F. Shen, Xinchi Qiu, Nicholas D. Lane
Categories: cs.LG
Comments: Keywords: Distributed Training, Foundation Models, Large Language
  Models, Optimizers, Communication Efficiency, Federated Learning, Distributed
  Systems, Optimization Theory, Scaling, Robustness. Preprint, under review at
  NeurIPS
\\
  Scaling foundation model training with Distributed Data Parallel (DDP)
methods is bandwidth-limited. Existing infrequent communication methods like
Local SGD were designed to synchronize only model parameters and cannot be
trivially applied to adaptive optimizers due to additional optimizer states.
Current approaches extending Local SGD either lack convergence guarantees or
require synchronizing all optimizer states, tripling communication costs. We
propose Desynced Low Communication Adaptive Optimizers (DES-LOC), a family of
optimizers assigning independent synchronization periods to parameters and
momenta, enabling lower communication costs while preserving convergence.
Through extensive experiments on language models of up to 1.7B, we show that
DES-LOC can communicate 170x less than DDP and 2x less than the previous
state-of-the-art Local ADAM. Furthermore, unlike previous heuristic approaches,
DES-LOC is suited for practical training scenarios prone to system failures.
DES-LOC offers a scalable, bandwidth-efficient, and fault-tolerant solution for
foundation model training.
\\ ( https://arxiv.org/abs/2505.22549 ,  38093kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22560
Date: Wed, 28 May 2025 16:38:35 GMT   (1747kb)

Title: Geometric Hyena Networks for Large-scale Equivariant Learning
Authors: Artem Moskalev, Mangal Prakash, Junjie Xu, Tianyu Cui, Rui Liao,
  Tommaso Mansi
Categories: cs.LG
\\
  Processing global geometric context while preserving equivariance is crucial
when modeling biological, chemical, and physical systems. Yet, this is
challenging due to the computational demands of equivariance and global context
at scale. Standard methods such as equivariant self-attention suffer from
quadratic complexity, while local methods such as distance-based message
passing sacrifice global information. Inspired by the recent success of
state-space and long-convolutional models, we introduce Geometric Hyena, the
first equivariant long-convolutional model for geometric systems. Geometric
Hyena captures global geometric context at sub-quadratic complexity while
maintaining equivariance to rotations and translations. Evaluated on all-atom
property prediction of large RNA molecules and full protein molecular dynamics,
Geometric Hyena outperforms existing equivariant models while requiring
significantly less memory and compute that equivariant self-attention. Notably,
our model processes the geometric context of 30k tokens 20x faster than the
equivariant transformer and allows 72x longer context within the same budget.
\\ ( https://arxiv.org/abs/2505.22560 ,  1747kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22573
Date: Wed, 28 May 2025 16:46:56 GMT   (4689kb,D)

Title: FNOPE: Simulation-based inference on function spaces with Fourier Neural
  Operators
Authors: Guy Moss, Leah Sophie Muhle, Reinhard Drews, Jakob H. Macke, Cornelius
  Schr\"oder
Categories: cs.LG
\\
  Simulation-based inference (SBI) is an established approach for performing
Bayesian inference on scientific simulators. SBI so far works best on
low-dimensional parametric models. However, it is difficult to infer
function-valued parameters, which frequently occur in disciplines that model
spatiotemporal processes such as the climate and earth sciences. Here, we
introduce an approach for efficient posterior estimation, using a Fourier
Neural Operator (FNO) architecture with a flow matching objective. We show that
our approach, FNOPE, can perform inference of function-valued parameters at a
fraction of the simulation budget of state of the art methods. In addition,
FNOPE supports posterior evaluation at arbitrary discretizations of the domain,
as well as simultaneous estimation of vector-valued parameters. We demonstrate
the effectiveness of our approach on several benchmark tasks and a challenging
spatial inference task from glaciology. FNOPE extends the applicability of SBI
methods to new scientific domains by enabling the inference of function-valued
parameters.
\\ ( https://arxiv.org/abs/2505.22573 ,  4689kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22578
Date: Wed, 28 May 2025 16:53:48 GMT   (127kb)

Title: Benignity of loss landscape with weight decay requires both large
  overparametrization and initialization
Authors: Etienne Boursier, Matthew Bowditch, Matthias Englert, Ranko Lazic
Categories: cs.LG
\\
  The optimization of neural networks under weight decay remains poorly
understood from a theoretical standpoint. While weight decay is standard
practice in modern training procedures, most theoretical analyses focus on
unregularized settings. In this work, we investigate the loss landscape of the
$\ell_2$-regularized training loss for two-layer ReLU networks. We show that
the landscape becomes benign -- i.e., free of spurious local minima -- under
large overparametrization, specifically when the network width $m$ satisfies $m
\gtrsim \min(n^d, 2^n)$, where $n$ is the number of data points and $d$ the
input dimension. More precisely in this regime, almost all constant activation
regions contain a global minimum and no spurious local minima. We further show
that this level of overparametrization is not only sufficient but also
necessary via the example of orthogonal data. Finally, we demonstrate that such
loss landscape results primarily hold relevance in the large initialization
regime. In contrast, for small initializations -- corresponding to the feature
learning regime -- optimization can still converge to spurious local minima,
despite the global benignity of the landscape.
\\ ( https://arxiv.org/abs/2505.22578 ,  127kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22601
Date: Wed, 28 May 2025 17:14:57 GMT   (1318kb)

Title: Machine Unlearning under Overparameterization
Authors: Jacob L. Block, Aryan Mokhtari, Sanjay Shakkottai
Categories: cs.LG cs.AI
\\
  Machine unlearning algorithms aim to remove the influence of specific
training samples, ideally recovering the model that would have resulted from
training on the remaining data alone. We study unlearning in the
overparameterized setting, where many models interpolate the data, and defining
the unlearning solution as any loss minimizer over the retained
set$\unicode{x2013}$as in prior work in the underparameterized
setting$\unicode{x2013}$is inadequate, since the original model may already
interpolate the retained data and satisfy this condition. In this regime, loss
gradients vanish, rendering prior methods based on gradient perturbations
ineffective, motivating both new unlearning definitions and algorithms. For
this setting, we define the unlearning solution as the minimum-complexity
interpolator over the retained data and propose a new algorithmic framework
that only requires access to model gradients on the retained set at the
original solution. We minimize a regularized objective over perturbations
constrained to be orthogonal to these model gradients, a first-order relaxation
of the interpolation condition. For different model classes, we provide exact
and approximate unlearning guarantees, and we demonstrate that an
implementation of our framework outperforms existing baselines across various
unlearning experiments.
\\ ( https://arxiv.org/abs/2505.22601 ,  1318kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22602
Date: Wed, 28 May 2025 17:16:24 GMT   (7618kb,D)

Title: One Rank at a Time: Cascading Error Dynamics in Sequential Learning
Authors: Mahtab Alizadeh Vandchali, Fangshuo (Jasper) Liao, Anastasios
  Kyrillidis
Categories: cs.LG cs.AI math.OC
Comments: 36 pages
\\
  Sequential learning -- where complex tasks are broken down into simpler,
hierarchical components -- has emerged as a paradigm in AI. This paper views
sequential learning through the lens of low-rank linear regression, focusing
specifically on how errors propagate when learning rank-1 subspaces
sequentially. We present an analysis framework that decomposes the learning
process into a series of rank-1 estimation problems, where each subsequent
estimation depends on the accuracy of previous steps. Our contribution is a
characterization of the error propagation in this sequential process,
establishing bounds on how errors -- e.g., due to limited computational budgets
and finite precision -- affect the overall model accuracy. We prove that these
errors compound in predictable ways, with implications for both algorithmic
design and stability guarantees.
\\ ( https://arxiv.org/abs/2505.22602 ,  7618kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22617
Date: Wed, 28 May 2025 17:38:45 GMT   (1508kb,D)

Title: The Entropy Mechanism of Reinforcement Learning for Reasoning Language
  Models
Authors: Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin
  Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, Zhiyuan Liu, Hao Peng,
  Lei Bai, Wanli Ouyang, Yu Cheng, Bowen Zhou and Ning Ding
Categories: cs.LG cs.AI cs.CL
\\
  This paper aims to overcome a major obstacle in scaling RL for reasoning with
LLMs, namely the collapse of policy entropy. Such phenomenon is consistently
observed across vast RL runs without entropy intervention, where the policy
entropy dropped sharply at the early training stage, this diminished
exploratory ability is always accompanied with the saturation of policy
performance. In practice, we establish a transformation equation R=-a*e^H+b
between entropy H and downstream performance R. This empirical law strongly
indicates that, the policy performance is traded from policy entropy, thus
bottlenecked by its exhaustion, and the ceiling is fully predictable H=0,
R=-a+b. Our finding necessitates entropy management for continuous exploration
toward scaling compute for RL. To this end, we investigate entropy dynamics
both theoretically and empirically. Our derivation highlights that, the change
in policy entropy is driven by the covariance between action probability and
the change in logits, which is proportional to its advantage when using Policy
Gradient-like algorithms. Empirical study shows that, the values of covariance
term and entropy differences matched exactly, supporting the theoretical
conclusion. Moreover, the covariance term stays mostly positive throughout
training, further explaining why policy entropy would decrease monotonically.
Through understanding the mechanism behind entropy dynamics, we motivate to
control entropy by restricting the update of high-covariance tokens.
Specifically, we propose two simple yet effective techniques, namely Clip-Cov
and KL-Cov, which clip and apply KL penalty to tokens with high covariances
respectively. Experiments show that these methods encourage exploration, thus
helping policy escape entropy collapse and achieve better downstream
performance.
\\ ( https://arxiv.org/abs/2505.22617 ,  1508kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22637
Date: Wed, 28 May 2025 17:53:31 GMT   (510kb)

Title: Understanding (Un)Reliability of Steering Vectors in Language Models
Authors: Joschka Braun, Carsten Eickhoff, David Krueger, Seyed Ali Bahrainian,
  Dmitrii Krasheninnikov
Categories: cs.LG
Comments: 17 pages, 10 figures. Presented at the ICLR 2025 Workshop on
  Foundation Models in the Wild
\\
  Steering vectors are a lightweight method to control language model behavior
by adding a learned bias to the activations at inference time. Although
steering demonstrates promising performance, recent work shows that it can be
unreliable or even counterproductive in some cases. This paper studies the
influence of prompt types and the geometry of activation differences on
steering reliability. First, we find that all seven prompt types used in our
experiments produce a net positive steering effect, but exhibit high variance
across samples, and often give an effect opposite of the desired one. No prompt
type clearly outperforms the others, and yet the steering vectors resulting
from the different prompt types often differ directionally (as measured by
cosine similarity). Second, we show that higher cosine similarity between
training set activation differences predicts more effective steering. Finally,
we observe that datasets where positive and negative activations are better
separated are more steerable. Our results suggest that vector steering is
unreliable when the target behavior is not represented by a coherent direction.
\\ ( https://arxiv.org/abs/2505.22637 ,  510kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22641
Date: Wed, 28 May 2025 17:54:39 GMT   (1525kb)

Title: Spectral Survival Analysis
Authors: Chengzhi Shi and Stratis Ioannidis
Categories: cs.LG
Comments: Extended version of conference paper appearing in KDD 2025
DOI: 10.1145/3711896.3737134
\\
  Survival analysis is widely deployed in a diverse set of fields, including
healthcare, business, ecology, etc. The Cox Proportional Hazard (CoxPH) model
is a semi-parametric model often encountered in the literature. Despite its
popularity, wide deployment, and numerous variants, scaling CoxPH to large
datasets and deep architectures poses a challenge, especially in the
high-dimensional regime. We identify a fundamental connection between rank
regression and the CoxPH model: this allows us to adapt and extend the
so-called spectral method for rank regression to survival analysis. Our
approach is versatile, naturally generalizing to several CoxPH variants,
including deep models. We empirically verify our method's scalability on
multiple real-world high-dimensional datasets; our method outperforms legacy
methods w.r.t. predictive performance and efficiency.
\\ ( https://arxiv.org/abs/2505.22641 ,  1525kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22650
Date: Wed, 28 May 2025 17:57:29 GMT   (25kb)

Title: On Learning Verifiers for Chain-of-Thought Reasoning
Authors: Maria-Florina Balcan, Avrim Blum, Zhiyuan Li, Dravyansh Sharma
Categories: cs.LG
\\
  Chain-of-Thought reasoning has emerged as a powerful approach for solving
complex mathematical and logical problems. However, it can often veer off track
through incorrect or unsubstantiated inferences. Formal mathematical reasoning,
which can be checked with a formal verifier, is one approach to addressing this
issue. However, currently LLMs are simply not good enough to solve complex
problems in a formal way, and even just formalizing an informal problem
statement can be challenging. Motivated by this fact, in this work we consider
the problem of learning reliable verifiers for natural language
Chain-of-Thought reasoning. That is, given a problem statement and step-by-step
solution in natural language, the aim of the verifier is to output [Yes] if the
reasoning steps in the solution are all valid, and [No] otherwise. In this work
we give a formal PAC-learning framework for studying this problem. We propose
and analyze several natural verification goals, at different levels of
strength, in this framework. We provide sample complexity upper-bounds for
learning verifiers satisfying these goals, as well as lower-bound and
impossibility results for learning other natural verification objectives
without additional assumptions.
\\ ( https://arxiv.org/abs/2505.22650 ,  25kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22655
Date: Wed, 28 May 2025 17:59:08 GMT   (1476kb)

Title: Position: Uncertainty Quantification Needs Reassessment for
  Large-language Model Agents
Authors: Michael Kirchhof, Gjergji Kasneci, Enkelejda Kasneci
Categories: cs.LG cs.AI cs.CL
Comments: Accepted at ICML 2025
\\
  Large-language models (LLMs) and chatbot agents are known to provide wrong
outputs at times, and it was recently found that this can never be fully
prevented. Hence, uncertainty quantification plays a crucial role, aiming to
quantify the level of ambiguity in either one overall number or two numbers for
aleatoric and epistemic uncertainty. This position paper argues that this
traditional dichotomy of uncertainties is too limited for the open and
interactive setup that LLM agents operate in when communicating with a user,
and that we need to research avenues that enrich uncertainties in this novel
scenario. We review the literature and find that popular definitions of
aleatoric and epistemic uncertainties directly contradict each other and lose
their meaning in interactive LLM agent settings. Hence, we propose three novel
research directions that focus on uncertainties in such human-computer
interactions: Underspecification uncertainties, for when users do not provide
all information or define the exact task at the first go, interactive learning,
to ask follow-up questions and reduce the uncertainty about the current
context, and output uncertainties, to utilize the rich language and speech
space to express uncertainties as more than mere numbers. We expect that these
new ways of dealing with and communicating uncertainties will lead to LLM agent
interactions that are more transparent, trustworthy, and intuitive.
\\ ( https://arxiv.org/abs/2505.22655 ,  1476kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22660
Date: Wed, 28 May 2025 17:59:37 GMT   (413kb)

Title: Maximizing Confidence Alone Improves Reasoning
Authors: Mihir Prabhudesai, Lili Chen, Alex Ippoliti, Katerina Fragkiadaki, Hao
  Liu, Deepak Pathak
Categories: cs.LG cs.AI
\\
  Reinforcement learning (RL) has enabled machine learning models to achieve
significant advances in many fields. Most recently, RL has empowered frontier
language models to solve challenging math, science, and coding problems.
However, central to any RL algorithm is the reward function, and reward
engineering is a notoriously difficult problem in any domain. In this paper, we
propose RENT: Reinforcement Learning via Entropy Minimization -- a fully
unsupervised RL method that requires no external reward or ground-truth
answers, and instead uses the model's entropy of its underlying distribution as
an intrinsic reward. We find that by reinforcing the chains of thought that
yield high model confidence on its generated answers, the model improves its
reasoning ability. In our experiments, we showcase these improvements on an
extensive suite of commonly-used reasoning benchmarks, including GSM8K,
MATH500, AMC, AIME, and GPQA, and models of varying sizes from the Qwen and
Mistral families. The generality of our unsupervised learning method lends
itself to applicability in a wide range of domains where external supervision
is limited or unavailable.
\\ ( https://arxiv.org/abs/2505.22660 ,  413kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21534
Date: Fri, 23 May 2025 22:26:22 GMT   (204kb)

Title: Uncovering Bottlenecks and Optimizing Scientific Lab Workflows with
  Cycle Time Reduction Agents
Authors: Yao Fehlis
Categories: cs.MA cs.AI
\\
  Scientific laboratories, particularly those in pharmaceutical and
biotechnology companies, encounter significant challenges in optimizing
workflows due to the complexity and volume of tasks such as compound screening
and assay execution. We introduce Cycle Time Reduction Agents (CTRA), a
LangGraph-based agentic workflow designed to automate the analysis of lab
operational metrics. CTRA comprises three main components: the Question
Creation Agent for initiating analysis, Operational Metrics Agents for data
extraction and validation, and Insights Agents for reporting and visualization,
identifying bottlenecks in lab processes. This paper details CTRA's
architecture, evaluates its performance on a lab dataset, and discusses its
potential to accelerate pharmaceutical and biotechnological development. CTRA
offers a scalable framework for reducing cycle times in scientific labs.
\\ ( https://arxiv.org/abs/2505.21534 ,  204kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21559
Date: Mon, 26 May 2025 20:39:31 GMT   (582kb)

Title: Streamlining Resilient Kubernetes Autoscaling with Multi-Agent Systems
  via an Automated Online Design Framework
Authors: Julien Soul\'e, Jean-Paul Jamont, Michel Occello, Louis-Marie
  Traonouez, Paul Th\'eron
Categories: cs.MA cs.AI
\\
  In cloud-native systems, Kubernetes clusters with interdependent services
often face challenges to their operational resilience due to poor workload
management issues such as resource blocking, bottlenecks, or continuous pod
crashes. These vulnerabilities are further amplified in adversarial scenarios,
such as Distributed Denial-of-Service attacks (DDoS). Conventional Horizontal
Pod Autoscaling (HPA) approaches struggle to address such dynamic conditions,
while reinforcement learning-based methods, though more adaptable, typically
optimize single goals like latency or resource usage, neglecting broader
failure scenarios. We propose decomposing the overarching goal of maintaining
operational resilience into failure-specific sub-goals delegated to
collaborative agents, collectively forming an HPA Multi-Agent System (MAS). We
introduce an automated, four-phase online framework for HPA MAS design: 1)
modeling a digital twin built from cluster traces; 2) training agents in
simulation using roles and missions tailored to failure contexts; 3) analyzing
agent behaviors for explainability; and 4) transferring learned policies to the
real cluster. Experimental results demonstrate that the generated HPA MASs
outperform three state-of-the-art HPA systems in sustaining operational
resilience under various adversarial conditions in a proposed complex cluster.
\\ ( https://arxiv.org/abs/2505.21559 ,  582kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21588
Date: Tue, 27 May 2025 12:12:56 GMT   (1197kb)

Title: Herd Behavior: Investigating Peer Influence in LLM-based Multi-Agent
  Systems
Authors: Young-Min Cho, Sharath Chandra Guntuku, Lyle Ungar
Categories: cs.MA cs.AI
Comments: Preprint
\\
  Recent advancements in Large Language Models (LLMs) have enabled the
emergence of multi-agent systems where LLMs interact, collaborate, and make
decisions in shared environments. While individual model behavior has been
extensively studied, the dynamics of peer influence in such systems remain
underexplored. In this paper, we investigate herd behavior, the tendency of
agents to align their outputs with those of their peers, within LLM-based
multi-agent interactions. We present a series of controlled experiments that
reveal how herd behaviors are shaped by multiple factors. First, we show that
the gap between self-confidence and perceived confidence in peers significantly
impacts an agent's likelihood to conform. Second, we find that the format in
which peer information is presented plays a critical role in modulating the
strength of herd behavior. Finally, we demonstrate that the degree of herd
behavior can be systematically controlled, and that appropriately calibrated
herd tendencies can enhance collaborative outcomes. These findings offer new
insights into the social dynamics of LLM-based systems and open pathways for
designing more effective and adaptive multi-agent collaboration frameworks.
\\ ( https://arxiv.org/abs/2505.21588 ,  1197kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21741
Date: Tue, 27 May 2025 20:29:53 GMT   (7111kb)

Title: AI-Supported Platform for System Monitoring and Decision-Making in
  Nuclear Waste Management with Large Language Models
Authors: Dongjune Chang, Sola Kim, Young Soo Park
Categories: cs.MA cs.CY cs.IR
Journal-ref: Proceedings of the WM2025 Conference, March 9-13, 2025, Phoenix,
  Arizona, USA
\\
  Nuclear waste management requires rigorous regulatory compliance assessment,
demanding advanced decision-support systems capable of addressing complex
legal, environmental, and safety considerations. This paper presents a
multi-agent Retrieval-Augmented Generation (RAG) system that integrates large
language models (LLMs) with document retrieval mechanisms to enhance decision
accuracy through structured agent collaboration. Through a structured 10-round
discussion model, agents collaborate to assess regulatory compliance and safety
requirements while maintaining document-grounded responses. Implemented on
consumer-grade hardware, the system leverages Llama 3.2 and
mxbai-embed-large-v1 embeddings for efficient retrieval and semantic
representation. A case study of a proposed temporary nuclear waste storage site
near Winslow, Arizona, demonstrates the framework's effectiveness. Results show
the Regulatory Agent achieves consistently higher relevance scores in
maintaining alignment with legal frameworks, while the Safety Agent effectively
manages complex risk assessments requiring multifaceted analysis. The system
demonstrates progressive improvement in agreement rates between agents across
discussion rounds while semantic drift decreases, indicating enhanced
decision-making consistency and response coherence. The system ensures
regulatory decisions remain factually grounded, dynamically adapting to
evolving regulatory frameworks through real-time document retrieval. By
balancing automated assessment with human oversight, this framework offers a
scalable and transparent approach to regulatory governance. These findings
underscore the potential of AI-driven, multi-agent systems in advancing
evidence-based, accountable, and adaptive decision-making for high-stakes
environmental management scenarios.
\\ ( https://arxiv.org/abs/2505.21741 ,  7111kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21880
Date: Wed, 28 May 2025 01:54:28 GMT   (4324kb)

Title: Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation
Authors: Yu-Lun Song, Chung-En Tsern, Che-Cheng Wu, Yu-Ming Chang, Syuan-Bo
  Huang, Wei-Chu Chen, Michael Chia-Liang Lin, Yu-Ta Lin
Categories: cs.MA cs.AI cs.CL cs.CY
Comments: 8 pages, 8 figures. This paper is reviewed and accepted by the CUPUM
  (Computational Urban Planning and Urban Management) Conference held by
  University College London (UCL) in 2025
\\
  This study presents an innovative approach to urban mobility simulation by
integrating a Large Language Model (LLM) with Agent-Based Modeling (ABM).
Unlike traditional rule-based ABM, the proposed framework leverages LLM to
enhance agent diversity and realism by generating synthetic population
profiles, allocating routine and occasional locations, and simulating
personalized routes. Using real-world data, the simulation models individual
behaviors and large-scale mobility patterns in Taipei City. Key insights, such
as route heat maps and mode-specific indicators, provide urban planners with
actionable information for policy-making. Future work focuses on establishing
robust validation frameworks to ensure accuracy and reliability in urban
planning applications.
\\ ( https://arxiv.org/abs/2505.21880 ,  4324kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21985
Date: Wed, 28 May 2025 05:23:47 GMT   (943kb)

Title: Reward-Independent Messaging for Decentralized Multi-Agent Reinforcement
  Learning
Authors: Naoto Yoshida and Tadahiro Taniguchi
Categories: cs.MA cs.AI cs.LG
\\
  In multi-agent reinforcement learning (MARL), effective communication
improves agent performance, particularly under partial observability. We
propose MARL-CPC, a framework that enables communication among fully
decentralized, independent agents without parameter sharing. MARL-CPC
incorporates a message learning model based on collective predictive coding
(CPC) from emergent communication research. Unlike conventional methods that
treat messages as part of the action space and assume cooperation, MARL-CPC
links messages to state inference, supporting communication in non-cooperative,
reward-independent settings. We introduce two algorithms -Bandit-CPC and
IPPO-CPC- and evaluate them in non-cooperative MARL tasks. Benchmarks show that
both outperform standard message-as-action approaches, establishing effective
communication even when messages offer no direct benefit to the sender. These
results highlight MARL-CPC's potential for enabling coordination in complex,
decentralized environments.
\\ ( https://arxiv.org/abs/2505.21985 ,  943kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22125
Date: Wed, 28 May 2025 08:50:56 GMT   (1552kb)

Title: Sentiment Simulation using Generative AI Agents
Authors: Melrose Tia, Jezreel Sophia Lanuzo, Lei Rigi Baltazar, Marie Joy
  Lopez-Relente, Diwa Malaya Qui\~nones, Jason Albia
Categories: cs.MA cs.AI cs.CY
Comments: 18 pages, 10 figures
ACM-class: I.2; I.6; J.4
\\
  Traditional sentiment analysis relies on surface-level linguistic patterns
and retrospective data, limiting its ability to capture the psychological and
contextual drivers of human sentiment. These limitations constrain its
effectiveness in applications that require predictive insight, such as policy
testing, narrative framing, and behavioral forecasting. We present a robust
framework for sentiment simulation using generative AI agents embedded with
psychologically rich profiles. Agents are instantiated from a nationally
representative survey of 2,485 Filipino respondents, combining sociodemographic
information with validated constructs of personality traits, values, beliefs,
and socio-political attitudes. The framework includes three stages: (1) agent
embodiment via categorical or contextualized encodings, (2) exposure to
real-world political and economic scenarios, and (3) generation of sentiment
ratings accompanied by explanatory rationales. Using Quadratic Weighted
Accuracy (QWA), we evaluated alignment between agent-generated and human
responses. Contextualized encoding achieved 92% alignment in replicating
original survey responses. In sentiment simulation tasks, agents reached
81%--86% accuracy against ground truth sentiment, with contextualized profile
encodings significantly outperforming categorical (p < 0.0001, Cohen's d =
0.70). Simulation results remained consistent across repeated trials
(+/-0.2--0.5% SD) and resilient to variation in scenario framing (p = 0.9676,
Cohen's d = 0.02). Our findings establish a scalable framework for sentiment
modeling through psychographically grounded AI agents. This work signals a
paradigm shift in sentiment analysis from retrospective classification to
prospective and dynamic simulation grounded in psychology of sentiment
formation.
\\ ( https://arxiv.org/abs/2505.22125 ,  1552kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22192
Date: Wed, 28 May 2025 10:08:31 GMT   (801kb)

Title: Efficient Leave-one-out Approximation in LLM Multi-agent Debate Based on
  Introspection
Authors: Yue Cui, Liuyi Yao, Zitao Li, Yaliang Li, Bolin Ding, Xiaofang Zhou
Categories: cs.MA
\\
  Multi-agent systems based on large language models (LLMs) advance automatic
task completion in various fields, where debate is a common cooperation form
for agents to solve complicated problems with reasoning and cross-review to
solidify answers. Assessing the individual contributions of agents within these
debates is crucial for system refinement and outcome reliability. Traditional
leave-one-out (LOO) method offers a clear framework for evaluating each agent's
role but face challenges in LLM-based systems due to high computational costs
and associated financial implications. This paper presents
introspective-leave-one-out (IntrospecLOO), a simple yet effective prompting
for approximation of LOO in LLM-powered multi-agent debates. IntrospecLOO
introduces an additional querying round after standard debates, prompting
agents to update their answers while ignoring responses from a designated
agent. This strategy effectively isolates and gauges each participant's
influence at a reduced query complexity compared to the original LOO
approaches. Validation through experiments on three benchmark datasets confirms
the effectiveness of IntrospecLOO.
\\ ( https://arxiv.org/abs/2505.22192 ,  801kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22467
Date: Wed, 28 May 2025 15:20:09 GMT   (328kb)

Title: Topological Structure Learning Should Be A Research Priority for
  LLM-Based Multi-Agent Systems
Authors: Jiaxi Yang, Mengqi Zhang, Yiqiao Jin, Hao Chen, Qingsong Wen, Lu Lin,
  Yi He, Weijie Xu, James Evans, Jindong Wang
Categories: cs.MA cs.AI cs.LG
\\
  Large Language Model-based Multi-Agent Systems (MASs) have emerged as a
powerful paradigm for tackling complex tasks through collaborative
intelligence. Nevertheless, the question of how agents should be structurally
organized for optimal cooperation remains largely unexplored. In this position
paper, we aim to gently redirect the focus of the MAS research community toward
this critical dimension: develop topology-aware MASs for specific tasks.
Specifically, the system consists of three core components - agents,
communication links, and communication patterns - that collectively shape its
coordination performance and efficiency. To this end, we introduce a
systematic, three-stage framework: agent selection, structure profiling, and
topology synthesis. Each stage would trigger new research opportunities in
areas such as language models, reinforcement learning, graph learning, and
generative modeling; together, they could unleash the full potential of MASs in
complicated real-world applications. Then, we discuss the potential challenges
and opportunities in the evaluation of multiple systems. We hope our
perspective and framework can offer critical new insights in the era of agentic
AI.
\\ ( https://arxiv.org/abs/2505.22467 ,  328kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21565
Date: Tue, 27 May 2025 05:04:01 GMT   (11631kb,D)

Title: Towards Human-Like Trajectory Prediction for Autonomous Driving: A
  Behavior-Centric Approach
Authors: Haicheng Liao, Zhenning Li, Guohui Zhang, Keqiang Li, Chengzhong Xu
Categories: cs.RO cs.AI
\\
  Predicting the trajectories of vehicles is crucial for the development of
autonomous driving (AD) systems, particularly in complex and dynamic traffic
environments. In this study, we introduce HiT (Human-like Trajectory
Prediction), a novel model designed to enhance trajectory prediction by
incorporating behavior-aware modules and dynamic centrality measures. Unlike
traditional methods that primarily rely on static graph structures, HiT
leverages a dynamic framework that accounts for both direct and indirect
interactions among traffic participants. This allows the model to capture the
subtle yet significant influences of surrounding vehicles, enabling more
accurate and human-like predictions. To evaluate HiT's performance, we
conducted extensive experiments using diverse and challenging real-world
datasets, including NGSIM, HighD, RounD, ApolloScape, and MoCAD++. The results
demonstrate that HiT consistently outperforms other top models across multiple
metrics, particularly excelling in scenarios involving aggressive driving
behaviors. This research presents a significant step forward in trajectory
prediction, offering a more reliable and interpretable approach for enhancing
the safety and efficiency of fully autonomous driving systems.
\\ ( https://arxiv.org/abs/2505.21565 ,  11631kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21581
Date: Tue, 27 May 2025 09:58:43 GMT   (6939kb)

Title: CogAD: Cognitive-Hierarchy Guided End-to-End Autonomous Driving
Authors: Zhennan Wang, Jianing Teng, Canqun Xiang, Kangliang Chen, Xing Pan, Lu
  Deng, Weihao Gu
Categories: cs.RO cs.CV
\\
  While end-to-end autonomous driving has advanced significantly, prevailing
methods remain fundamentally misaligned with human cognitive principles in both
perception and planning. In this paper, we propose CogAD, a novel end-to-end
autonomous driving model that emulates the hierarchical cognition mechanisms of
human drivers. CogAD implements dual hierarchical mechanisms: global-to-local
context processing for human-like perception and intent-conditioned multi-mode
trajectory generation for cognitively-inspired planning. The proposed method
demonstrates three principal advantages: comprehensive environmental
understanding through hierarchical perception, robust planning exploration
enabled by multi-level planning, and diverse yet reasonable multi-modal
trajectory generation facilitated by dual-level uncertainty modeling. Extensive
experiments on nuScenes and Bench2Drive demonstrate that CogAD achieves
state-of-the-art performance in end-to-end planning, exhibiting particular
superiority in long-tail scenarios and robust generalization to complex
real-world driving conditions.
\\ ( https://arxiv.org/abs/2505.21581 ,  6939kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21594
Date: Tue, 27 May 2025 14:55:16 GMT   (3777kb)

Title: Fast and Cost-effective Speculative Edge-Cloud Decoding with Early Exits
Authors: Yeshwanth Venkatesha, Souvik Kundu, Priyadarshini Panda
Categories: cs.RO cs.AI cs.DC
\\
  Large Language Models (LLMs) enable various applications on edge devices such
as smartphones, wearables, and embodied robots. However, their deployment often
depends on expensive cloud-based APIs, creating high operational costs, which
limit access for smaller organizations and raise sustainability concerns.
Certain LLMs can be deployed on-device, offering a cost-effective solution with
reduced latency and improved privacy. Yet, limited computing resources
constrain the size and accuracy of models that can be deployed, necessitating a
collaborative design between edge and cloud. We propose a fast and
cost-effective speculative edge-cloud decoding framework with a large target
model on the server and a small draft model on the device. By introducing early
exits in the target model, tokens are generated mid-verification, allowing the
client to preemptively draft subsequent tokens before final verification, thus
utilizing idle time and enhancing parallelism between edge and cloud. Using an
NVIDIA Jetson Nano (client) and an A100 GPU (server) with Vicuna-68M (draft)
and Llama2-7B (target) models, our method achieves up to a 35% reduction in
latency compared to cloud-based autoregressive decoding, with an additional 11%
improvement from preemptive drafting. To demonstrate real-world applicability,
we deploy our method on the Unitree Go2 quadruped robot using Vision-Language
Model (VLM) based control, achieving a 21% speedup over traditional cloud-based
autoregressive decoding. These results demonstrate the potential of our
framework for real-time LLM and VLM applications on resource-constrained edge
devices.
\\ ( https://arxiv.org/abs/2505.21594 ,  3777kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21652
Date: Tue, 27 May 2025 18:25:42 GMT   (5396kb)

Title: PartInstruct: Part-level Instruction Following for Fine-grained Robot
  Manipulation
Authors: Yifan Yin, Zhengtao Han, Shivam Aarya, Jianxin Wang, Shuhang Xu,
  Jiawei Peng, Angtian Wang, Alan Yuille, Tianmin Shu
Categories: cs.RO cs.AI
\\
  Fine-grained robot manipulation, such as lifting and rotating a bottle to
display the label on the cap, requires robust reasoning about object parts and
their relationships with intended tasks. Despite recent advances in training
general-purpose robot manipulation policies guided by language instructions,
there is a notable lack of large-scale datasets for fine-grained manipulation
tasks with part-level instructions and diverse 3D object instances annotated
with part-level labels. In this work, we introduce PartInstruct, the first
large-scale benchmark for training and evaluating fine-grained robot
manipulation models using part-level instructions. PartInstruct comprises 513
object instances across 14 categories, each annotated with part-level
information, and 1302 fine-grained manipulation tasks organized into 16 task
classes. Our training set consists of over 10,000 expert demonstrations
synthesized in a 3D simulator, where each demonstration is paired with a
high-level task instruction, a chain of base part-based skill instructions, and
ground-truth 3D information about the object and its parts. Additionally, we
designed a comprehensive test suite to evaluate the generalizability of learned
policies across new states, objects, and tasks. We evaluated several
state-of-the-art robot manipulation approaches, including end-to-end
vision-language policy learning and bi-level planning models for robot
manipulation on our benchmark. The experimental results reveal that current
models struggle to robustly ground part concepts and predict actions in 3D
space, and face challenges when manipulating object parts in long-horizon
tasks.
\\ ( https://arxiv.org/abs/2505.21652 ,  5396kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21665
Date: Tue, 27 May 2025 18:45:37 GMT   (33868kb,D)

Title: Convergent Functions, Divergent Forms
Authors: Hyeonseong Jeon, Ainaz Eftekhar, Aaron Walsman, Kuo-Hao Zeng, Ali
  Farhadi, Ranjay Krishna
Categories: cs.RO
\\
  We introduce LOKI, a compute-efficient framework for co-designing
morphologies and control policies that generalize across unseen tasks. Inspired
by biological adaptation -- where animals quickly adjust to morphological
changes -- our method overcomes the inefficiencies of traditional evolutionary
and quality-diversity algorithms. We propose learning convergent functions:
shared control policies trained across clusters of morphologically similar
designs in a learned latent space, drastically reducing the training cost per
design. Simultaneously, we promote divergent forms by replacing mutation with
dynamic local search, enabling broader exploration and preventing premature
convergence. The policy reuse allows us to explore 780$\times$ more designs
using 78% fewer simulation steps and 40% less compute per design. Local
competition paired with a broader search results in a diverse set of
high-performing final morphologies. Using the UNIMAL design space and a
flat-terrain locomotion task, LOKI discovers a rich variety of designs --
ranging from quadrupeds to crabs, bipedals, and spinners -- far more diverse
than those produced by prior work. These morphologies also transfer better to
unseen downstream tasks in agility, stability, and manipulation domains (e.g.,
2$\times$ higher reward on bump and push box incline tasks). Overall, our
approach produces designs that are both diverse and adaptable, with
substantially greater sample efficiency than existing co-design methods.
(Project website: https://loki-codesign.github.io/)
\\ ( https://arxiv.org/abs/2505.21665 ,  33868kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21676
Date: Tue, 27 May 2025 18:51:45 GMT   (4754kb)

Title: Real-World Deployment of Cloud Autonomous Mobility System Using 5G
  Networks for Outdoor and Indoor Environments
Authors: Yufeng Yang, Minghao Ning, Keqi Shu, Aladdin Saleh, Ehsan Hashemi, and
  Amir Khajepour
Categories: cs.RO cs.NI
Comments: This paper has been submitted to IEEE Intelligent Transportation
  Systems Magazine
\\
  The growing complexity of both outdoor and indoor mobility systems demands
scalable, cost-effective, and reliable perception and communication frameworks.
This work presents the real-world deployment and evaluation of a Cloud
Autonomous Mobility (CAM) system that leverages distributed sensor nodes
connected via 5G networks, which integrates LiDAR- and camera-based perception
at infrastructure units, cloud computing for global information fusion, and
Ultra-Reliable Low Latency Communications (URLLC) to enable real-time
situational awareness and autonomous operation. The CAM system is deployed in
two distinct environments: a dense urban roundabout and a narrow indoor
hospital corridor. Field experiments show improved traffic monitoring, hazard
detection, and asset management capabilities. The paper also discusses
practical deployment challenges and shares key insights for scaling CAM
systems. The results highlight the potential of cloud-based infrastructure
perception to advance both outdoor and indoor intelligent transportation
systems.
\\ ( https://arxiv.org/abs/2505.21676 ,  4754kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21734
Date: Tue, 27 May 2025 20:23:21 GMT   (19325kb,D)

Title: MIND-Stack: Modular, Interpretable, End-to-End Differentiability for
  Autonomous Navigation
Authors: Felix Jahncke, Johannes Betz
Categories: cs.RO cs.LG
Comments: 8 pages. Submitted to the IEEE Intelligent Vehicles Symposium (IV
  2025), Romania
\\
  Developing robust, efficient navigation algorithms is challenging. Rule-based
methods offer interpretability and modularity but struggle with learning from
large datasets, while end-to-end neural networks excel in learning but lack
transparency and modularity. In this paper, we present MIND-Stack, a modular
software stack consisting of a localization network and a Stanley Controller
with intermediate human interpretable state representations and end-to-end
differentiability. Our approach enables the upstream localization module to
reduce the downstream control error, extending its role beyond state
estimation. Unlike existing research on differentiable algorithms that either
lack modules of the autonomous stack to span from sensor input to actuator
output or real-world implementation, MIND-Stack offers both capabilities. We
conduct experiments that demonstrate the ability of the localization module to
reduce the downstream control loss through its end-to-end differentiability
while offering better performance than state-of-the-art algorithms. We showcase
sim-to-real capabilities by deploying the algorithm on a real-world embedded
autonomous platform with limited computation power and demonstrate simultaneous
training of both the localization and controller towards one goal. While
MIND-Stack shows good results, we discuss the incorporation of additional
modules from the autonomous navigation pipeline in the future, promising even
greater stability and performance in the next iterations of the framework.
\\ ( https://arxiv.org/abs/2505.21734 ,  19325kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21851
Date: Wed, 28 May 2025 00:48:19 GMT   (14154kb)

Title: Streaming Flow Policy: Simplifying diffusion$/$flow-matching policies by
  treating action trajectories as flow trajectories
Authors: Sunshine Jiang, Xiaolin Fang, Nicholas Roy, Tom\'as Lozano-P\'erez,
  Leslie Pack Kaelbling, Siddharth Ancha
Categories: cs.RO cs.AI cs.LG
Comments: ICRA 2025 Beyond Pick and Place Workshop
\\
  Recent advances in diffusion$/$flow-matching policies have enabled imitation
learning of complex, multi-modal action trajectories. However, they are
computationally expensive because they sample a trajectory of trajectories: a
diffusion$/$flow trajectory of action trajectories. They discard intermediate
action trajectories, and must wait for the sampling process to complete before
any actions can be executed on the robot. We simplify diffusion$/$flow policies
by treating action trajectories as flow trajectories. Instead of starting from
pure noise, our algorithm samples from a narrow Gaussian around the last
action. Then, it incrementally integrates a velocity field learned via flow
matching to produce a sequence of actions that constitute a single trajectory.
This enables actions to be streamed to the robot on-the-fly during the flow
sampling process, and is well-suited for receding horizon policy execution.
Despite streaming, our method retains the ability to model multi-modal
behavior. We train flows that stabilize around demonstration trajectories to
reduce distribution shift and improve imitation learning performance. Streaming
flow policy outperforms prior methods while enabling faster policy execution
and tighter sensorimotor loops for learning-based robot control. Project
website: https://streaming-flow-policy.github.io/
\\ ( https://arxiv.org/abs/2505.21851 ,  14154kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21864
Date: Wed, 28 May 2025 01:25:27 GMT   (20068kb,D)

Title: DexUMI: Using Human Hand as the Universal Manipulation Interface for
  Dexterous Manipulation
Authors: Mengda Xu, Han Zhang, Yifan Hou, Zhenjia Xu, Linxi Fan, Manuela
  Veloso, Shuran Song
Categories: cs.RO
\\
  We present DexUMI - a data collection and policy learning framework that uses
the human hand as the natural interface to transfer dexterous manipulation
skills to various robot hands. DexUMI includes hardware and software
adaptations to minimize the embodiment gap between the human hand and various
robot hands. The hardware adaptation bridges the kinematics gap using a
wearable hand exoskeleton. It allows direct haptic feedback in manipulation
data collection and adapts human motion to feasible robot hand motion. The
software adaptation bridges the visual gap by replacing the human hand in video
data with high-fidelity robot hand inpainting. We demonstrate DexUMI's
capabilities through comprehensive real-world experiments on two different
dexterous robot hand hardware platforms, achieving an average task success rate
of 86%.
\\ ( https://arxiv.org/abs/2505.21864 ,  20068kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21906
Date: Wed, 28 May 2025 02:48:42 GMT   (9628kb,D)

Title: Vision-Language-Action Model with Open-World Embodied Reasoning from
  Pretrained Knowledge
Authors: Zhongyi Zhou, Yichen Zhu, Junjie Wen, Chaomin Shen, Yi Xu
Categories: cs.RO cs.AI cs.CV
Comments: Project page: https://chatvla-2.github.io/
\\
  Vision-language-action (VLA) models have emerged as the next generation of
models in robotics. However, despite leveraging powerful pre-trained
Vision-Language Models (VLMs), existing end-to-end VLA systems often lose key
capabilities during fine-tuning as the model adapts to specific robotic tasks.
We argue that a generalizable VLA model should retain and expand upon the VLM's
core competencies: 1) Open-world embodied reasoning - the VLA should inherit
the knowledge from VLM, i.e., recognize anything that the VLM can recognize,
capable of solving math problems, possessing visual-spatial intelligence, 2)
Reasoning following - effectively translating the open-world reasoning into
actionable steps for the robot. In this work, we introduce ChatVLA-2, a novel
mixture-of-expert VLA model coupled with a specialized three-stage training
pipeline designed to preserve the VLM's original strengths while enabling
actionable reasoning. To validate our approach, we design a math-matching task
wherein a robot interprets math problems written on a whiteboard and picks
corresponding number cards from a table to solve equations. Remarkably, our
method exhibits exceptional mathematical reasoning and OCR capabilities,
despite these abilities not being explicitly trained within the VLA.
Furthermore, we demonstrate that the VLA possesses strong spatial reasoning
skills, enabling it to interpret novel directional instructions involving
previously unseen objects. Overall, our method showcases reasoning and
comprehension abilities that significantly surpass state-of-the-art imitation
learning methods such as OpenVLA, DexVLA, and pi-zero. This work represents a
substantial advancement toward developing truly generalizable robotic
foundation models endowed with robust reasoning capacities.
\\ ( https://arxiv.org/abs/2505.21906 ,  9628kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21916
Date: Wed, 28 May 2025 03:03:38 GMT   (10023kb)

Title: Mastering Agile Tasks with Limited Trials
Authors: Yihang Hu, Pingyue Sheng, Shengjie Wang, Yang Gao
Categories: cs.RO
\\
  Embodied robots nowadays can already handle many real-world manipulation
tasks. However, certain other real-world tasks (e.g., shooting a basketball
into a hoop) are highly agile and require high execution precision, presenting
additional challenges for methods primarily designed for quasi-static
manipulation tasks. This leads to increased efforts in costly data collection,
laborious reward design, or complex motion planning. Such tasks, however, are
far less challenging for humans. Say a novice basketball player typically needs
only $\sim$10 attempts to make their first successful shot, by roughly
imitating a motion prior and then iteratively adjusting their motion based on
the past outcomes. Inspired by this human learning paradigm, we propose the
Adaptive Diffusion Action Plannin (ADAP) algorithm, a simple & scalable
approach which iteratively refines its action plan by few real-world trials
within a learned prior motion pattern, until reaching a specific goal.
Experiments demonstrated that ADAP can learn and accomplish a wide range of
goal-conditioned agile dynamic tasks with human-level precision and efficiency
directly in real-world, such as throwing a basketball into the hoop in fewer
than 10 trials. Project website:https://adap-robotics.github.io/ .
\\ ( https://arxiv.org/abs/2505.21916 ,  10023kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21968
Date: Wed, 28 May 2025 04:45:25 GMT   (1660kb)

Title: Enhanced SIRRT*: A Structure-Aware RRT* for 2D Path Planning with Hybrid
  Smoothing and Bidirectional Rewiring
Authors: Hyejeong Ryu
Categories: cs.RO
\\
  Sampling-based motion planners such as Rapidly-exploring Random Tree* (RRT*)
and its informed variant IRRT* are widely used for optimal path planning in
complex environments. However, these methods often suffer from slow convergence
and high variance due to their reliance on random sampling, particularly when
initial solution discovery is delayed. This paper presents Enhanced SIRRT*
(E-SIRRT*), a structure-aware planner that improves upon the original SIRRT*
framework by introducing two key enhancements: hybrid path smoothing and
bidirectional rewiring. Hybrid path smoothing refines the initial path through
spline fitting and collision-aware correction, while bidirectional rewiring
locally optimizes tree connectivity around the smoothed path to improve cost
propagation. Experimental results demonstrate that E-SIRRT* consistently
outperforms IRRT* and SIRRT* in terms of initial path quality, convergence
rate, and robustness across 100 trials. Unlike IRRT*, which exhibits high
variability due to stochastic initialization, E-SIRRT* achieves repeatable and
efficient performance through deterministic skeleton-based initialization and
structural refinement.
\\ ( https://arxiv.org/abs/2505.21968 ,  1660kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21969
Date: Wed, 28 May 2025 04:46:13 GMT   (46861kb)

Title: DORAEMON: Decentralized Ontology-aware Reliable Agent with Enhanced
  Memory Oriented Navigation
Authors: Tianjun Gu, Linfeng Li, Xuhong Wang, Chenghua Gong, Jingyu Gong,
  Zhizhong Zhang, Yuan Xie, Lizhuang Ma, Xin Tan
Categories: cs.RO cs.AI
\\
  Adaptive navigation in unfamiliar environments is crucial for household
service robots but remains challenging due to the need for both low-level path
planning and high-level scene understanding. While recent vision-language model
(VLM) based zero-shot approaches reduce dependence on prior maps and
scene-specific training data, they face significant limitations: spatiotemporal
discontinuity from discrete observations, unstructured memory representations,
and insufficient task understanding leading to navigation failures. We propose
DORAEMON (Decentralized Ontology-aware Reliable Agent with Enhanced Memory
Oriented Navigation), a novel cognitive-inspired framework consisting of
Ventral and Dorsal Streams that mimics human navigation capabilities. The
Dorsal Stream implements the Hierarchical Semantic-Spatial Fusion and Topology
Map to handle spatiotemporal discontinuities, while the Ventral Stream combines
RAG-VLM and Policy-VLM to improve decision-making. Our approach also develops
Nav-Ensurance to ensure navigation safety and efficiency. We evaluate DORAEMON
on the HM3D, MP3D, and GOAT datasets, where it achieves state-of-the-art
performance on both success rate (SR) and success weighted by path length (SPL)
metrics, significantly outperforming existing methods. We also introduce a new
evaluation metric (AORI) to assess navigation intelligence better.
Comprehensive experiments demonstrate DORAEMON's effectiveness in zero-shot
autonomous navigation without requiring prior map building or pre-training.
\\ ( https://arxiv.org/abs/2505.21969 ,  46861kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21981
Date: Wed, 28 May 2025 05:19:59 GMT   (41320kb)

Title: Learning Compositional Behaviors from Demonstration and Language
Authors: Weiyu Liu, Neil Nie, Ruohan Zhang, Jiayuan Mao, Jiajun Wu
Categories: cs.RO cs.AI cs.CL cs.CV
Comments: Presented at CoRL 2024 and as an Oral Presentation at the 2024 CoRL
  LEAP Workshop. The first two authors contributed equally. The last two
  authors jointly advised the project. For videos and additional results,
  visit: https://blade-bot.github.io/
\\
  We introduce Behavior from Language and Demonstration (BLADE), a framework
for long-horizon robotic manipulation by integrating imitation learning and
model-based planning. BLADE leverages language-annotated demonstrations,
extracts abstract action knowledge from large language models (LLMs), and
constructs a library of structured, high-level action representations. These
representations include preconditions and effects grounded in visual perception
for each high-level action, along with corresponding controllers implemented as
neural network-based policies. BLADE can recover such structured
representations automatically, without manually labeled states or symbolic
definitions. BLADE shows significant capabilities in generalizing to novel
situations, including novel initial states, external state perturbations, and
novel goals. We validate the effectiveness of our approach both in simulation
and on real robots with a diverse set of objects with articulated parts,
partial observability, and geometric constraints.
\\ ( https://arxiv.org/abs/2505.21981 ,  41320kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21992
Date: Wed, 28 May 2025 05:45:10 GMT   (1723kb)

Title: Soft Electrothermal Meta-Actuator for Robust Multifunctional Control
Authors: Hanseong Jo, Pavel Shafirin, Christopher Le, Caden Chan, Artur Davoyan
Categories: cs.RO
Comments: 23 pages, 5 figures
\\
  Soft electrothermal actuators are of great interest in diverse application
domains for their simplicity, compliance, and ease of control. However, the
very nature of thermally induced mechanical actuation sets inherent operation
constraints: unidirectional motion, environmental sensitivity, and slow
response times limited by passive cooling. To overcome these constraints, we
propose a meta-actuator architecture, which uses engineered heat transfer in
thin films to achieve multifunctional operation. We demonstrate electrically
selectable bidirectional motion with large deflection ($ \geq $28% of actuator
length at 0.75 W), suppressed thermal sensitivity to ambient temperature
changes when compared to conventional actuators (>100$ \times $ lower), and
actively forced return to the rest state, which is 10 times faster than that
with passive cooling. We further show that our meta-actuator approach enables
extended ranges of motions for manipulating complex objects. Versatile soft
gripper operations highlight the meta-actuator's potential for soft robotics
and devices.
\\ ( https://arxiv.org/abs/2505.21992 ,  1723kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22091
Date: Wed, 28 May 2025 08:16:05 GMT   (12078kb)

Title: A simulation framework for autonomous lunar construction work
Authors: Mattias Linde, Daniel Lindmark, Sandra {\AA}lstig, and Martin Servin
Categories: cs.RO
Comments: 12 pages, 16 figures
\\
  We present a simulation framework for lunar construction work involving
multiple autonomous machines. The framework supports modelling of construction
scenarios and autonomy solutions, execution of the scenarios in simulation, and
analysis of work time and energy consumption throughout the construction
project. The simulations are based on physics-based models for contacting
multibody dynamics and deformable terrain, including vehicle-soil interaction
forces and soil flow in real time. A behaviour tree manages the operational
logic and error handling, which enables the representation of complex
behaviours through a discrete set of simpler tasks in a modular hierarchical
structure. High-level decision-making is separated from lower-level control
algorithms, with the two connected via ROS2. Excavation movements are
controlled through inverse kinematics and tracking controllers. The framework
is tested and demonstrated on two different lunar construction scenarios.
\\ ( https://arxiv.org/abs/2505.22091 ,  12078kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22094
Date: Wed, 28 May 2025 08:17:16 GMT   (633kb)

Title: ReinFlow: Fine-tuning Flow Matching Policy with Online Reinforcement
  Learning
Authors: Tonghe Zhang, Yu Chao, Sicang Su, Yu Wang
Categories: cs.RO cs.LG
Comments: 30 pages, 13 figures, 10 tables
\\
  We propose ReinFlow, a simple yet effective online reinforcement learning
(RL) framework that fine-tunes a family of flow matching policies for
continuous robotic control. Derived from rigorous RL theory, ReinFlow injects
learnable noise into a flow policy's deterministic path, converting the flow
into a discrete-time Markov Process for exact and straightforward likelihood
computation. This conversion facilitates exploration and ensures training
stability, enabling ReinFlow to fine-tune diverse flow model variants,
including Rectified Flow [35] and Shortcut Models [19], particularly at very
few or even one denoising step. We benchmark ReinFlow in representative
locomotion and manipulation tasks, including long-horizon planning with visual
input and sparse reward. The episode reward of Rectified Flow policies obtained
an average net growth of 135.36% after fine-tuning in challenging legged
locomotion tasks while saving denoising steps and 82.63% of wall time compared
to state-of-the-art diffusion RL fine-tuning method DPPO [43]. The success rate
of the Shortcut Model policies in state and visual manipulation tasks achieved
an average net increase of 40.34% after fine-tuning with ReinFlow at four or
even one denoising step, whose performance is comparable to fine-tuned DDIM
policies while saving computation time for an average of 23.20%. Project
Webpage: https://reinflow.github.io/
\\ ( https://arxiv.org/abs/2505.22094 ,  633kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22159
Date: Wed, 28 May 2025 09:24:25 GMT   (10085kb)

Title: ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich
  Manipulation
Authors: Jiawen Yu, Hairuo Liu, Qiaojun Yu, Jieji Ren, Ce Hao, Haitong Ding,
  Guangyu Huang, Guofan Huang, Yan Song, Panpan Cai, Cewu Lu, Wenqiang Zhang
Categories: cs.RO cs.CV
\\
  Vision-Language-Action (VLA) models have advanced general-purpose robotic
manipulation by leveraging pretrained visual and linguistic representations.
However, they struggle with contact-rich tasks that require fine-grained
control involving force, especially under visual occlusion or dynamic
uncertainty. To address these limitations, we propose \textbf{ForceVLA}, a
novel end-to-end manipulation framework that treats external force sensing as a
first-class modality within VLA systems. ForceVLA introduces \textbf{FVLMoE}, a
force-aware Mixture-of-Experts fusion module that dynamically integrates
pretrained visual-language embeddings with real-time 6-axis force feedback
during action decoding. This enables context-aware routing across
modality-specific experts, enhancing the robot's ability to adapt to subtle
contact dynamics. We also introduce \textbf{ForceVLA-Data}, a new dataset
comprising synchronized vision, proprioception, and force-torque signals across
five contact-rich manipulation tasks. ForceVLA improves average task success by
23.2\% over strong $\pi_0$-based baselines, achieving up to 80\% success in
tasks such as plug insertion. Our approach highlights the importance of
multimodal integration for dexterous manipulation and sets a new benchmark for
physically intelligent robotic control. Code and data will be released at
https://sites.google.com/view/forcevla2025.
\\ ( https://arxiv.org/abs/2505.22159 ,  10085kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22258
Date: Wed, 28 May 2025 11:45:14 GMT   (2407kb)

Title: LiDAR Based Semantic Perception for Forklifts in Outdoor Environments
Authors: Benjamin Serfling, Hannes Reichert, Lorenzo Bayerlein, Konrad Doll and
  Kati Radkhah-Lens
Categories: cs.RO cs.CV cs.LG
\\
  In this study, we present a novel LiDAR-based semantic segmentation framework
tailored for autonomous forklifts operating in complex outdoor environments.
Central to our approach is the integration of a dual LiDAR system, which
combines forward-facing and downward-angled LiDAR sensors to enable
comprehensive scene understanding, specifically tailored for industrial
material handling tasks. The dual configuration improves the detection and
segmentation of dynamic and static obstacles with high spatial precision. Using
high-resolution 3D point clouds captured from two sensors, our method employs a
lightweight yet robust approach that segments the point clouds into
safety-critical instance classes such as pedestrians, vehicles, and forklifts,
as well as environmental classes such as driveable ground, lanes, and
buildings. Experimental validation demonstrates that our approach achieves high
segmentation accuracy while satisfying strict runtime requirements,
establishing its viability for safety-aware, fully autonomous forklift
navigation in dynamic warehouse and yard environments.
\\ ( https://arxiv.org/abs/2505.22258 ,  2407kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22335
Date: Wed, 28 May 2025 13:23:16 GMT   (14584kb,D)

Title: UP-SLAM: Adaptively Structured Gaussian SLAM with Uncertainty Prediction
  in Dynamic Environments
Authors: Wancai Zheng, Linlin Ou, Jiajie He, Libo Zhou, Xinyi Yu, Yan Wei
Categories: cs.RO cs.CV
\\
  Recent 3D Gaussian Splatting (3DGS) techniques for Visual Simultaneous
Localization and Mapping (SLAM) have significantly progressed in tracking and
high-fidelity mapping. However, their sequential optimization framework and
sensitivity to dynamic objects limit real-time performance and robustness in
real-world scenarios. We present UP-SLAM, a real-time RGB-D SLAM system for
dynamic environments that decouples tracking and mapping through a parallelized
framework. A probabilistic octree is employed to manage Gaussian primitives
adaptively, enabling efficient initialization and pruning without hand-crafted
thresholds. To robustly filter dynamic regions during tracking, we propose a
training-free uncertainty estimator that fuses multi-modal residuals to
estimate per-pixel motion uncertainty, achieving open-set dynamic object
handling without reliance on semantic labels. Furthermore, a temporal encoder
is designed to enhance rendering quality. Concurrently, low-dimensional
features are efficiently transformed via a shallow multilayer perceptron to
construct DINO features, which are then employed to enrich the Gaussian field
and improve the robustness of uncertainty prediction. Extensive experiments on
multiple challenging datasets suggest that UP-SLAM outperforms state-of-the-art
methods in both localization accuracy (by 59.8%) and rendering quality (by 4.57
dB PSNR), while maintaining real-time performance and producing reusable,
artifact-free static maps in dynamic environments.The project:
https://aczheng-cai.github.io/up_slam.github.io/
\\ ( https://arxiv.org/abs/2505.22335 ,  14584kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22497
Date: Wed, 28 May 2025 15:47:43 GMT   (1047kb)

Title: Fully Packed and Ready to Go: High-Density, Rearrangement-Free,
  Grid-Based Storage and Retrieval
Authors: Tzvika Geft, Kostas Bekris, Jingjin Yu
Categories: cs.RO cs.DS
\\
  Grid-based storage systems with uniformly shaped loads (e.g., containers,
pallets, totes) are commonplace in logistics, industrial, and transportation
domains. A key performance metric for such systems is the maximization of space
utilization, which requires some loads to be placed behind or below others,
preventing direct access to them. Consequently, dense storage settings bring up
the challenge of determining how to place loads while minimizing costly
rearrangement efforts necessary during retrieval. This paper considers the
setting involving an inbound phase, during which loads arrive, followed by an
outbound phase, during which loads depart. The setting is prevalent in
distribution centers, automated parking garages, and container ports. In both
phases, minimizing the number of rearrangement actions results in more optimal
(e.g., fast, energy-efficient, etc.) operations. In contrast to previous work
focusing on stack-based systems, this effort examines the case where loads can
be freely moved along the grid, e.g., by a mobile robot, expanding the range of
possible motions. We establish that for a range of scenarios, such as having
limited prior knowledge of the loads' arrival sequences or grids with a narrow
opening, a (best possible) rearrangement-free solution always exists, including
when the loads fill the grid to its capacity. In particular, when the sequences
are fully known, we establish an intriguing characterization showing that
rearrangement can always be avoided if and only if the open side of the grid
(used to access the storage) is at least 3 cells wide. We further discuss
useful practical implications of our solutions.
\\ ( https://arxiv.org/abs/2505.22497 ,  1047kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22503
Date: Wed, 28 May 2025 15:51:13 GMT   (4464kb)

Title: From Strangers to Assistants: Fast Desire Alignment for Embodied
  Agent-User Adaptation
Authors: Yuanfei Wang, Xinju Huang, Fangwei Zhong, Yaodong Yang, Yizhou Wang,
  Yuanpei Chen, Hao Dong
Categories: cs.RO cs.AI cs.MA
\\
  While embodied agents have made significant progress in performing complex
physical tasks, real-world applications demand more than pure task execution.
The agents must collaborate with unfamiliar agents and human users, whose goals
are often vague and implicit. In such settings, interpreting ambiguous
instructions and uncovering underlying desires is essential for effective
assistance. Therefore, fast and accurate desire alignment becomes a critical
capability for embodied agents. In this work, we first develop a home
assistance simulation environment HA-Desire that integrates an LLM-driven human
user agent exhibiting realistic value-driven goal selection and communication.
The ego agent must interact with this proxy user to infer and adapt to the
user's latent desires. To achieve this, we present a novel framework FAMER for
fast desire alignment, which introduces a desire-based mental reasoning
mechanism to identify user intent and filter desire-irrelevant actions. We
further design a reflection-based communication module that reduces redundant
inquiries, and incorporate goal-relevant information extraction with memory
persistence to improve information reuse and reduce unnecessary exploration.
Extensive experiments demonstrate that our framework significantly enhances
both task execution and communication efficiency, enabling embodied agents to
quickly adapt to user-specific desires in complex embodied environments.
\\ ( https://arxiv.org/abs/2505.22503 ,  4464kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22599
Date: Wed, 28 May 2025 17:14:30 GMT   (4272kb)

Title: VR-Based Control of Multi-Copter Operation
Authors: Jack T. Hughes, Mohammad Ghufran, and Hossein Rastgoftar
Categories: cs.RO
\\
  We aim to use virtual reality (VR) to improve the spatial awareness of pilots
by real-time scanning of the environment around the drone using onboard
sensors, live streaming of this environment to a VR headset, and rendering a
virtual representation of the drone and its environment for the pilot. This
way, the pilot can see the immediate environment of the drone up close from a
third-person perspective, as opposed to the first-person perspective that most
drone cameras provide. This provides much more information about the drone
surroundings for the pilot while operating the drone than existing
teleoperation solutions. Previous solutions using VR have relied upon pre-made
designs of the environment, which makes it difficult to adapt to changing
environments. Our solution, in contrast, scans the environment as you fly,
making it much more flexible for use in unknown environments.
\\ ( https://arxiv.org/abs/2505.22599 ,  4272kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22626
Date: Wed, 28 May 2025 17:45:05 GMT   (2197kb)

Title: SCIZOR: A Self-Supervised Approach to Data Curation for Large-Scale
  Imitation Learning
Authors: Yu Zhang, Yuqi Xie, Huihan Liu, Rutav Shah, Michael Wan, Linxi Fan,
  Yuke Zhu
Categories: cs.RO cs.AI cs.LG
\\
  Imitation learning advances robot capabilities by enabling the acquisition of
diverse behaviors from human demonstrations. However, large-scale datasets used
for policy training often introduce substantial variability in quality, which
can negatively impact performance. As a result, automatically curating datasets
by filtering low-quality samples to improve quality becomes essential. Existing
robotic curation approaches rely on costly manual annotations and perform
curation at a coarse granularity, such as the dataset or trajectory level,
failing to account for the quality of individual state-action pairs. To address
this, we introduce SCIZOR, a self-supervised data curation framework that
filters out low-quality state-action pairs to improve the performance of
imitation learning policies. SCIZOR targets two complementary sources of
low-quality data: suboptimal data, which hinders learning with undesirable
actions, and redundant data, which dilutes training with repetitive patterns.
SCIZOR leverages a self-supervised task progress predictor for suboptimal data
to remove samples lacking task progression, and a deduplication module
operating on joint state-action representation for samples with redundant
patterns. Empirically, we show that SCIZOR enables imitation learning policies
to achieve higher performance with less data, yielding an average improvement
of 15.4% across multiple benchmarks. More information is available at:
https://ut-austin-rpl.github.io/SCIZOR/
\\ ( https://arxiv.org/abs/2505.22626 ,  2197kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22634
Date: Wed, 28 May 2025 17:50:53 GMT   (11791kb)

Title: LabUtopia: High-Fidelity Simulation and Hierarchical Benchmark for
  Scientific Embodied Agents
Authors: Rui Li, Zixuan Hu, Wenxi Qu, Jinouwen Zhang, Zhenfei Yin, Sha Zhang,
  Xuantuo Huang, Hanqing Wang, Tai Wang, Jiangmiao Pang, Wanli Ouyang, Lei Bai,
  Wangmeng Zuo, Ling-Yu Duan, Dongzhan Zhou, Shixiang Tang
Categories: cs.RO cs.SE
\\
  Scientific embodied agents play a crucial role in modern laboratories by
automating complex experimental workflows. Compared to typical household
environments, laboratory settings impose significantly higher demands on
perception of physical-chemical transformations and long-horizon planning,
making them an ideal testbed for advancing embodied intelligence. However, its
development has been long hampered by the lack of suitable simulator and
benchmarks. In this paper, we address this gap by introducing LabUtopia, a
comprehensive simulation and benchmarking suite designed to facilitate the
development of generalizable, reasoning-capable embodied agents in laboratory
settings. Specifically, it integrates i) LabSim, a high-fidelity simulator
supporting multi-physics and chemically meaningful interactions; ii) LabScene,
a scalable procedural generator for diverse scientific scenes; and iii)
LabBench, a hierarchical benchmark spanning five levels of complexity from
atomic actions to long-horizon mobile manipulation. LabUtopia supports 30
distinct tasks and includes more than 200 scene and instrument assets, enabling
large-scale training and principled evaluation in high-complexity environments.
We demonstrate that LabUtopia offers a powerful platform for advancing the
integration of perception, planning, and control in scientific-purpose agents
and provides a rigorous testbed for exploring the practical capabilities and
generalization limits of embodied intelligence in future research.
\\ ( https://arxiv.org/abs/2505.22634 ,  11791kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22642
Date: Wed, 28 May 2025 17:55:26 GMT   (3531kb,D)

Title: FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid
  Control
Authors: Younggyo Seo, Carmelo Sferrazza, Haoran Geng, Michal Nauman, Zhao-Heng
  Yin, Pieter Abbeel
Categories: cs.RO cs.AI cs.LG
Comments: Project webpage: https://younggyo.me/fast_td3
\\
  Reinforcement learning (RL) has driven significant progress in robotics, but
its complexity and long training times remain major bottlenecks. In this
report, we introduce FastTD3, a simple, fast, and capable RL algorithm that
significantly speeds up training for humanoid robots in popular suites such as
HumanoidBench, IsaacLab, and MuJoCo Playground. Our recipe is remarkably
simple: we train an off-policy TD3 agent with several modifications -- parallel
simulation, large-batch updates, a distributional critic, and carefully tuned
hyperparameters. FastTD3 solves a range of HumanoidBench tasks in under 3 hours
on a single A100 GPU, while remaining stable during training. We also provide a
lightweight and easy-to-use implementation of FastTD3 to accelerate RL research
in robotics.
\\ ( https://arxiv.org/abs/2505.22642 ,  3531kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2404.11045 (*cross-listing*)
Date: Wed, 17 Apr 2024 03:39:51 GMT   (640kb,D)
Date (revised v2): Fri, 23 May 2025 00:22:01 GMT   (2253kb,D)

Title: Offset Unlearning for Large Language Models
Authors: James Y. Huang, Wenxuan Zhou, Fei Wang, Fred Morstatter, Sheng Zhang,
  Hoifung Poon, Muhao Chen
Categories: cs.CL cs.AI cs.LG
Comments: Published in TMLR. https://openreview.net/pdf?id=A4RLpHPXCu
\\
  Despite the strong capabilities of Large Language Models (LLMs) to acquire
knowledge from their training corpora, the memorization of sensitive
information in the corpora such as copyrighted, biased, and private content has
led to ethical and legal concerns. In response to these challenges, unlearning
has emerged as a potential remedy for LLMs affected by problematic training
data. However, previous unlearning techniques are either not applicable to
black-box LLMs due to required access to model internal weights, or violate
data protection principles by retaining sensitive data for inference-time
correction. We propose {\delta}-Unlearning, an offset unlearning framework for
black-box LLMs. Instead of tuning the black-box LLM itself, {\delta}-Unlearning
learns the logit offset needed for unlearning by contrasting the logits from a
pair of smaller models. Experiments demonstrate that {\delta}- Unlearning can
effectively unlearn target data while maintaining similar or even stronger
performance on general out-of-forget-scope tasks. {\delta}-Unlearning also
effectively incorporates different unlearning algorithms, making our approach a
versatile solution to adapting various existing unlearning algorithms to
black-box LLMs.
\\ ( https://arxiv.org/abs/2404.11045 ,  2253kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21506 (*cross-listing*)
Date: Sun, 9 Mar 2025 16:42:59 GMT   (2379kb,D)

Title: Conformance Checking for Less: Efficient Conformance Checking for Long
  Event Sequences
Authors: Eli Bogdanov, Izack Cohen, Avigdor Gal
Categories: cs.DB cs.AI cs.PL
Comments: 17 pages, 4 figures
\\
  Long event sequences (termed traces) and large data logs that originate from
sensors and prediction models are becoming increasingly common in our data-rich
world. In such scenarios, conformance checking-validating a data log against an
expected system behavior (the process model) can become computationally
infeasible due to the exponential complexity of finding an optimal alignment.
To alleviate scalability challenges for this task, we propose ConLES, a
sliding-window conformance checking approach for long event sequences that
preserves the interpretability of alignment-based methods. ConLES partitions
traces into manageable subtraces and iteratively aligns each against the
expected behavior, leading to significant reduction of the search space while
maintaining overall accuracy. We use global information that captures
structural properties of both the trace and the process model, enabling
informed alignment decisions and discarding unpromising alignments, even if
they appear locally optimal. Performance evaluations across multiple datasets
highlight that ConLES outperforms the leading optimal and heuristic algorithms
for long traces, consistently achieving the optimal or near-optimal solution.
Unlike other conformance methods that struggle with long event sequences,
ConLES significantly reduces the search space, scales efficiently, and uniquely
supports both predefined and discovered process models, making it a viable and
leading option for conformance checking of long event sequences.
\\ ( https://arxiv.org/abs/2505.21506 ,  2379kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21513 (*cross-listing*)
Date: Tue, 20 May 2025 23:16:10 GMT   (5126kb)

Title: Enhancing Vision Transformer Explainability Using Artificial Astrocytes
Authors: Nicolas Echevarrieta-Catalan, Ana Ribas-Rodriguez, Francisco Cedron,
  Odelia Schwartz, Vanessa Aguiar-Pulido
Categories: cs.CV cs.AI cs.LG
Comments: LXCV Workshop at IEEE / CVF Computer Vision and Pattern Recognition
  Conference (CVPR) 2025
\\
  Machine learning models achieve high precision, but their decision-making
processes often lack explainability. Furthermore, as model complexity
increases, explainability typically decreases. Existing efforts to improve
explainability primarily involve developing new eXplainable artificial
intelligence (XAI) techniques or incorporating explainability constraints
during training. While these approaches yield specific improvements, their
applicability remains limited. In this work, we propose the Vision Transformer
with artificial Astrocytes (ViTA). This training-free approach is inspired by
neuroscience and enhances the reasoning of a pretrained deep neural network to
generate more human-aligned explanations. We evaluated our approach employing
two well-known XAI techniques, Grad-CAM and Grad-CAM++, and compared it to a
standard Vision Transformer (ViT). Using the ClickMe dataset, we quantified the
similarity between the heatmaps produced by the XAI techniques and a
(human-aligned) ground truth. Our results consistently demonstrate that
incorporating artificial astrocytes enhances the alignment of model
explanations with human perception, leading to statistically significant
improvements across all XAI techniques and metrics utilized.
\\ ( https://arxiv.org/abs/2505.21513 ,  5126kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21520 (*cross-listing*)
Date: Thu, 22 May 2025 13:49:05 GMT   (494kb)

Title: Do DeepFake Attribution Models Generalize?
Authors: Spiros Baxavanakis, Manos Schinas, Symeon Papadopoulos
Categories: cs.CV cs.AI
\\
  Recent advancements in DeepFake generation, along with the proliferation of
open-source tools, have significantly lowered the barrier for creating
synthetic media. This trend poses a serious threat to the integrity and
authenticity of online information, undermining public trust in institutions
and media. State-of-the-art research on DeepFake detection has primarily
focused on binary detection models. A key limitation of these models is that
they treat all manipulation techniques as equivalent, despite the fact that
different methods introduce distinct artifacts and visual cues. Only a limited
number of studies explore DeepFake attribution models, although such models are
crucial in practical settings. By providing the specific manipulation method
employed, these models could enhance both the perceived trustworthiness and
explainability for end users. In this work, we leverage five state-of-the-art
backbone models and conduct extensive experiments across six DeepFake datasets.
First, we compare binary and multi-class models in terms of cross-dataset
generalization. Second, we examine the accuracy of attribution models in
detecting seen manipulation methods in unknown datasets, hence uncovering data
distribution shifts on the same DeepFake manipulations. Last, we assess the
effectiveness of contrastive methods in improving cross-dataset generalization
performance. Our findings indicate that while binary models demonstrate better
generalization abilities, larger models, contrastive methods, and higher data
quality can lead to performance improvements in attribution models. The code of
this work is available on GitHub.
\\ ( https://arxiv.org/abs/2505.21520 ,  494kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21522 (*cross-listing*)
Date: Fri, 23 May 2025 02:26:56 GMT   (1552kb)

Title: CIM-NET: A Video Denoising Deep Neural Network Model Optimized for
  Computing-in-Memory Architectures
Authors: Shan Gao, Zhiqiang Wu, Yawen Niu, Xiaotao Li, Qingqing Xu
Categories: cs.CV cs.AI cs.LG eess.IV
\\
  While deep neural network (DNN)-based video denoising has demonstrated
significant performance, deploying state-of-the-art models on edge devices
remains challenging due to stringent real-time and energy efficiency
requirements. Computing-in-Memory (CIM) chips offer a promising solution by
integrating computation within memory cells, enabling rapid matrix-vector
multiplication (MVM). However, existing DNN models are often designed without
considering CIM architectural constraints, thus limiting their acceleration
potential during inference. To address this, we propose a hardware-algorithm
co-design framework incorporating two innovations: (1) a CIM-Aware
Architecture, CIM-NET, optimized for large receptive field operation and CIM's
crossbar-based MVM acceleration; and (2) a pseudo-convolutional operator,
CIM-CONV, used within CIM-NET to integrate slide-based processing with fully
connected transformations for high-quality feature extraction and
reconstruction. This framework significantly reduces the number of MVM
operations, improving inference speed on CIM chips while maintaining
competitive performance. Experimental results indicate that, compared to the
conventional lightweight model FastDVDnet, CIM-NET substantially reduces MVM
operations with a slight decrease in denoising performance. With a stride value
of 8, CIM-NET reduces MVM operations to 1/77th of the original, while
maintaining competitive PSNR (35.11 dB vs. 35.56 dB
\\ ( https://arxiv.org/abs/2505.21522 ,  1552kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21523 (*cross-listing*)
Date: Fri, 23 May 2025 05:08:40 GMT   (6670kb)

Title: More Thinking, Less Seeing? Assessing Amplified Hallucination in
  Multimodal Reasoning Models
Authors: Chengzhi Liu, Zhongxing Xu, Qingyue Wei, Juncheng Wu, James Zou, Xin
  Eric Wang, Yuyin Zhou, Sheng Liu
Categories: cs.CL cs.AI cs.CV
\\
  Test-time compute has empowered multimodal large language models to generate
extended reasoning chains, yielding strong performance on tasks such as
multimodal math reasoning. However, this improved reasoning ability often comes
with increased hallucination: as generations become longer, models tend to
drift away from image-grounded content and rely more heavily on language
priors. Attention analysis shows that longer reasoning chains lead to reduced
focus on visual inputs, which contributes to hallucination. To systematically
study this phenomenon, we introduce RH-AUC, a metric that quantifies how a
model's perception accuracy changes with reasoning length, allowing us to
evaluate whether the model preserves visual grounding during reasoning. We also
release RH-Bench, a diagnostic benchmark that spans a variety of multimodal
tasks, designed to assess the trade-off between reasoning ability and
hallucination. Our analysis reveals that (i) larger models typically achieve a
better balance between reasoning and perception, and (ii) this balance is
influenced more by the types and domains of training data than by its overall
volume. These findings underscore the importance of evaluation frameworks that
jointly consider both reasoning quality and perceptual fidelity.
\\ ( https://arxiv.org/abs/2505.21523 ,  6670kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21527 (*cross-listing*)
Date: Fri, 23 May 2025 14:26:11 GMT   (1058kb)

Title: VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled
  data and Large-Scale Speech Pretraining
Authors: Jianheng Zhuo, Yifan Yang, Yiwen Shao, Yong Xu, Dong Yu, Kai Yu, Xie
  Chen
Categories: eess.AS cs.AI cs.CL cs.SD
\\
  Automatic speech recognition (ASR) has made remarkable progress but heavily
relies on large-scale labeled data, which is scarce for low-resource languages
like Vietnamese. While existing systems such as Whisper, USM, and MMS achieve
promising performance, their efficacy remains inadequate in terms of training
costs, latency, and accessibility. To address these issues, we propose VietASR,
a novel ASR training pipeline that leverages vast amounts of unlabeled data and
a small set of labeled data. Through multi-iteration ASR-biased self-supervised
learning on a large-scale unlabeled dataset, VietASR offers a cost-effective
and practical solution for enhancing ASR performance. Experiments demonstrate
that pre-training on 70,000-hour unlabeled data and fine-tuning on merely
50-hour labeled data yield a lightweight but powerful ASR model. It outperforms
Whisper Large-v3 and commercial ASR systems on real-world data. Our code and
models will be open-sourced to facilitate research in low-resource ASR.
\\ ( https://arxiv.org/abs/2505.21527 ,  1058kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21528 (*cross-listing*)
Date: Fri, 23 May 2025 15:03:02 GMT   (27645kb,D)

Title: UniDB++: Fast Sampling of Unified Diffusion Bridge
Authors: Mokai Pan, Kaizhen Zhu, Yuexin Ma, Yanwei Fu, Jingyi Yu, Jingya Wang,
  Ye Shi
Categories: cs.CV cs.AI cs.LG
\\
  Diffusion Bridges enable transitions between arbitrary distributions, with
the Unified Diffusion Bridge (UniDB) framework achieving high-fidelity image
generation via a Stochastic Optimal Control (SOC) formulation. However, UniDB's
reliance on iterative Euler sampling methods results in slow, computationally
expensive inference, while existing acceleration techniques for diffusion or
diffusion bridge models fail to address its unique challenges: missing terminal
mean constraints and SOC-specific penalty coefficients in its SDEs. We present
UniDB++, a training-free sampling algorithm that significantly improves upon
these limitations. The method's key advancement comes from deriving exact
closed-form solutions for UniDB's reverse-time SDEs, effectively reducing the
error accumulation inherent in Euler approximations and enabling high-quality
generation with up to 20$\times$ fewer sampling steps. This method is further
complemented by replacing conventional noise prediction with a more stable data
prediction model, along with an SDE-Corrector mechanism that maintains
perceptual quality for low-step regimes (5-10 steps). Additionally, we
demonstrate that UniDB++ aligns with existing diffusion bridge acceleration
methods by evaluating their update rules, and UniDB++ can recover DBIMs as
special cases under some theoretical conditions. Experiments demonstrate
UniDB++'s state-of-the-art performance in image restoration tasks,
outperforming Euler-based methods in fidelity and speed while reducing
inference time significantly. This work bridges the gap between theoretical
generality and practical efficiency in SOC-driven diffusion bridge models. Our
code is available at https://github.com/2769433owo/UniDB-plusplus.
\\ ( https://arxiv.org/abs/2505.21528 ,  27645kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21530 (*cross-listing*)
Date: Fri, 23 May 2025 15:27:17 GMT   (3885kb)

Title: High-Fidelity Functional Ultrasound Reconstruction via A Visual
  Auto-Regressive Framework
Authors: Xuhang Chen, Zhuo Li, Yanyan Shen, Mufti Mahmud, Hieu Pham, Chi-Man
  Pun and Shuqiang Wang
Categories: eess.IV cs.AI cs.CV
\\
  Functional ultrasound (fUS) imaging provides exceptional spatiotemporal
resolution for neurovascular mapping, yet its practical application is
significantly hampered by critical challenges. Foremost among these are data
scarcity, arising from ethical considerations and signal degradation through
the cranium, which collectively limit dataset diversity and compromise the
fairness of downstream machine learning models.
\\ ( https://arxiv.org/abs/2505.21530 ,  3885kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21531 (*cross-listing*)
Date: Fri, 23 May 2025 16:01:08 GMT   (15088kb,D)

Title: How Much Do Large Language Models Know about Human Motion? A Case Study
  in 3D Avatar Control
Authors: Kunhang Li, Jason Naradowsky, Yansong Feng, Yusuke Miyao
Categories: cs.CV cs.AI cs.CL cs.RO
\\
  We explore Large Language Models (LLMs)' human motion knowledge through 3D
avatar control. Given a motion instruction, we prompt LLMs to first generate a
high-level movement plan with consecutive steps (High-level Planning), then
specify body part positions in each step (Low-level Planning), which we
linearly interpolate into avatar animations as a clear verification lens for
human evaluators. Through carefully designed 20 representative motion
instructions with full coverage of basic movement primitives and balanced body
part usage, we conduct comprehensive evaluations including human assessment of
both generated animations and high-level movement plans, as well as automatic
comparison with oracle positions in low-level planning. We find that LLMs are
strong at interpreting the high-level body movements but struggle with precise
body part positioning. While breaking down motion queries into atomic
components improves planning performance, LLMs have difficulty with multi-step
movements involving high-degree-of-freedom body parts. Furthermore, LLMs
provide reasonable approximation for general spatial descriptions, but fail to
handle precise spatial specifications in text, and the precise spatial-temporal
parameters needed for avatar control. Notably, LLMs show promise in
conceptualizing creative motions and distinguishing culturally-specific motion
patterns.
\\ ( https://arxiv.org/abs/2505.21531 ,  15088kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21532 (*cross-listing*)
Date: Fri, 23 May 2025 16:38:13 GMT   (2389kb)

Title: EvidenceMoE: A Physics-Guided Mixture-of-Experts with Evidential Critics
  for Advancing Fluorescence Light Detection and Ranging in Scattering Media
Authors: Ismail Erbas, Ferhat Demirkiran, Karthik Swaminathan, Naigang Wang,
  Navid Ibtehaj Nizam, Stefan T. Radev, Kaoutar El Maghraoui, Xavier Intes,
  Vikas Pandey
Categories: cs.CV cs.AI cs.LG physics.optics
Comments: 18 pages, 4 figures
\\
  Fluorescence LiDAR (FLiDAR), a Light Detection and Ranging (LiDAR) technology
employed for distance and depth estimation across medical, automotive, and
other fields, encounters significant computational challenges in scattering
media. The complex nature of the acquired FLiDAR signal, particularly in such
environments, makes isolating photon time-of-flight (related to target depth)
and intrinsic fluorescence lifetime exceptionally difficult, thus limiting the
effectiveness of current analytical and computational methodologies. To
overcome this limitation, we present a Physics-Guided Mixture-of-Experts (MoE)
framework tailored for specialized modeling of diverse temporal components. In
contrast to the conventional MoE approaches our expert models are informed by
underlying physics, such as the radiative transport equation governing photon
propagation in scattering media. Central to our approach is EvidenceMoE, which
integrates Evidence-Based Dirichlet Critics (EDCs). These critic models assess
the reliability of each expert's output by providing per-expert quality scores
and corrective feedback. A Decider Network then leverages this information to
fuse expert predictions into a robust final estimate adaptively. We validate
our method using realistically simulated Fluorescence LiDAR (FLiDAR) data for
non-invasive cancer cell depth detection generated from photon transport models
in tissue. Our framework demonstrates strong performance, achieving a
normalized root mean squared error (NRMSE) of 0.030 for depth estimation and
0.074 for fluorescence lifetime.
\\ ( https://arxiv.org/abs/2505.21532 ,  2389kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21535 (*cross-listing*)
Date: Sat, 24 May 2025 02:23:46 GMT   (5645kb)

Title: Is Attention Required for Transformer Inference? Explore
  Function-preserving Attention Replacement
Authors: Yuxin Ren, Maxwell D Collins, Miao Hu, Huanrui Yang
Categories: cs.CV cs.AI cs.LG
Comments: 12 pages main paper + 6 pages appendix, 14 figures; submitted to
  NeurIPS 2025
\\
  While transformers excel across vision and language pretraining tasks, their
reliance on attention mechanisms poses challenges for inference efficiency,
especially on edge and embedded accelerators with limited parallelism and
memory bandwidth. Hinted by the observed redundancy of attention at inference
time, we hypothesize that though the model learns complicated token dependency
through pretraining, the inference-time sequence-to-sequence mapping in each
attention layer is actually ''simple'' enough to be represented with a much
cheaper function. In this work, we explore FAR, a Function-preserving Attention
Replacement framework that replaces all attention blocks in pretrained
transformers with learnable sequence-to-sequence modules, exemplified by an
LSTM. FAR optimize a multi-head LSTM architecture with a block-wise
distillation objective and a global structural pruning framework to achieve a
family of efficient LSTM-based models from pretrained transformers. We validate
FAR on the DeiT vision transformer family and demonstrate that it matches the
accuracy of the original models on ImageNet and multiple downstream tasks with
reduced parameters and latency. Further analysis shows that FAR preserves the
semantic token relationships and the token-to-token correlation learned in the
transformer's attention module.
\\ ( https://arxiv.org/abs/2505.21535 ,  5645kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21538 (*cross-listing*)
Date: Sat, 24 May 2025 14:25:48 GMT   (2648kb,D)

Title: Caption This, Reason That: VLMs Caught in the Middle
Authors: Zihan Weng and Lucas Gomez and Taylor Whittington Webb and Pouya
  Bashivan
Categories: cs.CV cs.AI
\\
  Vision-Language Models (VLMs) have shown remarkable progress in visual
understanding in recent years. Yet, they still lag behind human capabilities in
specific visual tasks such as counting or relational reasoning. To understand
the underlying limitations, we adopt methodologies from cognitive science,
analyzing VLM performance along core cognitive axes: Perception, Attention, and
Memory. Using a suite of tasks targeting these abilities, we evaluate
state-of-the-art VLMs, including GPT-4o. Our analysis reveals distinct
cognitive profiles: while advanced models approach ceiling performance on some
tasks (e.g. category identification), a significant gap persists, particularly
in tasks requiring spatial understanding or selective attention. Investigating
the source of these failures and potential methods for improvement, we employ a
vision-text decoupling analysis, finding that models struggling with direct
visual reasoning show marked improvement when reasoning over their own
generated text captions. These experiments reveal a strong need for improved
VLM Chain-of-Thought (CoT) abilities, even in models that consistently exceed
human performance. Furthermore, we demonstrate the potential of targeted
fine-tuning on composite visual reasoning tasks and show that fine-tuning
smaller VLMs substantially improves core cognitive abilities. While this
improvement does not translate to large enhancements on challenging,
out-of-distribution benchmarks, we show broadly that VLM performance on our
datasets strongly correlates with performance on these other benchmarks. Our
work provides a detailed analysis of VLM cognitive strengths and weaknesses and
identifies key bottlenecks in simultaneous perception and reasoning while also
providing an effective and simple solution.
\\ ( https://arxiv.org/abs/2505.21538 ,  2648kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21539 (*cross-listing*)
Date: Sat, 24 May 2025 14:27:20 GMT   (2160kb)

Title: Equivariant Flow Matching for Point Cloud Assembly
Authors: Ziming Wang, Nan Xue, Rebecka J\"ornsten
Categories: cs.CV cs.AI
\\
  The goal of point cloud assembly is to reconstruct a complete 3D shape by
aligning multiple point cloud pieces. This work presents a novel equivariant
solver for assembly tasks based on flow matching models. We first theoretically
show that the key to learning equivariant distributions via flow matching is to
learn related vector fields. Based on this result, we propose an assembly
model, called equivariant diffusion assembly (Eda), which learns related vector
fields conditioned on the input pieces. We further construct an equivariant
path for Eda, which guarantees high data efficiency of the training process.
Our numerical results show that Eda is highly competitive on practical
datasets, and it can even handle the challenging situation where the input
pieces are non-overlapped.
\\ ( https://arxiv.org/abs/2505.21539 ,  2160kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21541 (*cross-listing*)
Date: Sat, 24 May 2025 16:08:04 GMT   (29816kb)

Title: DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via
  Diffusion Transformers
Authors: Zitong Wang, Hang Zhao, Qianyu Zhou, Xuequan Lu, Xiangtai Li, Yiren
  Song
Categories: cs.CV cs.AI
\\
  Diffusion models have recently motivated great success in many generation
tasks like object removal. Nevertheless, existing image decomposition methods
struggle to disentangle semi-transparent or transparent layer occlusions due to
mask prior dependencies, static object assumptions, and the lack of datasets.
In this paper, we delve into a novel task: Layer-Wise Decomposition of
Alpha-Composited Images, aiming to recover constituent layers from single
overlapped images under the condition of semi-transparent/transparent alpha
layer non-linear occlusion. To address challenges in layer ambiguity,
generalization, and data scarcity, we first introduce AlphaBlend, the first
large-scale and high-quality dataset for transparent and semi-transparent layer
decomposition, supporting six real-world subtasks (e.g., translucent flare
removal, semi-transparent cell decomposition, glassware decomposition).
Building on this dataset, we present DiffDecompose, a diffusion
Transformer-based framework that learns the posterior over possible layer
decompositions conditioned on the input image, semantic prompts, and blending
type. Rather than regressing alpha mattes directly, DiffDecompose performs
In-Context Decomposition, enabling the model to predict one or multiple layers
without per-layer supervision, and introduces Layer Position Encoding Cloning
to maintain pixel-level correspondence across layers. Extensive experiments on
the proposed AlphaBlend dataset and public LOGO dataset verify the
effectiveness of DiffDecompose. The code and dataset will be available upon
paper acceptance. Our code will be available at:
https://github.com/Wangzt1121/DiffDecompose.
\\ ( https://arxiv.org/abs/2505.21541 ,  29816kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21547 (*cross-listing*)
Date: Sat, 24 May 2025 22:36:15 GMT   (3182kb)

Title: Image Tokens Matter: Mitigating Hallucination in Discrete
  Tokenizer-based Large Vision-Language Models via Latent Editing
Authors: Weixing Wang, Zifeng Ding, Jindong Gu, Rui Cao, Christoph Meinel,
  Gerard de Melo, Haojin Yang
Categories: cs.CV cs.AI
\\
  Large Vision-Language Models (LVLMs) with discrete image tokenizers unify
multimodal representations by encoding visual inputs into a finite set of
tokens. Despite their effectiveness, we find that these models still
hallucinate non-existent objects. We hypothesize that this may be due to visual
priors induced during training: When certain image tokens frequently co-occur
in the same spatial regions and represent shared objects, they become strongly
associated with the verbalizations of those objects. As a result, the model may
hallucinate by evoking visually absent tokens that often co-occur with present
ones. To test this assumption, we construct a co-occurrence graph of image
tokens using a segmentation dataset and employ a Graph Neural Network (GNN)
with contrastive learning followed by a clustering method to group tokens that
frequently co-occur in similar visual contexts. We find that hallucinations
predominantly correspond to clusters whose tokens dominate the input, and more
specifically, that the visually absent tokens in those clusters show much
higher correlation with hallucinated objects compared to tokens present in the
image. Based on this observation, we propose a hallucination mitigation method
that suppresses the influence of visually absent tokens by modifying latent
image embeddings during generation. Experiments show our method reduces
hallucinations while preserving expressivity. Code is available at
https://github.com/weixingW/CGC-VTD/tree/main
\\ ( https://arxiv.org/abs/2505.21547 ,  3182kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21548 (*cross-listing*)
Date: Sun, 25 May 2025 01:59:23 GMT   (581kb)

Title: Fluent but Culturally Distant: Can Regional Training Teach Cultural
  Understanding?
Authors: Dhruv Agarwal, Anya Shukla, Sunayana Sitaram, Aditya Vashistha
Categories: physics.soc-ph cs.AI cs.CL cs.CY
Comments: Under review
\\
  Large language models (LLMs) are used around the world but exhibit Western
cultural tendencies. To address this cultural misalignment, many countries have
begun developing "regional" LLMs tailored to local communities. Yet it remains
unclear whether these models merely speak the language of their users or also
reflect their cultural values and practices. Using India as a case study, we
evaluate five Indic and five global LLMs along two key dimensions: values (via
the Inglehart-Welzel map and GlobalOpinionQA) and practices (via CulturalBench
and NormAd). Across all four tasks, we find that Indic models do not align more
closely with Indian cultural norms than global models. In fact, an average
American person is a better proxy for Indian cultural values than any Indic
model. Even prompting strategies fail to meaningfully improve alignment.
Ablations show that regional fine-tuning does not enhance cultural competence
and may in fact hurt it by impeding recall of existing knowledge. We trace this
failure to the scarcity of high-quality, untranslated, and culturally grounded
pretraining and fine-tuning data. Our study positions cultural evaluation as a
first-class requirement alongside multilingual benchmarks and offers a reusable
methodology for developers. We call for deeper investments in culturally
representative data to build and evaluate truly sovereign LLMs.
\\ ( https://arxiv.org/abs/2505.21548 ,  581kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21550 (*cross-listing*)
Date: Sun, 25 May 2025 14:25:08 GMT   (1066kb)

Title: Collaborative Agentic AI Needs Interoperability Across Ecosystems
Authors: Rishi Sharma, Martijn de Vos, Pradyumna Chari, Ramesh Raskar,
  Anne-Marie Kermarrec
Categories: cs.NI cs.AI cs.MA
\\
  Collaborative agentic AI is projected to transform entire industries by
enabling AI-powered agents to autonomously perceive, plan, and act within
digital environments. Yet, current solutions in this field are all built in
isolation, and we are rapidly heading toward a landscape of fragmented,
incompatible ecosystems. In this position paper, we argue that
interoperability, achieved by the adoption of minimal standards, is essential
to ensure open, secure, web-scale, and widely-adopted agentic ecosystems. To
this end, we devise a minimal architectural foundation for collaborative
agentic AI, named Web of Agents, which is composed of four components:
agent-to-agent messaging, interaction interoperability, state management, and
agent discovery. Web of Agents adopts existing standards and reuses existing
infrastructure where possible. With Web of Agents, we take the first but
critical step toward interoperable agentic systems and offer a pragmatic path
forward before ecosystem fragmentation becomes the norm.
\\ ( https://arxiv.org/abs/2505.21550 ,  1066kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21551 (*cross-listing*)
Date: Sun, 25 May 2025 21:48:03 GMT   (40kb)

Title: WhisperD: Dementia Speech Recognition and Filler Word Detection with
  Whisper
Authors: Emmanuel Akinrintoyo, Nadine Abdelhalim, Nicole Salomons
Categories: eess.AS cs.AI cs.LG cs.SD
Comments: Submitted to Interspeech 2025 (Accepted)
\\
  Whisper fails to correctly transcribe dementia speech because persons with
dementia (PwDs) often exhibit irregular speech patterns and disfluencies such
as pauses, repetitions, and fragmented sentences. It was trained on standard
speech and may have had little or no exposure to dementia-affected speech.
However, correct transcription is vital for dementia speech for cost-effective
diagnosis and the development of assistive technology. In this work, we
fine-tune Whisper with the open-source dementia speech dataset (DementiaBank)
and our in-house dataset to improve its word error rate (WER). The fine-tuning
also includes filler words to ascertain the filler inclusion rate (FIR) and F1
score. The fine-tuned models significantly outperformed the off-the-shelf
models. The medium-sized model achieved a WER of 0.24, outperforming previous
work. Similarly, there was a notable generalisability to unseen data and speech
patterns.
\\ ( https://arxiv.org/abs/2505.21551 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21553 (*cross-listing*)
Date: Mon, 26 May 2025 04:23:54 GMT   (590kb)

Title: MetaSTNet: Multimodal Meta-learning for Cellular Traffic Conformal
  Prediction
Authors: Hui Ma, and Kai Yang
Categories: cs.NI cs.AI cs.LG
\\
  Network traffic prediction techniques have attracted much attention since
they are valuable for network congestion control and user experience
improvement. While existing prediction techniques can achieve favorable
performance when there is sufficient training data, it remains a great
challenge to make accurate predictions when only a small amount of training
data is available. To tackle this problem, we propose a deep learning model,
entitled MetaSTNet, based on a multimodal meta-learning framework. It is an
end-to-end network architecture that trains the model in a simulator and
transfers the meta-knowledge to a real-world environment, which can quickly
adapt and obtain accurate predictions on a new task with only a small amount of
real-world training data. In addition, we further employ cross conformal
prediction to assess the calibrated prediction intervals. Extensive experiments
have been conducted on real-world datasets to illustrate the efficiency and
effectiveness of MetaSTNet.
\\ ( https://arxiv.org/abs/2505.21553 ,  590kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21556 (*cross-listing*)
Date: Mon, 26 May 2025 17:27:32 GMT   (1679kb)

Title: Benign-to-Toxic Jailbreaking: Inducing Harmful Responses from Harmless
  Prompts
Authors: Hee-Seon Kim, Minbeom Kim, Wonjun Lee, Kihyun Kim, Changick Kim
Categories: cs.CV cs.AI
Comments: LVLM, Jailbreak
\\
  Optimization-based jailbreaks typically adopt the Toxic-Continuation setting
in large vision-language models (LVLMs), following the standard next-token
prediction objective. In this setting, an adversarial image is optimized to
make the model predict the next token of a toxic prompt. However, we find that
the Toxic-Continuation paradigm is effective at continuing already-toxic
inputs, but struggles to induce safety misalignment when explicit toxic signals
are absent. We propose a new paradigm: Benign-to-Toxic (B2T) jailbreak. Unlike
prior work, we optimize adversarial images to induce toxic outputs from benign
conditioning. Since benign conditioning contains no safety violations, the
image alone must break the model's safety mechanisms. Our method outperforms
prior approaches, transfers in black-box settings, and complements text-based
jailbreaks. These results reveal an underexplored vulnerability in multimodal
alignment and introduce a fundamentally new direction for jailbreak approaches.
\\ ( https://arxiv.org/abs/2505.21556 ,  1679kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21557 (*cross-listing*)
Date: Mon, 26 May 2025 19:17:19 GMT   (2439kb)

Title: Analytical Calculation of Weights Convolutional Neural Network
Authors: Polad Geidarov
Categories: cs.CV cs.AI
Journal-ref: Optical Memory and Neural Networks 33 (2024) 157-177
\\
  This paper presents an algorithm for analytically calculating the weights and
thresholds of convolutional neural networks (CNNs) without using standard
training procedures. The algorithm enables the determination of CNN parameters
based on just 10 selected images from the MNIST dataset, each representing a
digit from 0 to 9. As part of the method, the number of channels in CNN layers
is also derived analytically. A software module was implemented in C++ Builder,
and a series of experiments were conducted using the MNIST dataset. Results
demonstrate that the analytically computed CNN can recognize over half of 1000
handwritten digit images without any training, achieving inference in fractions
of a second. These findings suggest that CNNs can be constructed and applied
directly for classification tasks without training, using purely analytical
computation of weights.
\\ ( https://arxiv.org/abs/2505.21557 ,  2439kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21558 (*cross-listing*)
Date: Mon, 26 May 2025 20:18:45 GMT   (1114kb)

Title: A Novel Convolutional Neural Network-Based Framework for Complex
  Multiclass Brassica Seed Classification
Authors: Elhoucine Elfatimia, Recep Eryigitb and Lahcen Elfatimi
Categories: cs.CV cs.AI cs.LG
Comments: 11 Figure
MSC-class: na
\\
  Agricultural research has accelerated in recent years, yet farmers often lack
the time and resources for on-farm research due to the demands of crop
production and farm operations. Seed classification offers valuable insights
into quality control, production efficiency, and impurity detection. Early
identification of seed types is critical to reducing the cost and risk
associated with field emergence, which can lead to yield losses or disruptions
in downstream processes like harvesting. Seed sampling supports growers in
monitoring and managing seed quality, improving precision in determining seed
purity levels, guiding management adjustments, and enhancing yield estimations.
This study proposes a novel convolutional neural network (CNN)-based framework
for the efficient classification of ten common Brassica seed types. The
approach addresses the inherent challenge of texture similarity in seed images
using a custom-designed CNN architecture. The model's performance was evaluated
against several pre-trained state-of-the-art architectures, with adjustments to
layer configurations for optimized classification. Experimental results using
our collected Brassica seed dataset demonstrate that the proposed model
achieved a high accuracy rate of 93 percent.
\\ ( https://arxiv.org/abs/2505.21558 ,  1114kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21563 (*cross-listing*)
Date: Tue, 27 May 2025 03:35:07 GMT   (879kb)

Title: Fog Intelligence for Network Anomaly Detection
Authors: Kai Yang, Hui Ma, and Shaoyu Dou
Categories: cs.NI cs.AI
Comments: published in IEEE Network
DOI: 10.1109/MNET.001.1900156
\\
  Anomalies are common in network system monitoring. When manifested as network
threats to be mitigated, service outages to be prevented, and security risks to
be ameliorated, detecting such anomalous network behaviors becomes of great
importance. However, the growing scale and complexity of the mobile
communication networks, as well as the ever-increasing amount and
dimensionality of the network surveillance data, make it extremely difficult to
monitor a mobile network and discover abnormal network behaviors. Recent
advances in machine learning allow for obtaining near-optimal solutions to
complicated decision-making problems with many sources of uncertainty that
cannot be accurately characterized by traditional mathematical models. However,
most machine learning algorithms are centralized, which renders them
inapplicable to a large-scale distributed wireless networks with tens of
millions of mobile devices. In this article, we present fog intelligence, a
distributed machine learning architecture that enables intelligent wireless
network management. It preserves the advantage of both edge processing and
centralized cloud computing. In addition, the proposed architecture is
scalable, privacy-preserving, and well suited for intelligent management of a
distributed wireless network.
\\ ( https://arxiv.org/abs/2505.21563 ,  879kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21568 (*cross-listing*)
Date: Tue, 27 May 2025 05:59:34 GMT   (7984kb)

Title: VoiceMark: Zero-Shot Voice Cloning-Resistant Watermarking Approach
  Leveraging Speaker-Specific Latents
Authors: Haiyun Li, Zhiyong Wu, Xiaofeng Xie, Jingran Xie, Yaoxun Xu, Hanyang
  Peng
Categories: cs.SD cs.AI cs.CR eess.AS
Comments: Accepted by Interspeech 2025
\\
  Voice cloning (VC)-resistant watermarking is an emerging technique for
tracing and preventing unauthorized cloning. Existing methods effectively trace
traditional VC models by training them on watermarked audio but fail in
zero-shot VC scenarios, where models synthesize audio from an audio prompt
without training. To address this, we propose VoiceMark, the first zero-shot
VC-resistant watermarking method that leverages speaker-specific latents as the
watermark carrier, allowing the watermark to transfer through the zero-shot VC
process into the synthesized audio. Additionally, we introduce VC-simulated
augmentations and VAD-based loss to enhance robustness against distortions.
Experiments on multiple zero-shot VC models demonstrate that VoiceMark achieves
over 95% accuracy in watermark detection after zero-shot VC synthesis,
significantly outperforming existing methods, which only reach around 50%. See
our code and demos at: https://huggingface.co/spaces/haiyunli/VoiceMark
\\ ( https://arxiv.org/abs/2505.21568 ,  7984kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21572 (*cross-listing*)
Date: Tue, 27 May 2025 07:18:08 GMT   (6695kb)

Title: Thickness-aware E(3)-Equivariant 3D Mesh Neural Networks
Authors: Sungwon Kim, Namkyeong Lee, Yunyoung Doh, Seungmin Shin, Guimok Cho,
  Seung-Won Jeon, Sangkook Kim, Chanyoung Park
Categories: cs.CV cs.AI cs.LG
Comments: ICML 2025
\\
  Mesh-based 3D static analysis methods have recently emerged as efficient
alternatives to traditional computational numerical solvers, significantly
reducing computational costs and runtime for various physics-based analyses.
However, these methods primarily focus on surface topology and geometry, often
overlooking the inherent thickness of real-world 3D objects, which exhibits
high correlations and similar behavior between opposing surfaces. This
limitation arises from the disconnected nature of these surfaces and the
absence of internal edge connections within the mesh. In this work, we propose
a novel framework, the Thickness-aware E(3)-Equivariant 3D Mesh Neural Network
(T-EMNN), that effectively integrates the thickness of 3D objects while
maintaining the computational efficiency of surface meshes. Additionally, we
introduce data-driven coordinates that encode spatial information while
preserving E(3)-equivariance or invariance properties, ensuring consistent and
robust analysis. Evaluations on a real-world industrial dataset demonstrate the
superior performance of T-EMNN in accurately predicting node-level 3D
deformations, effectively capturing thickness effects while maintaining
computational efficiency.
\\ ( https://arxiv.org/abs/2505.21572 ,  6695kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21575 (*cross-listing*)
Date: Tue, 27 May 2025 07:44:16 GMT   (438kb)

Title: StreamLink: Large-Language-Model Driven Distributed Data Engineering
  System
Authors: Dawei Feng, Di Mei, Huiri Tan, Lei Ren, Xianying Lou, Zhangxi Tan
Categories: cs.DB cs.AI
Comments: Accepted by CIKM Workshop 2024,
  https://sites.google.com/view/cikm2024-rag/papers?authuser=0#h.ddm5fg2z885t
\\
  Large Language Models (LLMs) have shown remarkable proficiency in natural
language understanding (NLU), opening doors for innovative applications. We
introduce StreamLink - an LLM-driven distributed data system designed to
improve the efficiency and accessibility of data engineering tasks. We build
StreamLink on top of distributed frameworks such as Apache Spark and Hadoop to
handle large data at scale. One of the important design philosophies of
StreamLink is to respect user data privacy by utilizing local fine-tuned LLMs
instead of a public AI service like ChatGPT. With help from domain-adapted
LLMs, we can improve our system's understanding of natural language queries
from users in various scenarios and simplify the procedure of generating
database queries like the Structured Query Language (SQL) for information
processing. We also incorporate LLM-based syntax and security checkers to
guarantee the reliability and safety of each generated query. StreamLink
illustrates the potential of merging generative LLMs with distributed data
processing for comprehensive and user-centric data engineering. With this
architecture, we allow users to interact with complex database systems at
different scales in a user-friendly and security-ensured manner, where the SQL
generation reaches over 10\% of execution accuracy compared to baseline
methods, and allow users to find the most concerned item from hundreds of
millions of items within a few seconds using natural language.
\\ ( https://arxiv.org/abs/2505.21575 ,  438kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21577 (*cross-listing*)
Date: Tue, 27 May 2025 08:35:05 GMT   (3521kb)

Title: RepoMaster: Autonomous Exploration and Understanding of GitHub
  Repositories for Complex Task Solving
Authors: Huacan Wang, Ziyi Ni, Shuo Zhang, Shuo Lu, Sen Hu, Ziyang He, Chen Hu,
  Jiaye Lin, Yifu Guo, Yuntao Du, Pin Lyu
Categories: cs.SE cs.AI
Comments: A novel approach; Very practical
\\
  The ultimate goal of code agents is to solve complex tasks autonomously.
Although large language models (LLMs) have made substantial progress in code
generation, real-world tasks typically demand full-fledged code repositories
rather than simple scripts. Building such repositories from scratch remains a
major challenge. Fortunately, GitHub hosts a vast, evolving collection of
open-source repositories, which developers frequently reuse as modular
components for complex tasks. Yet, existing frameworks like OpenHands and
SWE-Agent still struggle to effectively leverage these valuable resources.
Relying solely on README files provides insufficient guidance, and deeper
exploration reveals two core obstacles: overwhelming information and tangled
dependencies of repositories, both constrained by the limited context windows
of current LLMs. To tackle these issues, we propose RepoMaster, an autonomous
agent framework designed to explore and reuse GitHub repositories for solving
complex tasks. For efficient understanding, RepoMaster constructs function-call
graphs, module-dependency graphs, and hierarchical code trees to identify
essential components, providing only identified core elements to the LLMs
rather than the entire repository. During autonomous execution, it
progressively explores related components using our exploration tools and
prunes information to optimize context usage. Evaluated on the adjusted
MLE-bench, RepoMaster achieves a 110% relative boost in valid submissions over
the strongest baseline OpenHands. On our newly released GitTaskBench,
RepoMaster lifts the task-pass rate from 24.1% to 62.9% while reducing token
usage by 95%. Our code and demonstration materials are publicly available at
https://github.com/wanghuacan/RepoMaster.
\\ ( https://arxiv.org/abs/2505.21577 ,  3521kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21589 (*cross-listing*)
Date: Tue, 27 May 2025 12:22:59 GMT   (2806kb)

Title: Do you see what I see? An Ambiguous Optical Illusion Dataset exposing
  limitations of Explainable AI
Authors: Carina Newen, Luca Hinkamp, Maria Ntonti, Emmanuel M\"uller
Categories: cs.CV cs.AI cs.LG
Comments: 19 pages, 18 figures
\\
  From uncertainty quantification to real-world object detection, we recognize
the importance of machine learning algorithms, particularly in safety-critical
domains such as autonomous driving or medical diagnostics. In machine learning,
ambiguous data plays an important role in various machine learning domains.
Optical illusions present a compelling area of study in this context, as they
offer insight into the limitations of both human and machine perception.
Despite this relevance, optical illusion datasets remain scarce. In this work,
we introduce a novel dataset of optical illusions featuring intermingled animal
pairs designed to evoke perceptual ambiguity. We identify generalizable visual
concepts, particularly gaze direction and eye cues, as subtle yet impactful
features that significantly influence model accuracy. By confronting models
with perceptual ambiguity, our findings underscore the importance of concepts
in visual learning and provide a foundation for studying bias and alignment
between human and machine vision. To make this dataset useful for general
purposes, we generate optical illusions systematically with different concepts
discussed in our bias mitigation section. The dataset is accessible in Kaggle
via
https://kaggle.com/datasets/693bf7c6dd2cb45c8a863f9177350c8f9849a9508e9d50526e2ffcc5559a8333.
Our source code can be found at
https://github.com/KDD-OpenSource/Ambivision.git.
\\ ( https://arxiv.org/abs/2505.21589 ,  2806kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21593 (*cross-listing*)
Date: Tue, 27 May 2025 14:33:54 GMT   (23976kb)

Title: Any-to-Bokeh: One-Step Video Bokeh via Multi-Plane Image Guided
  Diffusion
Authors: Yang Yang, Siming Zheng, Jinwei Chen, Boxi Wu, Xiaofei He, Deng Cai,
  Bo Li, Peng-Tao Jiang
Categories: cs.CV cs.AI
Comments: project page: https://vivocameraresearch.github.io/any2bokeh/
\\
  Recent advances in diffusion based editing models have enabled realistic
camera simulation and image-based bokeh, but video bokeh remains largely
unexplored. Existing video editing models cannot explicitly control focus
planes or adjust bokeh intensity, limiting their applicability for controllable
optical effects. Moreover, naively extending image-based bokeh methods to video
often results in temporal flickering and unsatisfactory edge blur transitions
due to the lack of temporal modeling and generalization capability. To address
these challenges, we propose a novel one-step video bokeh framework that
converts arbitrary input videos into temporally coherent, depth-aware bokeh
effects. Our method leverages a multi-plane image (MPI) representation
constructed through a progressively widening depth sampling function, providing
explicit geometric guidance for depth-dependent blur synthesis. By conditioning
a single-step video diffusion model on MPI layers and utilizing the strong 3D
priors from pre-trained models such as Stable Video Diffusion, our approach
achieves realistic and consistent bokeh effects across diverse scenes.
Additionally, we introduce a progressive training strategy to enhance temporal
consistency, depth robustness, and detail preservation. Extensive experiments
demonstrate that our method produces high-quality, controllable bokeh effects
and achieves state-of-the-art performance on multiple evaluation benchmarks.
\\ ( https://arxiv.org/abs/2505.21593 ,  23976kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21596 (*cross-listing*)
Date: Tue, 27 May 2025 16:53:29 GMT   (2072kb)

Title: Learning optimal treatment strategies for intraoperative hypotension
  using deep reinforcement learning
Authors: Esra Adiyeke, Tianqi Liu, Venkata Sai Dheeraj Naganaboina, Han Li,
  Tyler J. Loftus, Yuanfang Ren, Benjamin Shickel, Matthew M. Ruppert,
  Karandeep Singh, Ruogu Fang, Parisa Rashidi, Azra Bihorac, Tezcan
  Ozrazgat-Baslanti
Categories: q-bio.QM cs.AI cs.LG
Comments: 41 pages, 1 table, 5 figures, 5 supplemental tables, 6 supplemental
  figures
\\
  Traditional methods of surgical decision making heavily rely on human
experience and prompt actions, which are variable. A data-driven system
generating treatment recommendations based on patient states can be a
substantial asset in perioperative decision-making, as in cases of
intraoperative hypotension, for which suboptimal management is associated with
acute kidney injury (AKI), a common and morbid postoperative complication. We
developed a Reinforcement Learning (RL) model to recommend optimum dose of
intravenous (IV) fluid and vasopressors during surgery to avoid intraoperative
hypotension and postoperative AKI. We retrospectively analyzed 50,021 surgeries
from 42,547 adult patients who underwent major surgery at a quaternary care
hospital between June 2014 and September 2020. Of these, 34,186 surgeries were
used for model training and 15,835 surgeries were reserved for testing. We
developed a Deep Q-Networks based RL model using 16 variables including
intraoperative physiologic time series, total dose of IV fluid and vasopressors
extracted for every 15-minute epoch. The model replicated 69% of physician's
decisions for the dosage of vasopressors and proposed higher or lower dosage of
vasopressors than received in 10% and 21% of the treatments, respectively. In
terms of IV fluids, the model's recommendations were within 0.05 ml/kg/15 min
of the actual dose in 41% of the cases, with higher or lower doses recommended
for 27% and 32% of the treatments, respectively. The model resulted in a higher
estimated policy value compared to the physicians' actual treatments, as well
as random and zero-drug policies. AKI prevalence was the lowest in patients
receiving medication dosages that aligned with model's decisions. Our findings
suggest that implementation of the model's policy has the potential to reduce
postoperative AKI and improve other outcomes driven by intraoperative
hypotension.
\\ ( https://arxiv.org/abs/2505.21596 ,  2072kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21600 (*cross-listing*)
Date: Tue, 27 May 2025 16:57:20 GMT   (479kb,D)

Title: R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large
  Model Token Routing
Authors: Tianyu Fu, Yi Ge, Yichen You, Enshu Liu, Zhihang Yuan, Guohao Dai,
  Shengen Yan, Huazhong Yang, Yu Wang
Categories: cs.CL cs.AI cs.LG cs.PF
ACM-class: I.2.7
\\
  Large Language Models (LLMs) achieve impressive reasoning capabilities at the
cost of substantial inference overhead, posing substantial deployment
challenges. Although distilled Small Language Models (SLMs) significantly
enhance efficiency, their performance suffers as they fail to follow LLMs'
reasoning paths. Luckily, we reveal that only a small fraction of tokens
genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens
are either identical or exhibit neutral differences, such as minor variations
in abbreviations or expressions. Leveraging this insight, we introduce **Roads
to Rome (R2R)**, a neural token routing method that selectively utilizes LLMs
only for these critical, path-divergent tokens, while leaving the majority of
token generation to the SLM. We also develop an automatic data generation
pipeline that identifies divergent tokens and generates token-level routing
labels to train the lightweight router. We apply R2R to combine R1-1.5B and
R1-32B models from the DeepSeek family, and evaluate on challenging math,
coding, and QA benchmarks. With an average activated parameter size of 5.6B,
R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the
R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with
comparable performance, advancing the Pareto frontier of test-time scaling
efficiency. Our code is available at https://github.com/thu-nics/R2R.
\\ ( https://arxiv.org/abs/2505.21600 ,  479kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21603 (*cross-listing*)
Date: Tue, 27 May 2025 17:16:31 GMT   (506kb)

Title: Leveraging XP and CRISP-DM for Agile Data Science Projects
Authors: Andre Massahiro Shimaoka, Renato Cordeiro Ferreira, Alfredo Goldman
Categories: cs.SE cs.AI cs.LG
\\
  This study explores the integration of eXtreme Programming (XP) and the
Cross-Industry Standard Process for Data Mining (CRISP-DM) in agile Data
Science projects. We conducted a case study at the e-commerce company Elo7 to
answer the research question: How can the agility of the XP method be
integrated with CRISP-DM in Data Science projects? Data was collected through
interviews and questionnaires with a Data Science team consisting of data
scientists, ML engineers, and data product managers. The results show that 86%
of the team frequently or always applies CRISP-DM, while 71% adopt XP practices
in their projects. Furthermore, the study demonstrates that it is possible to
combine CRISP-DM with XP in Data Science projects, providing a structured and
collaborative approach. Finally, the study generated improvement
recommendations for the company.
\\ ( https://arxiv.org/abs/2505.21603 ,  506kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21608 (*cross-listing*)
Date: Tue, 27 May 2025 17:57:44 GMT   (13039kb)

Title: How does Misinformation Affect Large Language Model Behaviors and
  Preferences?
Authors: Miao Peng, Nuo Chen, Jianheng Tang, Jia Li
Categories: cs.CL cs.AI
Comments: Accepted to ACL 2025 Main Conference
\\
  Large Language Models (LLMs) have shown remarkable capabilities in
knowledge-intensive tasks, while they remain vulnerable when encountering
misinformation. Existing studies have explored the role of LLMs in combating
misinformation, but there is still a lack of fine-grained analysis on the
specific aspects and extent to which LLMs are influenced by misinformation. To
bridge this gap, we present MisBench, the current largest and most
comprehensive benchmark for evaluating LLMs' behavior and knowledge preference
toward misinformation. MisBench consists of 10,346,712 pieces of
misinformation, which uniquely considers both knowledge-based conflicts and
stylistic variations in misinformation. Empirical results reveal that while
LLMs demonstrate comparable abilities in discerning misinformation, they still
remain susceptible to knowledge conflicts and stylistic variations. Based on
these findings, we further propose a novel approach called Reconstruct to
Discriminate (RtD) to strengthen LLMs' ability to detect misinformation. Our
study provides valuable insights into LLMs' interactions with misinformation,
and we believe MisBench can serve as an effective benchmark for evaluating
LLM-based detectors and enhancing their reliability in real-world applications.
Codes and data are available at https://github.com/GKNL/MisBench.
\\ ( https://arxiv.org/abs/2505.21608 ,  13039kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21609 (*cross-listing*)
Date: Tue, 27 May 2025 17:59:05 GMT   (6623kb)

Title: Preventing Adversarial AI Attacks Against Autonomous Situational
  Awareness: A Maritime Case Study
Authors: Mathew J. Walter, Aaron Barrett, and Kimberly Tam
Categories: cs.CR cs.AI
\\
  Adversarial artificial intelligence (AI) attacks pose a significant threat to
autonomous transportation, such as maritime vessels, that rely on AI
components. Malicious actors can exploit these systems to deceive and
manipulate AI-driven operations. This paper addresses three critical research
challenges associated with adversarial AI: the limited scope of traditional
defences, inadequate security metrics, and the need to build resilience beyond
model-level defences. To address these challenges, we propose building defences
utilising multiple inputs and data fusion to create defensive components and an
AI security metric as a novel approach toward developing more secure AI
systems. We name this approach the Data Fusion Cyber Resilience (DFCR) method,
and we evaluate it through real-world demonstrations and comprehensive
quantitative analyses, comparing a system built with the DFCR method against
single-input models and models utilising existing state-of-the-art defences.
The findings show that the DFCR approach significantly enhances resilience
against adversarial machine learning attacks in maritime autonomous system
operations, achieving up to a 35\% reduction in loss for successful
multi-pronged perturbation attacks, up to a 100\% reduction in loss for
successful adversarial patch attacks and up to 100\% reduction in loss for
successful spoofing attacks when using these more resilient systems. We
demonstrate how DFCR and DFCR confidence scores can reduce adversarial AI
contact confidence and improve decision-making by the system, even when typical
adversarial defences have been compromised. Ultimately, this work contributes
to the development of more secure and resilient AI-driven systems against
adversarial attacks.
\\ ( https://arxiv.org/abs/2505.21609 ,  6623kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21620 (*cross-listing*)
Date: Tue, 27 May 2025 18:00:03 GMT   (41238kb)

Title: VideoMarkBench: Benchmarking Robustness of Video Watermarking
Authors: Zhengyuan Jiang, Moyang Guo, Kecen Li, Yuepeng Hu, Yupu Wang, Zhicong
  Huang, Cheng Hong, Neil Zhenqiang Gong
Categories: cs.CR cs.AI cs.CV cs.LG
\\
  The rapid development of video generative models has led to a surge in highly
realistic synthetic videos, raising ethical concerns related to disinformation
and copyright infringement. Recently, video watermarking has been proposed as a
mitigation strategy by embedding invisible marks into AI-generated videos to
enable subsequent detection. However, the robustness of existing video
watermarking methods against both common and adversarial perturbations remains
underexplored. In this work, we introduce VideoMarkBench, the first systematic
benchmark designed to evaluate the robustness of video watermarks under
watermark removal and watermark forgery attacks. Our study encompasses a
unified dataset generated by three state-of-the-art video generative models,
across three video styles, incorporating four watermarking methods and seven
aggregation strategies used during detection. We comprehensively evaluate 12
types of perturbations under white-box, black-box, and no-box threat models.
Our findings reveal significant vulnerabilities in current watermarking
approaches and highlight the urgent need for more robust solutions. Our code is
available at https://github.com/zhengyuan-jiang/VideoMarkBench.
\\ ( https://arxiv.org/abs/2505.21620 ,  41238kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21636 (*cross-listing*)
Date: Tue, 27 May 2025 18:09:27 GMT   (9489kb,D)

Title: The Feasibility of Topic-Based Watermarking on Academic Peer Reviews
Authors: Alexander Nemecek, Yuzhou Jiang, Erman Ayday
Categories: cs.CR cs.AI
Comments: 8 pages main, 9 pages appendix
\\
  Large language models (LLMs) are increasingly integrated into academic
workflows, with many conferences and journals permitting their use for tasks
such as language refinement and literature summarization. However, their use in
peer review remains prohibited due to concerns around confidentiality breaches,
hallucinated content, and inconsistent evaluations. As LLM-generated text
becomes more indistinguishable from human writing, there is a growing need for
reliable attribution mechanisms to preserve the integrity of the review
process. In this work, we evaluate topic-based watermarking (TBW), a
lightweight, semantic-aware technique designed to embed detectable signals into
LLM-generated text. We conduct a comprehensive assessment across multiple LLM
configurations, including base, few-shot, and fine-tuned variants, using
authentic peer review data from academic conferences. Our results show that TBW
maintains review quality relative to non-watermarked outputs, while
demonstrating strong robustness to paraphrasing-based evasion. These findings
highlight the viability of TBW as a minimally intrusive and practical solution
for enforcing LLM usage in peer review.
\\ ( https://arxiv.org/abs/2505.21636 ,  9489kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21657 (*cross-listing*)
Date: Tue, 27 May 2025 18:32:38 GMT   (2976kb)

Title: Explainability of Large Language Models using SMILE: Statistical
  Model-agnostic Interpretability with Local Explanations
Authors: Zeinab Dehghani, Koorosh Aslansefat, Adil Khan and Mohammed Naveed
  Akram
Categories: cs.CL cs.AI cs.LG
Comments: arXiv admin note: text overlap with arXiv:2412.16277
\\
  Large language models like GPT, LLAMA, and Claude have become incredibly
powerful at generating text, but they are still black boxes, so it is hard to
understand how they decide what to say. That lack of transparency can be
problematic, especially in fields where trust and accountability matter. To
help with this, we introduce SMILE, a new method that explains how these models
respond to different parts of a prompt. SMILE is model-agnostic and works by
slightly changing the input, measuring how the output changes, and then
highlighting which words had the most impact. Create simple visual heat maps
showing which parts of a prompt matter the most. We tested SMILE on several
leading LLMs and used metrics such as accuracy, consistency, stability, and
fidelity to show that it gives clear and reliable explanations. By making these
models easier to understand, SMILE brings us one step closer to making AI more
transparent and trustworthy.
\\ ( https://arxiv.org/abs/2505.21657 ,  2976kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21670 (*cross-listing*)
Date: Tue, 27 May 2025 18:48:40 GMT   (3159kb)

Title: Rethinking the Outlier Distribution in Large Language Models: An
  In-depth Study
Authors: Rahul Raman, Khushi Sharma, Sai Qian Zhang
Categories: cs.CL cs.AI
\\
  Investigating outliers in large language models (LLMs) is crucial due to
their significant impact on various aspects of LLM performance, including
quantization and compression. Outliers often cause considerable quantization
errors, leading to degraded model performance. Identifying and addressing these
outliers can enhance the accuracy and efficiency of the quantization process,
enabling smoother deployment on edge devices or specialized hardware. Recent
studies have identified two common types of outliers in LLMs: massive
activations and channel-wise outliers. While numerous quantization algorithms
have been proposed to mitigate their effects and maintain satisfactory
accuracy, few have thoroughly explored the root causes of these outliers in
depth. In this paper, we conduct a comprehensive investigation into the
formation mechanisms of these outliers and propose potential strategies to
mitigate their occurrence. Ultimately, we introduce some efficient approaches
to eliminate most massive activations and channel-wise outliers with minimal
impact on accuracy.
\\ ( https://arxiv.org/abs/2505.21670 ,  3159kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21689 (*cross-listing*)
Date: Tue, 27 May 2025 19:25:24 GMT   (1051kb,D)

Title: LLMPR: A Novel LLM-Driven Transfer Learning based Petition Ranking Model
Authors: Avijit Gayen, Somyajit Chakraborty, Mainak Sen, Soham Paul, Angshuman
  Jana
Categories: cs.CL cs.AI cs.LG
Comments: 28 pages, 5 figures, journal paper, submitted to AI and Law
\\
  The persistent accumulation of unresolved legal cases, especially within the
Indian judiciary, significantly hampers the timely delivery of justice. Manual
methods of prioritizing petitions are often prone to inefficiencies and
subjective biases further exacerbating delays. To address this issue, we
propose LLMPR (Large Language Model-based Petition Ranking), an automated
framework that utilizes transfer learning and machine learning to assign
priority rankings to legal petitions based on their contextual urgency.
Leveraging the ILDC dataset comprising 7,593 annotated petitions, we process
unstructured legal text and extract features through various embedding
techniques, including DistilBERT, LegalBERT, and MiniLM. These textual
embeddings are combined with quantitative indicators such as gap days, rank
scores, and word counts to train multiple machine learning models, including
Random Forest, Decision Tree, XGBoost, LightGBM, and CatBoost. Our experiments
demonstrate that Random Forest and Decision Tree models yield superior
performance, with accuracy exceeding 99% and a Spearman rank correlation of
0.99. Notably, models using only numerical features achieve nearly optimal
ranking results (R2 = 0.988, \r{ho} = 0.998), while LLM-based embeddings offer
only marginal gains. These findings suggest that automated petition ranking can
effectively streamline judicial workflows, reduce case backlog, and improve
fairness in legal prioritization.
\\ ( https://arxiv.org/abs/2505.21689 ,  1051kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21699 (*cross-listing*)
Date: Tue, 27 May 2025 19:38:23 GMT   (1583kb)

Title: STA-Risk: A Deep Dive of Spatio-Temporal Asymmetries for Breast Cancer
  Risk Prediction
Authors: Zhengbo Zhou, Dooman Arefan, Margarita Zuley, Jules Sumkin, Shandong
  Wu
Categories: eess.IV cs.AI cs.CV
\\
  Predicting the risk of developing breast cancer is an important clinical tool
to guide early intervention and tailoring personalized screening strategies.
Early risk models have limited performance and recently machine learning-based
analysis of mammogram images showed encouraging risk prediction effects. These
models however are limited to the use of a single exam or tend to overlook
nuanced breast tissue evolvement in spatial and temporal details of
longitudinal imaging exams that are indicative of breast cancer risk. In this
paper, we propose STA-Risk (Spatial and Temporal Asymmetry-based Risk
Prediction), a novel Transformer-based model that captures fine-grained
mammographic imaging evolution simultaneously from bilateral and longitudinal
asymmetries for breast cancer risk prediction. STA-Risk is innovative by the
side encoding and temporal encoding to learn spatial-temporal asymmetries,
regulated by a customized asymmetry loss. We performed extensive experiments
with two independent mammogram datasets and achieved superior performance than
four representative SOTA models for 1- to 5-year future risk prediction. Source
codes will be released upon publishing of the paper.
\\ ( https://arxiv.org/abs/2505.21699 ,  1583kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21703 (*cross-listing*)
Date: Tue, 27 May 2025 19:40:57 GMT   (980kb)

Title: A Joint Reconstruction-Triplet Loss Autoencoder Approach Towards Unseen
  Attack Detection in IoV Networks
Authors: Julia Boone, Tolunay Seyfi, Fatemeh Afghah
Categories: cs.CR cs.AI cs.NI
Comments: Accepted for publication in the IEEE Internet of Things Journal
  (IoT-J)
\\
  Internet of Vehicles (IoV) systems, while offering significant advancements
in transportation efficiency and safety, introduce substantial security
vulnerabilities due to their highly interconnected nature. These dynamic
systems produce massive amounts of data between vehicles, infrastructure, and
cloud services and present a highly distributed framework with a wide attack
surface. In considering network-centered attacks on IoV systems, attacks such
as Denial-of-Service (DoS) can prohibit the communication of essential physical
traffic safety information between system elements, illustrating that the
security concerns for these systems go beyond the traditional confidentiality,
integrity, and availability concerns of enterprise systems. Given the
complexity and volume of data generated by IoV systems, traditional security
mechanisms are often inadequate for accurately detecting sophisticated and
evolving cyberattacks. Here, we present an unsupervised autoencoder method
trained entirely on benign network data for the purpose of unseen attack
detection in IoV networks. We leverage a weighted combination of reconstruction
and triplet margin loss to guide the autoencoder training and develop a diverse
representation of the benign training set. We conduct extensive experiments on
recent network intrusion datasets from two different application domains,
industrial IoT and home IoT, that represent the modern IoV task. We show that
our method performs robustly for all unseen attack types, with roughly 99%
accuracy on benign data and between 97% and 100% performance on anomaly data.
We extend these results to show that our model is adaptable through the use of
transfer learning, achieving similarly high results while leveraging domain
features from one domain to another.
\\ ( https://arxiv.org/abs/2505.21703 ,  980kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21715 (*cross-listing*)
Date: Tue, 27 May 2025 20:01:12 GMT   (1679kb)

Title: Privacy-Preserving Chest X-ray Report Generation via Multimodal
  Federated Learning with ViT and GPT-2
Authors: Md. Zahid Hossain, Mustofa Ahmed, Most. Sharmin Sultana Samu, Md.
  Rakibul Islam
Categories: eess.IV cs.AI cs.CV
Comments: Preprint, manuscript under-review
\\
  The automated generation of radiology reports from chest X-ray images holds
significant promise in enhancing diagnostic workflows while preserving patient
privacy. Traditional centralized approaches often require sensitive data
transfer, posing privacy concerns. To address this, the study proposes a
Multimodal Federated Learning framework for chest X-ray report generation using
the IU-Xray dataset. The system utilizes a Vision Transformer (ViT) as the
encoder and GPT-2 as the report generator, enabling decentralized training
without sharing raw data. Three Federated Learning (FL) aggregation strategies:
FedAvg, Krum Aggregation and a novel Loss-aware Federated Averaging (L-FedAvg)
were evaluated. Among these, Krum Aggregation demonstrated superior performance
across lexical and semantic evaluation metrics such as ROUGE, BLEU, BERTScore
and RaTEScore. The results show that FL can match or surpass centralized models
in generating clinically relevant and semantically rich radiology reports. This
lightweight and privacy-preserving framework paves the way for collaborative
medical AI development without compromising data confidentiality.
\\ ( https://arxiv.org/abs/2505.21715 ,  1679kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21724 (*cross-listing*)
Date: Tue, 27 May 2025 20:12:46 GMT   (20667kb)

Title: OmniResponse: Online Multimodal Conversational Response Generation in
  Dyadic Interactions
Authors: Cheng Luo, Jianghui Wang, Bing Li, Siyang Song, Bernard Ghanem
Categories: cs.CV cs.AI cs.HC
Comments: 23 pages, 9 figures
\\
  In this paper, we introduce Online Multimodal Conversational Response
Generation (OMCRG), a novel task that aims to online generate synchronized
verbal and non-verbal listener feedback, conditioned on the speaker's
multimodal input. OMCRG reflects natural dyadic interactions and poses new
challenges in achieving synchronization between the generated audio and facial
responses of the listener. To address these challenges, we innovatively
introduce text as an intermediate modality to bridge the audio and facial
responses. We hence propose OmniResponse, a Multimodal Large Language Model
(MLLM) that autoregressively generates high-quality multi-modal listener
responses. OmniResponse leverages a pretrained LLM enhanced with two novel
components: Chrono-Text, which temporally anchors generated text tokens, and
TempoVoice, a controllable online TTS module that produces speech synchronized
with facial reactions. To support further OMCRG research, we present
ResponseNet, a new dataset comprising 696 high-quality dyadic interactions
featuring synchronized split-screen videos, multichannel audio, transcripts,
and facial behavior annotations. Comprehensive evaluations conducted on
ResponseNet demonstrate that OmniResponse significantly outperforms baseline
models in terms of semantic speech content, audio-visual synchronization, and
generation quality.
\\ ( https://arxiv.org/abs/2505.21724 ,  20667kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21740 (*cross-listing*)
Date: Tue, 27 May 2025 20:29:50 GMT   (7183kb)

Title: Counterfactual Simulatability of LLM Explanations for Generation Tasks
Authors: Marvin Limpijankit, Yanda Chen, Melanie Subbiah, Nicholas Deas,
  Kathleen McKeown
Categories: cs.CL cs.AI
\\
  LLMs can be unpredictable, as even slight alterations to the prompt can cause
the output to change in unexpected ways. Thus, the ability of models to
accurately explain their behavior is critical, especially in high-stakes
settings. One approach for evaluating explanations is counterfactual
simulatability, how well an explanation allows users to infer the model's
output on related counterfactuals. Counterfactual simulatability has been
previously studied for yes/no question answering tasks. We provide a general
framework for extending this method to generation tasks, using news
summarization and medical suggestion as example use cases. We find that while
LLM explanations do enable users to better predict LLM outputs on
counterfactuals in the summarization setting, there is significant room for
improvement for medical suggestion. Furthermore, our results suggest that the
evaluation for counterfactual simulatability may be more appropriate for
skill-based tasks as opposed to knowledge-based tasks.
\\ ( https://arxiv.org/abs/2505.21740 ,  7183kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21746 (*cross-listing*)
Date: Tue, 27 May 2025 20:34:56 GMT   (6994kb)

Title: Learning to See More: UAS-Guided Super-Resolution of Satellite Imagery
  for Precision Agriculture
Authors: Arif Masrur, Peder A. Olsen, Paul R. Adler, Carlan Jackson, Matthew W.
  Myers, Nathan Sedghi, Ray R. Weil
Categories: cs.CV cs.AI cs.LG
\\
  Unmanned Aircraft Systems (UAS) and satellites are key data sources for
precision agriculture, yet each presents trade-offs. Satellite data offer broad
spatial, temporal, and spectral coverage but lack the resolution needed for
many precision farming applications, while UAS provide high spatial detail but
are limited by coverage and cost, especially for hyperspectral data. This study
presents a novel framework that fuses satellite and UAS imagery using
super-resolution methods. By integrating data across spatial, spectral, and
temporal domains, we leverage the strengths of both platforms cost-effectively.
We use estimation of cover crop biomass and nitrogen (N) as a case study to
evaluate our approach. By spectrally extending UAS RGB data to the vegetation
red edge and near-infrared regions, we generate high-resolution Sentinel-2
imagery and improve biomass and N estimation accuracy by 18% and 31%,
respectively. Our results show that UAS data need only be collected from a
subset of fields and time points. Farmers can then 1) enhance the spectral
detail of UAS RGB imagery; 2) increase the spatial resolution by using
satellite data; and 3) extend these enhancements spatially and across the
growing season at the frequency of the satellite flights. Our SRCNN-based
spectral extension model shows considerable promise for model transferability
over other cropping systems in the Upper and Lower Chesapeake Bay regions.
Additionally, it remains effective even when cloud-free satellite data are
unavailable, relying solely on the UAS RGB input. The spatial extension model
produces better biomass and N predictions than models built on raw UAS RGB
images. Once trained with targeted UAS RGB data, the spatial extension model
allows farmers to stop repeated UAS flights. While we introduce
super-resolution advances, the core contribution is a lightweight and scalable
system for affordable on-farm use.
\\ ( https://arxiv.org/abs/2505.21746 ,  6994kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21755 (*cross-listing*)
Date: Tue, 27 May 2025 20:44:44 GMT   (42534kb,D)

Title: FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal
  Shifts in Visual Question Answering
Authors: Chengyue Huang, Brisa Maneechotesuwan, Shivang Chopra, Zsolt Kira
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: Accepted to CVPR 2025
\\
  Visual question answering (VQA) systems face significant challenges when
adapting to real-world data shifts, especially in multi-modal contexts. While
robust fine-tuning strategies are essential for maintaining performance across
in-distribution (ID) and out-of-distribution (OOD) scenarios, current
evaluation settings are primarily unimodal or particular to some types of OOD,
offering limited insight into the complexities of multi-modal contexts. In this
work, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across
Multi-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We
utilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA
and others, and categorize them into ID, near and far OOD datasets covering
uni-modal, multi-modal and adversarial distribution shifts. We first conduct a
comprehensive comparison of existing robust fine-tuning methods. We then
quantify the distribution shifts by calculating the Mahalanobis distance using
uni-modal and multi-modal embeddings extracted from various models. Further, we
perform an extensive analysis to explore the interactions between uni- and
multi-modal shifts as well as modality importance for ID and OOD samples. These
analyses offer valuable guidance on developing more robust fine-tuning methods
to handle multi-modal distribution shifts. The code is available at
https://github.com/chengyuehuang511/FRAMES-VQA .
\\ ( https://arxiv.org/abs/2505.21755 ,  42534kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21771 (*cross-listing*)
Date: Tue, 27 May 2025 21:09:11 GMT   (2024kb,D)

Title: MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning
Authors: Prasham Yatinkumar Titiya, Jainil Trivedi, Chitta Baral, Vivek Gupta
Categories: cs.CV cs.AI
\\
  Multimodal tables those that integrate semi structured data with visual
elements such as charts and maps are ubiquitous across real world domains, yet
they pose a formidable challenge to current vision language models (VLMs).
While Large Language models (LLMs) and VLMs have demonstrated strong
capabilities in text and image understanding, their performance on complex,
real world multimodal table reasoning remains unexplored. To bridge this gap,
we introduce MMTBENCH (Multimodal Table Benchmark), a benchmark consisting of
500 real world multimodal tables drawn from diverse real world sources, with a
total of 4021 question answer pairs. MMTBENCH questions cover four question
types (Explicit, Implicit, Answer Mention, and Visual Based), five reasoning
types (Mathematical, Extrema Identification, Fact Verification, Vision Based,
and Others), and eight table types (Single/Multiple Entity, Maps and Charts
with Entities, Single/Multiple Charts, Maps, and Visualizations). Extensive
evaluation of state of the art models on all types reveals substantial
performance gaps, particularly on questions requiring visual-based reasoning
and multi-step inference. These findings show the urgent need for improved
architectures that more tightly integrate vision and language processing. By
providing a challenging, high-quality resource that mirrors the complexity of
real-world tasks, MMTBENCH underscores its value as a resource for future
research on multimodal tables.
\\ ( https://arxiv.org/abs/2505.21771 ,  2024kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21786 (*cross-listing*)
Date: Tue, 27 May 2025 21:36:56 GMT   (411kb,D)

Title: VeriTrail: Closed-Domain Hallucination Detection with Traceability
Authors: Dasha Metropolitansky and Jonathan Larson
Categories: cs.CL cs.AI
\\
  Even when instructed to adhere to source material, Language Models often
generate unsubstantiated content - a phenomenon known as "closed-domain
hallucination." This risk is amplified in processes with multiple generative
steps (MGS), compared to processes with a single generative step (SGS).
However, due to the greater complexity of MGS processes, we argue that
detecting hallucinations in their final outputs is necessary but not
sufficient: it is equally important to trace where hallucinated content was
likely introduced and how faithful content may have been derived from the
source through intermediate outputs. To address this need, we present
VeriTrail, the first closed-domain hallucination detection method designed to
provide traceability for both MGS and SGS processes. We also introduce the
first datasets to include all intermediate outputs as well as human annotations
of final outputs' faithfulness for their respective MGS processes. We
demonstrate that VeriTrail outperforms baseline methods on both datasets.
\\ ( https://arxiv.org/abs/2505.21786 ,  411kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21811 (*cross-listing*)
Date: Tue, 27 May 2025 22:38:32 GMT   (793kb)

Title: Revisiting Self-attention for Cross-domain Sequential Recommendation
Authors: Clark Mingxuan Ju, Leonardo Neves, Bhuvesh Kumar, Liam Collins, Tong
  Zhao, Yuwei Qiu, Qing Dou, Sohail Nizam, Sen Yang, Neil Shah
Categories: cs.IR cs.AI
Comments: Accepted to KDD'25
\\
  Sequential recommendation is a popular paradigm in modern recommender
systems. In particular, one challenging problem in this space is cross-domain
sequential recommendation (CDSR), which aims to predict future behaviors given
user interactions across multiple domains. Existing CDSR frameworks are mostly
built on the self-attention transformer and seek to improve by explicitly
injecting additional domain-specific components (e.g. domain-aware module
blocks). While these additional components help, we argue they overlook the
core self-attention module already present in the transformer, a naturally
powerful tool to learn correlations among behaviors. In this work, we aim to
improve the CDSR performance for simple models from a novel perspective of
enhancing the self-attention. Specifically, we introduce a Pareto-optimal
self-attention and formulate the cross-domain learning as a multi-objective
problem, where we optimize the recommendation task while dynamically minimizing
the cross-domain attention scores. Our approach automates knowledge transfer in
CDSR (dubbed as AutoCDSR) -- it not only mitigates negative transfer but also
encourages complementary knowledge exchange among auxiliary domains. Based on
the idea, we further introduce AutoCDSR+, a more performant variant with slight
additional cost. Our proposal is easy to implement and works as a plug-and-play
module that can be incorporated into existing transformer-based recommenders.
Besides flexibility, it is practical to deploy because it brings little extra
computational overheads without heavy hyper-parameter tuning. AutoCDSR on
average improves Recall@10 for SASRec and Bert4Rec by 9.8% and 16.0% and
NDCG@10 by 12.0% and 16.7%, respectively. Code is available at
https://github.com/snap-research/AutoCDSR.
\\ ( https://arxiv.org/abs/2505.21811 ,  793kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21815 (*cross-listing*)
Date: Tue, 27 May 2025 22:49:18 GMT   (958kb,D)

Title: Scientific Paper Retrieval with LLM-Guided Semantic-Based Ranking
Authors: Yunyi Zhang, Ruozhen Yang, Siqi Jiao, SeongKu Kang, Jiawei Han
Categories: cs.IR cs.AI cs.CL
\\
  Scientific paper retrieval is essential for supporting literature discovery
and research. While dense retrieval methods demonstrate effectiveness in
general-purpose tasks, they often fail to capture fine-grained scientific
concepts that are essential for accurate understanding of scientific queries.
Recent studies also use large language models (LLMs) for query understanding;
however, these methods often lack grounding in corpus-specific knowledge and
may generate unreliable or unfaithful content. To overcome these limitations,
we propose SemRank, an effective and efficient paper retrieval framework that
combines LLM-guided query understanding with a concept-based semantic index.
Each paper is indexed using multi-granular scientific concepts, including
general research topics and detailed key phrases. At query time, an LLM
identifies core concepts derived from the corpus to explicitly capture the
query's information need. These identified concepts enable precise semantic
matching, significantly enhancing retrieval accuracy. Experiments show that
SemRank consistently improves the performance of various base retrievers,
surpasses strong existing LLM-based baselines, and remains highly efficient.
\\ ( https://arxiv.org/abs/2505.21815 ,  958kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21827 (*cross-listing*)
Date: Tue, 27 May 2025 23:27:31 GMT   (5228kb)

Title: Music Source Restoration
Authors: Yongyi Zang, Zheqi Dai, Mark D. Plumbley and Qiuqiang Kong
Categories: cs.SD cs.AI cs.LG eess.AS
Comments: A modified version of this paper is in review
\\
  We introduce Music Source Restoration (MSR), a novel task addressing the gap
between idealized source separation and real-world music production. Current
Music Source Separation (MSS) approaches assume mixtures are simple sums of
sources, ignoring signal degradations employed during music production like
equalization, compression, and reverb. MSR models mixtures as degraded sums of
individually degraded sources, with the goal of recovering original, undegraded
signals. Due to the lack of data for MSR, we present RawStems, a dataset
annotation of 578 songs with unprocessed source signals organized into 8
primary and 17 secondary instrument groups, totaling 354.13 hours. To the best
of our knowledge, RawStems is the first dataset that contains unprocessed music
stems with hierarchical categories. We consider spectral filtering, dynamic
range compression, harmonic distortion, reverb and lossy codec as possible
degradations, and establish U-Former as a baseline method, demonstrating the
feasibility of MSR on our dataset. We release the RawStems dataset annotations,
degradation simulation pipeline, training code and pre-trained models to be
publicly available.
\\ ( https://arxiv.org/abs/2505.21827 ,  5228kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21838 (*cross-listing*)
Date: Wed, 28 May 2025 00:13:37 GMT   (151kb)

Title: Nonadaptive Output Regulation of Second-Order Nonlinear Uncertain
  Systems
Authors: Maobin Lu and Martin Guay and Telema Harry and Shimin Wang and Jordan
  Cooper
Categories: eess.SY cs.AI cs.SY math.OC nlin.CD
Comments: 8 pages, 3 figures
\\
  This paper investigates the robust output regulation problem of second-order
nonlinear uncertain systems with an unknown exosystem. Instead of the adaptive
control approach, this paper resorts to a robust control methodology to solve
the problem and thus avoid the bursting phenomenon. In particular, this paper
constructs generic internal models for the steady-state state and input
variables of the system. By introducing a coordinate transformation, this paper
converts the robust output regulation problem into a nonadaptive stabilization
problem of an augmented system composed of the second-order nonlinear uncertain
system and the generic internal models. Then, we design the stabilization
control law and construct a strict Lyapunov function that guarantees the
robustness with respect to unmodeled disturbances. The analysis shows that the
output zeroing manifold of the augmented system can be made attractive by the
proposed nonadaptive control law, which solves the robust output regulation
problem. Finally, we demonstrate the effectiveness of the proposed nonadaptive
internal model approach by its application to the control of the Duffing
system.
\\ ( https://arxiv.org/abs/2505.21838 ,  151kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21847 (*cross-listing*)
Date: Wed, 28 May 2025 00:27:18 GMT   (200kb)

Title: RePaViT: Scalable Vision Transformer Acceleration via Structural
  Reparameterization on Feedforward Network Layers
Authors: Xuwei Xu, Yang Li, Yudong Chen, Jiajun Liu, Sen Wang
Categories: cs.CV cs.AI
Comments: Accepted to ICML2025
\\
  We reveal that feedforward network (FFN) layers, rather than attention
layers, are the primary contributors to Vision Transformer (ViT) inference
latency, with their impact signifying as model size increases. This finding
highlights a critical opportunity for optimizing the efficiency of large-scale
ViTs by focusing on FFN layers. In this work, we propose a novel channel idle
mechanism that facilitates post-training structural reparameterization for
efficient FFN layers during testing. Specifically, a set of feature channels
remains idle and bypasses the nonlinear activation function in each FFN layer,
thereby forming a linear pathway that enables structural reparameterization
during inference. This mechanism results in a family of ReParameterizable
Vision Transformers (RePaViTs), which achieve remarkable latency reductions
with acceptable sacrifices (sometimes gains) in accuracy across various ViTs.
The benefits of our method scale consistently with model sizes, demonstrating
greater speed improvements and progressively narrowing accuracy gaps or even
higher accuracies on larger models. In particular, RePa-ViT-Large and
RePa-ViT-Huge enjoy 66.8% and 68.7% speed-ups with +1.7% and +1.1% higher top-1
accuracies under the same training strategy, respectively. RePaViT is the first
to employ structural reparameterization on FFN layers to expedite ViTs to our
best knowledge, and we believe that it represents an auspicious direction for
efficient ViTs. Source code is available at
https://github.com/Ackesnal/RePaViT.
\\ ( https://arxiv.org/abs/2505.21847 ,  200kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21849 (*cross-listing*)
Date: Wed, 28 May 2025 00:30:22 GMT   (5813kb,D)

Title: Xinyu AI Search: Enhanced Relevance and Comprehensive Results with Rich
  Answer Presentations
Authors: Bo Tang, Junyi Zhu, Chenyang Xi, Yunhang Ge, Jiahao Wu, Yuchen Feng,
  Yijun Niu, Wenqiang Wei, Yu Yu, Chunyu Li, Zehao Lin, Hao Wu, Ning Liao,
  Yebin Yang, Jiajia Wang, Zhiyu Li, Feiyu Xiong, Jingrun Chen
Categories: cs.IR cs.AI
\\
  Traditional search engines struggle to synthesize fragmented information for
complex queries, while generative AI search engines face challenges in
relevance, comprehensiveness, and presentation. To address these limitations,
we introduce Xinyu AI Search, a novel system that incorporates a
query-decomposition graph to dynamically break down complex queries into
sub-queries, enabling stepwise retrieval and generation. Our retrieval pipeline
enhances diversity through multi-source aggregation and query expansion, while
filtering and re-ranking strategies optimize passage relevance. Additionally,
Xinyu AI Search introduces a novel approach for fine-grained, precise built-in
citation and innovates in result presentation by integrating timeline
visualization and textual-visual choreography. Evaluated on recent real-world
queries, Xinyu AI Search outperforms eight existing technologies in human
assessments, excelling in relevance, comprehensiveness, and insightfulness.
Ablation studies validate the necessity of its key sub-modules. Our work
presents the first comprehensive framework for generative AI search engines,
bridging retrieval, generation, and user-centric presentation.
\\ ( https://arxiv.org/abs/2505.21849 ,  5813kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21850 (*cross-listing*)
Date: Wed, 28 May 2025 00:34:45 GMT   (3518kb)

Title: Beyond Perception: Evaluating Abstract Visual Reasoning through
  Multi-Stage Task
Authors: Yanbei Jiang, Yihao Ding, Chao Lei, Jiayang Ao, Jey Han Lau, Krista A.
  Ehinger
Categories: cs.CV cs.AI
Comments: Accepted at ACL Findings
\\
  Current Multimodal Large Language Models (MLLMs) excel in general visual
reasoning but remain underexplored in Abstract Visual Reasoning (AVR), which
demands higher-order reasoning to identify abstract rules beyond simple
perception. Existing AVR benchmarks focus on single-step reasoning, emphasizing
the end result but neglecting the multi-stage nature of reasoning process. Past
studies found MLLMs struggle with these benchmarks, but it doesn't explain how
they fail. To address this gap, we introduce MultiStAR, a Multi-Stage AVR
benchmark, based on RAVEN, designed to assess reasoning across varying levels
of complexity. Additionally, existing metrics like accuracy only focus on the
final outcomes while do not account for the correctness of intermediate steps.
Therefore, we propose a novel metric, MSEval, which considers the correctness
of intermediate steps in addition to the final outcomes. We conduct
comprehensive experiments on MultiStAR using 17 representative close-source and
open-source MLLMs. The results reveal that while existing MLLMs perform
adequately on basic perception tasks, they continue to face challenges in more
complex rule detection stages.
\\ ( https://arxiv.org/abs/2505.21850 ,  3518kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21854 (*cross-listing*)
Date: Wed, 28 May 2025 00:55:36 GMT   (42461kb)

Title: Rethinking Gradient-based Adversarial Attacks on Point Cloud
  Classification
Authors: Jun Chen, Xinke Li, Mingyue Xu, Tianrui Li, Chongshou Li
Categories: cs.CV cs.AI
\\
  Gradient-based adversarial attacks have become a dominant approach for
evaluating the robustness of point cloud classification models. However,
existing methods often rely on uniform update rules that fail to consider the
heterogeneous nature of point clouds, resulting in excessive and perceptible
perturbations. In this paper, we rethink the design of gradient-based attacks
by analyzing the limitations of conventional gradient update mechanisms and
propose two new strategies to improve both attack effectiveness and
imperceptibility. First, we introduce WAAttack, a novel framework that
incorporates weighted gradients and an adaptive step-size strategy to account
for the non-uniform contribution of points during optimization. This approach
enables more targeted and subtle perturbations by dynamically adjusting updates
according to the local structure and sensitivity of each point. Second, we
propose SubAttack, a complementary strategy that decomposes the point cloud
into subsets and focuses perturbation efforts on structurally critical regions.
Together, these methods represent a principled rethinking of gradient-based
adversarial attacks for 3D point cloud classification. Extensive experiments
demonstrate that our approach outperforms state-of-the-art baselines in
generating highly imperceptible adversarial examples. Code will be released
upon paper acceptance.
\\ ( https://arxiv.org/abs/2505.21854 ,  42461kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21855 (*cross-listing*)
Date: Wed, 28 May 2025 01:00:32 GMT   (560kb,D)

Title: Extracting Research Instruments from Educational Literature Using LLMs
Authors: Jiseung Yoo, Curran Mahowald, Meiyu Li, Wei Ai
Categories: cs.IR cs.AI
\\
  Large Language Models (LLMs) are transforming information extraction from
academic literature, offering new possibilities for knowledge management. This
study presents an LLM-based system designed to extract detailed information
about research instruments used in the education field, including their names,
types, target respondents, measured constructs, and outcomes. Using multi-step
prompting and a domain-specific data schema, it generates structured outputs
optimized for educational research. Our evaluation shows that this system
significantly outperforms other approaches, particularly in identifying
instrument names and detailed information. This demonstrates the potential of
LLM-powered information extraction in educational contexts, offering a
systematic way to organize research instrument information. The ability to
aggregate such information at scale enhances accessibility for researchers and
education leaders, facilitating informed decision-making in educational
research and policy.
\\ ( https://arxiv.org/abs/2505.21855 ,  560kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21866 (*cross-listing*)
Date: Wed, 28 May 2025 01:29:29 GMT   (1677kb)

Title: CSI-Bench: A Large-Scale In-the-Wild Dataset for Multi-task WiFi Sensing
Authors: Guozhen Zhu, Yuqian Hu, Weihang Gao, Wei-Hsiang Wang, Beibei Wang, K.
  J. Ray Liu
Categories: eess.SP cs.AI cs.DB
Comments: 21 pages, 4 figures
\\
  WiFi sensing has emerged as a compelling contactless modality for human
activity monitoring by capturing fine-grained variations in Channel State
Information (CSI). Its ability to operate continuously and non-intrusively
while preserving user privacy makes it particularly suitable for health
monitoring. However, existing WiFi sensing systems struggle to generalize in
real-world settings, largely due to datasets collected in controlled
environments with homogeneous hardware and fragmented, session-based recordings
that fail to reflect continuous daily activity.
  We present CSI-Bench, a large-scale, in-the-wild benchmark dataset collected
using commercial WiFi edge devices across 26 diverse indoor environments with
35 real users. Spanning over 461 hours of effective data, CSI-Bench captures
realistic signal variability under natural conditions. It includes
task-specific datasets for fall detection, breathing monitoring, localization,
and motion source recognition, as well as a co-labeled multitask dataset with
joint annotations for user identity, activity, and proximity. To support the
development of robust and generalizable models, CSI-Bench provides standardized
evaluation splits and baseline results for both single-task and multi-task
learning. CSI-Bench offers a foundation for scalable, privacy-preserving WiFi
sensing systems in health and broader human-centric applications.
\\ ( https://arxiv.org/abs/2505.21866 ,  1677kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21870 (*cross-listing*)
Date: Wed, 28 May 2025 01:34:31 GMT   (658kb,D)

Title: Evaluating the Retrieval Robustness of Large Language Models
Authors: Shuyang Cao, Karthik Radhakrishnan, David Rosenberg, Steven Lu,
  Pengxiang Cheng, Lu Wang, Shiyue Zhang
Categories: cs.CL cs.AI
Comments: 19 pages
\\
  Retrieval-augmented generation (RAG) generally enhances large language
models' (LLMs) ability to solve knowledge-intensive tasks. But RAG may also
lead to performance degradation due to imperfect retrieval and the model's
limited ability to leverage retrieved content. In this work, we evaluate the
robustness of LLMs in practical RAG setups (henceforth retrieval robustness).
We focus on three research questions: (1) whether RAG is always better than
non-RAG; (2) whether more retrieved documents always lead to better
performance; (3) and whether document orders impact results. To facilitate this
study, we establish a benchmark of 1500 open-domain questions, each with
retrieved documents from Wikipedia. We introduce three robustness metrics, each
corresponds to one research question. Our comprehensive experiments, involving
11 LLMs and 3 prompting strategies, reveal that all of these LLMs exhibit
surprisingly high retrieval robustness; nonetheless, different degrees of
imperfect robustness hinders them from fully utilizing the benefits of RAG.
\\ ( https://arxiv.org/abs/2505.21870 ,  658kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21873 (*cross-listing*)
Date: Wed, 28 May 2025 01:39:31 GMT   (26357kb)

Title: HelixDesign-Binder: A Scalable Production-Grade Platform for Binder
  Design Built on HelixFold3
Authors: Jie Gao, Jun Li, Jing Hu, Shanzhuo Zhang, Kunrui Zhu, Yueyang Huang,
  Xiaonan Zhang, Xiaomin Fang
Categories: q-bio.BM cs.AI q-bio.QM
\\
  Protein binder design is central to therapeutics, diagnostics, and synthetic
biology, yet practical deployment remains challenging due to fragmented
workflows, high computational costs, and complex tool integration. We present
HelixDesign-Binder, a production-grade, high-throughput platform built on
HelixFold3 that automates the full binder design pipeline, from backbone
generation and sequence design to structural evaluation and multi-dimensional
scoring. By unifying these stages into a scalable and user-friendly system,
HelixDesign-Binder enables efficient exploration of binder candidates with
favorable structural, energetic, and physicochemical properties. The platform
leverages Baidu Cloud's high-performance infrastructure to support large-scale
design and incorporates advanced scoring metrics, including ipTM, predicted
binding free energy, and interface hydrophobicity. Benchmarking across six
protein targets demonstrates that HelixDesign-Binder reliably produces diverse
and high-quality binders, some of which match or exceed validated designs in
predicted binding affinity. HelixDesign-Binder is accessible via an interactive
web interface in PaddleHelix platform, supporting both academic research and
industrial applications in antibody and protein binder development.
\\ ( https://arxiv.org/abs/2505.21873 ,  26357kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21876 (*cross-listing*)
Date: Wed, 28 May 2025 01:45:26 GMT   (34139kb)

Title: EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video
  Guidance
Authors: Zun Wang, Jaemin Cho, Jialu Li, Han Lin, Jaehong Yoon, Yue Zhang,
  Mohit Bansal
Categories: cs.CV cs.AI
Comments: Project website: https://zunwang1.github.io/Epic
\\
  Recent approaches on 3D camera control in video diffusion models (VDMs) often
create anchor videos to guide diffusion models as a structured prior by
rendering from estimated point clouds following annotated camera trajectories.
However, errors inherent in point cloud estimation often lead to inaccurate
anchor videos. Moreover, the requirement for extensive camera trajectory
annotations further increases resource demands. To address these limitations,
we introduce EPiC, an efficient and precise camera control learning framework
that automatically constructs high-quality anchor videos without expensive
camera trajectory annotations. Concretely, we create highly precise anchor
videos for training by masking source videos based on first-frame visibility.
This approach ensures high alignment, eliminates the need for camera trajectory
annotations, and thus can be readily applied to any in-the-wild video to
generate image-to-video (I2V) training pairs. Furthermore, we introduce
Anchor-ControlNet, a lightweight conditioning module that integrates anchor
video guidance in visible regions to pretrained VDMs, with less than 1% of
backbone model parameters. By combining the proposed anchor video data and
ControlNet module, EPiC achieves efficient training with substantially fewer
parameters, training steps, and less data, without requiring modifications to
the diffusion model backbone typically needed to mitigate rendering
misalignments. Although being trained on masking-based anchor videos, our
method generalizes robustly to anchor videos made with point clouds during
inference, enabling precise 3D-informed camera control. EPiC achieves SOTA
performance on RealEstate10K and MiraData for I2V camera control task,
demonstrating precise and robust camera control ability both quantitatively and
qualitatively. Notably, EPiC also exhibits strong zero-shot generalization to
video-to-video scenarios.
\\ ( https://arxiv.org/abs/2505.21876 ,  34139kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21879 (*cross-listing*)
Date: Wed, 28 May 2025 01:53:29 GMT   (21446kb)

Title: Symbolic Foundation Regressor on Complex Networks
Authors: Weiting Liu and Jiaxu Cui and Jiao Hu and En Wang and Bo Yang
Categories: cs.SC cs.AI cs.LG
Comments: 60 pages
\\
  In science, we are interested not only in forecasting but also in
understanding how predictions are made, specifically what the interpretable
underlying model looks like. Data-driven machine learning technology can
significantly streamline the complex and time-consuming traditional manual
process of discovering scientific laws, helping us gain insights into
fundamental issues in modern science. In this work, we introduce a pre-trained
symbolic foundation regressor that can effectively compress complex data with
numerous interacting variables while producing interpretable physical
representations. Our model has been rigorously tested on non-network symbolic
regression, symbolic regression on complex networks, and the inference of
network dynamics across various domains, including physics, biochemistry,
ecology, and epidemiology. The results indicate a remarkable improvement in
equation inference efficiency, being three times more effective than baseline
approaches while maintaining accurate predictions. Furthermore, we apply our
model to uncover more intuitive laws of interaction transmission from global
epidemic outbreak data, achieving optimal data fitting. This model extends the
application boundary of pre-trained symbolic regression models to complex
networks, and we believe it provides a foundational solution for revealing the
hidden mechanisms behind changes in complex phenomena, enhancing
interpretability, and inspiring further scientific discoveries.
\\ ( https://arxiv.org/abs/2505.21879 ,  21446kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21898 (*cross-listing*)
Date: Wed, 28 May 2025 02:23:53 GMT   (1679kb)

Title: Co-Saving: Resource Aware Multi-Agent Collaboration for Software
  Development
Authors: Rennai Qiu, Chen Qian, Ran Li, Yufan Dang, Weize Chen, Cheng Yang,
  Yingli Zhang, Ye Tian, Xuantang Xiong, Lei Han, Zhiyuan Liu, Maosong Sun
Categories: cs.CL cs.AI cs.MA cs.SE
Comments: Work in Progress
\\
  Recent advancements in Large Language Models (LLMs) and autonomous agents
have demonstrated remarkable capabilities across various domains. However,
standalone agents frequently encounter limitations when handling complex tasks
that demand extensive interactions and substantial computational resources.
Although Multi-Agent Systems (MAS) alleviate some of these limitations through
collaborative mechanisms like task decomposition, iterative communication, and
role specialization, they typically remain resource-unaware, incurring
significant inefficiencies due to high token consumption and excessive
execution time. To address these limitations, we propose a resource-aware
multi-agent system -- Co-Saving (meaning that multiple agents collaboratively
engage in resource-saving activities), which leverages experiential knowledge
to enhance operational efficiency and solution quality. Our key innovation is
the introduction of "shortcuts" -- instructional transitions learned from
historically successful trajectories -- which allows to bypass redundant
reasoning agents and expedite the collective problem-solving process.
Experiments for software development tasks demonstrate significant advantages
over existing methods. Specifically, compared to the state-of-the-art MAS
ChatDev, our method achieves an average reduction of 50.85% in token usage, and
improves the overall code quality by 10.06%.
\\ ( https://arxiv.org/abs/2505.21898 ,  1679kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21904 (*cross-listing*)
Date: Wed, 28 May 2025 02:45:42 GMT   (23918kb,D)

Title: CAST: Contrastive Adaptation and Distillation for Semi-Supervised
  Instance Segmentation
Authors: Pardis Taghavi, Tian Liu, Renjie Li, Reza Langari, Zhengzhong Tu
Categories: cs.CV cs.AI
\\
  Instance segmentation demands costly per-pixel annotations and large models.
We introduce CAST, a semi-supervised knowledge distillation (SSKD) framework
that compresses pretrained vision foundation models (VFM) into compact experts
using limited labeled and abundant unlabeled data. CAST unfolds in three
stages: (1) domain adaptation of the VFM teacher(s) via self-training with
contrastive pixel calibration, (2) distillation into a compact student via a
unified multi-objective loss that couples standard supervision and
pseudo-labels with our instance-aware pixel-wise contrastive term, and (3)
fine-tuning on labeled data to remove residual pseudo-label bias. Central to
CAST is an \emph{instance-aware pixel-wise contrastive loss} that fuses mask
and class scores to mine informative negatives and enforce clear inter-instance
margins. By maintaining this contrastive signal across both adaptation and
distillation, we align teacher and student embeddings and fully leverage
unlabeled images. On Cityscapes and ADE20K, our ~11X smaller student surpasses
its adapted VFM teacher(s) by +3.4 AP (33.9 vs. 30.5) and +1.5 AP (16.7 vs.
15.2) and outperforms state-of-the-art semi-supervised approaches.
\\ ( https://arxiv.org/abs/2505.21904 ,  23918kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21919 (*cross-listing*)
Date: Wed, 28 May 2025 03:05:55 GMT   (243kb)

Title: Towards Efficient Key-Value Cache Management for Prefix Prefilling in
  LLM Inference
Authors: Yue Zhu, Hao Yu, Chen Wang, Zhuoran Liu, Eun Kyung Lee
Categories: cs.ET cs.AI cs.DC
Comments: This paper has been accepted at IEEE Cloud 2025 as WIP paper. The
  final version will appear in IEEE Xplore
\\
  The increasing adoption of large language models (LLMs) with extended context
windows necessitates efficient Key-Value Cache (KVC) management to optimize
inference performance. Inference workloads like Retrieval-Augmented Generation
(RAG) and agents exhibit high cache reusability, making efficient caching
critical to reducing redundancy and improving speed. We analyze real-world KVC
access patterns using publicly available traces and evaluate commercial
key-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1]
and Sherman [2]) for KVC metadata management. Our work demonstrates the lack of
tailored storage solution for KVC prefilling, underscores the need for an
efficient distributed caching system with optimized metadata management for LLM
workloads, and provides insights into designing improved KVC management systems
for scalable, low-latency inference.
\\ ( https://arxiv.org/abs/2505.21919 ,  243kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21926 (*cross-listing*)
Date: Wed, 28 May 2025 03:21:28 GMT   (281kb)

Title: Beyond Completion: A Foundation Model for General Knowledge Graph
  Reasoning
Authors: Yin Hua, Zhiqiang Liu, Mingyang Chen, Zheng Fang, Chi Man Wong,
  Lingxiao Li, Chi Man Vong, Huajun Chen, Wen Zhang
Categories: cs.CL cs.AI
Comments: ACL 2025 Findings
\\
  In natural language processing (NLP) and computer vision (CV), the successful
application of foundation models across diverse tasks has demonstrated their
remarkable potential. However, despite the rich structural and textual
information embedded in knowledge graphs (KGs), existing research of foundation
model for KG has primarily focused on their structural aspects, with most
efforts restricted to in-KG tasks (e.g., knowledge graph completion, KGC). This
limitation has hindered progress in addressing more challenging out-of-KG
tasks. In this paper, we introduce MERRY, a foundation model for general
knowledge graph reasoning, and investigate its performance across two task
categories: in-KG reasoning tasks (e.g., KGC) and out-of-KG tasks (e.g., KG
question answering, KGQA). We not only utilize the structural information, but
also the textual information in KGs. Specifically, we propose a
multi-perspective Conditional Message Passing (CMP) encoding architecture to
bridge the gap between textual and structural modalities, enabling their
seamless integration. Additionally, we introduce a dynamic residual fusion
module to selectively retain relevant textual information and a flexible edge
scoring mechanism to adapt to diverse downstream tasks. Comprehensive
evaluations on 28 datasets demonstrate that MERRY outperforms existing
baselines in most scenarios, showcasing strong reasoning capabilities within
KGs and excellent generalization to out-of-KG tasks such as KGQA.
\\ ( https://arxiv.org/abs/2505.21926 ,  281kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21928 (*cross-listing*)
Date: Wed, 28 May 2025 03:22:08 GMT   (4548kb)

Title: Subspecialty-Specific Foundation Model for Intelligent Gastrointestinal
  Pathology
Authors: Lianghui Zhu, Xitong Ling, Minxi Ouyang, Xiaoping Liu, Mingxi Fu, Tian
  Guan, Fanglei Fu, Xuanyu Wang, Maomao Zeng, Mingxi Zhu, Yibo Jin, Liming Liu,
  Song Duan, Qiming He, Yizhi Wang, Luxi Xie, Houqiang Li, Yonghong He, Sufang
  Tian
Categories: eess.IV cs.AI cs.CV cs.LG
\\
  Gastrointestinal (GI) diseases represent a clinically significant burden,
necessitating precise diagnostic approaches to optimize patient outcomes.
Conventional histopathological diagnosis, heavily reliant on the subjective
interpretation of pathologists, suffers from limited reproducibility and
diagnostic variability. To overcome these limitations and address the lack of
pathology-specific foundation models for GI diseases, we develop Digepath, a
specialized foundation model for GI pathology. Our framework introduces a
dual-phase iterative optimization strategy combining pretraining with
fine-screening, specifically designed to address the detection of sparsely
distributed lesion areas in whole-slide images. Digepath is pretrained on more
than 353 million image patches from over 200,000 hematoxylin and eosin-stained
slides of GI diseases. It attains state-of-the-art performance on 33 out of 34
tasks related to GI pathology, including pathological diagnosis, molecular
prediction, gene mutation prediction, and prognosis evaluation, particularly in
diagnostically ambiguous cases and resolution-agnostic tissue classification.We
further translate the intelligent screening module for early GI cancer and
achieve near-perfect 99.6% sensitivity across 9 independent medical
institutions nationwide. The outstanding performance of Digepath highlights its
potential to bridge critical gaps in histopathological practice. This work not
only advances AI-driven precision pathology for GI diseases but also
establishes a transferable paradigm for other pathology subspecialties.
\\ ( https://arxiv.org/abs/2505.21928 ,  4548kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21954 (*cross-listing*)
Date: Wed, 28 May 2025 04:08:59 GMT   (13732kb)

Title: UniTalk: Towards Universal Active Speaker Detection in Real World
  Scenarios
Authors: Le Thien Phuc Nguyen, Zhuoran Yu, Khoa Quang Nhat Cao, Yuwei Guo, Tu
  Ho Manh Pham, Tuan Tai Nguyen, Toan Ngo Duc Vo, Lucas Poon, Soochahn Lee,
  Yong Jae Lee
Categories: cs.CV cs.AI
\\
  We present UniTalk, a novel dataset specifically designed for the task of
active speaker detection, emphasizing challenging scenarios to enhance model
generalization. Unlike previously established benchmarks such as AVA, which
predominantly features old movies and thus exhibits significant domain gaps,
UniTalk focuses explicitly on diverse and difficult real-world conditions.
These include underrepresented languages, noisy backgrounds, and crowded scenes
- such as multiple visible speakers speaking concurrently or in overlapping
turns. It contains over 44.5 hours of video with frame-level active speaker
annotations across 48,693 speaking identities, and spans a broad range of video
types that reflect real-world conditions. Through rigorous evaluation, we show
that state-of-the-art models, while achieving nearly perfect scores on AVA,
fail to reach saturation on UniTalk, suggesting that the ASD task remains far
from solved under realistic conditions. Nevertheless, models trained on UniTalk
demonstrate stronger generalization to modern "in-the-wild" datasets like
Talkies and ASW, as well as to AVA. UniTalk thus establishes a new benchmark
for active speaker detection, providing researchers with a valuable resource
for developing and evaluating versatile and resilient models.
  Dataset: https://huggingface.co/datasets/plnguyen2908/UniTalk-ASD
  Code: https://github.com/plnguyen2908/UniTalk-ASD-code
\\ ( https://arxiv.org/abs/2505.21954 ,  13732kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21955 (*cross-listing*)
Date: Wed, 28 May 2025 04:09:42 GMT   (8699kb)

Title: Towards Comprehensive Scene Understanding: Integrating First and
  Third-Person Views for LVLMs
Authors: Insu Lee, Wooje Park, Jaeyun Jang, Minyoung Noh, Kyuhong Shim,
  Byonghyo Shim
Categories: cs.CV cs.AI
\\
  Large vision-language models (LVLMs) are increasingly deployed in interactive
applications such as virtual and augmented reality, where first-person
(egocentric) view captured by head-mounted cameras serves as key input. While
this view offers fine-grained cues about user attention and hand-object
interactions, their narrow field of view and lack of global context often lead
to failures on spatially or contextually demanding queries. To address this, we
introduce a framework that augments egocentric inputs with third-person
(exocentric) views, providing complementary information such as global scene
layout and object visibility to LVLMs. We present E3VQA, the first benchmark
for multi-view question answering with 4K high-quality question-answer pairs
grounded in synchronized ego-exo image pairs. Additionally, we propose M3CoT, a
training-free prompting technique that constructs a unified scene
representation by integrating scene graphs from three complementary
perspectives. M3CoT enables LVLMs to reason more effectively across views,
yielding consistent performance gains (4.84% for GPT-4o and 5.94% for Gemini
2.0 Flash) over a recent CoT baseline. Our extensive evaluation reveals key
strengths and limitations of LVLMs in multi-view reasoning and highlights the
value of leveraging both egocentric and exocentric inputs.
\\ ( https://arxiv.org/abs/2505.21955 ,  8699kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21956 (*cross-listing*)
Date: Wed, 28 May 2025 04:09:49 GMT   (15622kb)

Title: Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image
  Generation
Authors: Mengdan Zhu, Senhao Cheng, Guangji Bai, Yifei Zhang, Liang Zhao
Categories: cs.CV cs.AI cs.CL cs.LG
\\
  Text-to-image generation increasingly demands access to domain-specific,
fine-grained, and rapidly evolving knowledge that pretrained models cannot
fully capture. Existing Retrieval-Augmented Generation (RAG) methods attempt to
address this by retrieving globally relevant images, but they fail when no
single image contains all desired elements from a complex user query. We
propose Cross-modal RAG, a novel framework that decomposes both queries and
images into sub-dimensional components, enabling subquery-aware retrieval and
generation. Our method introduces a hybrid retrieval strategy - combining a
sub-dimensional sparse retriever with a dense retriever - to identify a
Pareto-optimal set of images, each contributing complementary aspects of the
query. During generation, a multimodal large language model is guided to
selectively condition on relevant visual features aligned to specific
subqueries, ensuring subquery-aware image synthesis. Extensive experiments on
MS-COCO, Flickr30K, WikiArt, CUB, and ImageNet-LT demonstrate that Cross-modal
RAG significantly outperforms existing baselines in both retrieval and
generation quality, while maintaining high efficiency.
\\ ( https://arxiv.org/abs/2505.21956 ,  15622kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21963 (*cross-listing*)
Date: Wed, 28 May 2025 04:30:51 GMT   (961kb,D)

Title: LaMDAgent: An Autonomous Framework for Post-Training Pipeline
  Optimization via LLM Agents
Authors: Taro Yano, Yoichi Ishibashi, Masafumi Oyamada
Categories: cs.CL cs.AI
\\
  Large Language Models (LLMs) have demonstrated exceptional performance across
a wide range of tasks. To further tailor LLMs to specific domains or
applications, post-training techniques such as Supervised Fine-Tuning (SFT),
Preference Learning, and model merging are commonly employed. While each of
these methods has been extensively studied in isolation, the automated
construction of complete post-training pipelines remains an underexplored area.
Existing approaches typically rely on manual design or focus narrowly on
optimizing individual components, such as data ordering or merging strategies.
In this work, we introduce LaMDAgent (short for Language Model Developing
Agent), a novel framework that autonomously constructs and optimizes full
post-training pipelines through the use of LLM-based agents. LaMDAgent
systematically explores diverse model generation techniques, datasets, and
hyperparameter configurations, leveraging task-based feedback to discover
high-performing pipelines with minimal human intervention. Our experiments show
that LaMDAgent improves tool-use accuracy by 9.0 points while preserving
instruction-following capabilities. Moreover, it uncovers effective
post-training strategies that are often overlooked by conventional human-driven
exploration. We further analyze the impact of data and model size scaling to
reduce computational costs on the exploration, finding that model size scalings
introduces new challenges, whereas scaling data size enables cost-effective
pipeline discovery.
\\ ( https://arxiv.org/abs/2505.21963 ,  961kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21996 (*cross-listing*)
Date: Wed, 28 May 2025 05:55:44 GMT   (21897kb)

Title: Learning World Models for Interactive Video Generation
Authors: Taiye Chen, Xun Hu, Zihan Ding, Chi Jin
Categories: cs.CV cs.AI
\\
  Foundational world models must be both interactive and preserve
spatiotemporal coherence for effective future planning with action choices.
However, present models for long video generation have limited inherent world
modeling capabilities due to two main challenges: compounding errors and
insufficient memory mechanisms. We enhance image-to-video models with
interactive capabilities through additional action conditioning and
autoregressive framework, and reveal that compounding error is inherently
irreducible in autoregressive video generation, while insufficient memory
mechanism leads to incoherence of world models. We propose video retrieval
augmented generation (VRAG) with explicit global state conditioning, which
significantly reduces long-term compounding errors and increases spatiotemporal
consistency of world models. In contrast, naive autoregressive generation with
extended context windows and retrieval-augmented generation prove less
effective for video generation, primarily due to the limited in-context
learning capabilities of current video models. Our work illuminates the
fundamental challenges in video world models and establishes a comprehensive
benchmark for improving video generation models with internal world modeling
capabilities.
\\ ( https://arxiv.org/abs/2505.21996 ,  21897kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22003 (*cross-listing*)
Date: Wed, 28 May 2025 06:06:53 GMT   (398kb)

Title: Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal
  Assistance
Authors: Jatin Gupta, Akhil Sharma, Saransh Singhania, and Ali Imam Abidi
Categories: cs.CL cs.AI
Comments: 9 pages, 5 tables, 4 figures. This is a revised version of a preprint
  previously available at this URL: https://doi.org/10.21203/rs.3.rs-5351879/v1
\\
  Pursuit of accessible legal assistance in India faces a critical gap, as many
citizens struggle to leverage their legal rights due to limited awareness and
access to relevant legal information. This paper introduces Legal Assist AI, a
transformer-based model designed to bridge this gap by offering effective legal
assistance through large language models (LLMs). The system retrieves relevant
legal information from a curated database and generates accurate responses,
enabling effective assistance for diverse users, including legal professionals,
scholars, and the general public. The model was fine-tuned on extensive
datasets from the Indian legal domain, including Indian Constitution, Bharatiya
Nyaya Sanhita (BNS), Bharatiya Nagarik Suraksha Sanhita (BNSS) and so forth,
providing a robust understanding of the complexities of Indian law. By
incorporating domain-specific legal datasets, the proposed model demonstrated
remarkable efficiency and specialization in legal Question-Answering. The model
was evaluated against state-of-the-art models such as GPT-3.5 Turbo and Mistral
7B, achieving a 60.08% score on the AIBE, outperforming its competitors in
legal reasoning and accuracy. Unlike other models, Legal Assist AI avoided
common issues such as hallucinations, making it highly reliable for practical
legal applications. It showcases the model's applicability in real-world legal
scenarios, with future iterations aiming to enhance performance and expand its
dataset to cover a broader range of multilingual and case-specific queries as
well.
\\ ( https://arxiv.org/abs/2505.22003 ,  398kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22019 (*cross-listing*)
Date: Wed, 28 May 2025 06:30:51 GMT   (1005kb,D)

Title: VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich
  Information Understanding via Iterative Reasoning with Reinforcement Learning
Authors: Qiuchen Wang, Ruixue Ding, Yu Zeng, Zehui Chen, Lin Chen, Shihang
  Wang, Pengjun Xie, Fei Huang, Feng Zhao
Categories: cs.CL cs.AI cs.CV
\\
  Effectively retrieving, reasoning and understanding visually rich information
remains a challenge for RAG methods. Traditional text-based methods cannot
handle visual-related information. On the other hand, current vision-based RAG
approaches are often limited by fixed pipelines and frequently struggle to
reason effectively due to the insufficient activation of the fundamental
capabilities of models. As RL has been proven to be beneficial for model
reasoning, we introduce VRAG-RL, a novel RL framework tailored for complex
reasoning across visually rich information. With this framework, VLMs interact
with search engines, autonomously sampling single-turn or multi-turn reasoning
trajectories with the help of visual perception tokens and undergoing continual
optimization based on these samples. Our approach highlights key limitations of
RL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely
incorporate images into the context, leading to insufficient reasoning token
allocation and neglecting visual-specific perception; and (ii) When models
interact with search engines, their queries often fail to retrieve relevant
information due to the inability to articulate requirements, thereby leading to
suboptimal performance. To address these challenges, we define an action space
tailored for visually rich inputs, with actions including cropping and scaling,
allowing the model to gather information from a coarse-to-fine perspective.
Furthermore, to bridge the gap between users' original inquiries and the
retriever, we employ a simple yet effective reward that integrates query
rewriting and retrieval performance with a model-based reward. Our VRAG-RL
optimizes VLMs for RAG tasks using specially designed RL strategies, aligning
the model with real-world applications. The code is available at
\hyperlink{https://github.com/Alibaba-NLP/VRAG}{https://github.com/Alibaba-NLP/VRAG}.
\\ ( https://arxiv.org/abs/2505.22019 ,  1005kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22021 (*cross-listing*)
Date: Wed, 28 May 2025 06:37:06 GMT   (15818kb,D)

Title: GL-PGENet: A Parameterized Generation Framework for Robust Document
  Image Enhancement
Authors: Zhihong Tang, Yang Li
Categories: cs.CV cs.AI
Comments: 12 pages, 7 figures
\\
  Document Image Enhancement (DIE) serves as a critical component in Document
AI systems, where its performance substantially determines the effectiveness of
downstream tasks. To address the limitations of existing methods confined to
single-degradation restoration or grayscale image processing, we present Global
with Local Parametric Generation Enhancement Network (GL-PGENet), a novel
architecture designed for multi-degraded color document images, ensuring both
efficiency and robustness in real-world scenarios. Our solution incorporates
three key innovations: First, a hierarchical enhancement framework that
integrates global appearance correction with local refinement, enabling
coarse-to-fine quality improvement. Second, a Dual-Branch Local-Refine Network
with parametric generation mechanisms that replaces conventional direct
prediction, producing enhanced outputs through learned intermediate parametric
representations rather than pixel-wise mapping. This approach enhances local
consistency while improving model generalization. Finally, a modified NestUNet
architecture incorporating dense block to effectively fuse low-level pixel
features and high-level semantic features, specifically adapted for document
image characteristics. In addition, to enhance generalization performance, we
adopt a two-stage training strategy: large-scale pretraining on a synthetic
dataset of 500,000+ samples followed by task-specific fine-tuning. Extensive
experiments demonstrate the superiority of GL-PGENet, achieving
state-of-the-art SSIM scores of 0.7721 on DocUNet and 0.9480 on RealDAE. The
model also exhibits remarkable cross-domain adaptability and maintains
computational efficiency for high-resolution images without performance
degradation, confirming its practical utility in real-world scenarios.
\\ ( https://arxiv.org/abs/2505.22021 ,  15818kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22027 (*cross-listing*)
Date: Wed, 28 May 2025 06:49:18 GMT   (900kb,D)

Title: Improving Respiratory Sound Classification with Architecture-Agnostic
  Knowledge Distillation from Ensembles
Authors: Miika Toikkanen, June-Woo Kim
Categories: cs.SD cs.AI eess.AS
Comments: Accepted to Interspeech 2025
\\
  Respiratory sound datasets are limited in size and quality, making high
performance difficult to achieve. Ensemble models help but inevitably increase
compute cost at inference time. Soft label training distills knowledge
efficiently with extra cost only at training. In this study, we explore soft
labels for respiratory sound classification as an architecture-agnostic
approach to distill an ensemble of teacher models into a student model. We
examine different variations of our approach and find that even a single
teacher, identical to the student, considerably improves performance beyond its
own capability, with optimal gains achieved using only a few teachers. We
achieve the new state-of-the-art Score of 64.39 on ICHBI, surpassing the
previous best by 0.85 and improving average Scores across architectures by more
than 1.16. Our results highlight the effectiveness of knowledge distillation
with soft labels for respiratory sound classification, regardless of size or
architecture.
\\ ( https://arxiv.org/abs/2505.22027 ,  900kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22029 (*cross-listing*)
Date: Wed, 28 May 2025 06:52:10 GMT   (1619kb,D)

Title: Analysis and Evaluation of Synthetic Data Generation in Speech
  Dysfluency Detection
Authors: Jinming Zhang, Xuanru Zhou, Jiachen Lian, Shuhe Li, William Li, Zoe
  Ezzes, Rian Bogley, Lisa Wauters, Zachary Miller, Jet Vonk, Brittany Morin,
  Maria Gorno-Tempini, Gopala Anumanchipalli
Categories: eess.AS cs.AI cs.SD
Comments: Submitted to Interspeech 2025
\\
  Speech dysfluency detection is crucial for clinical diagnosis and language
assessment, but existing methods are limited by the scarcity of high-quality
annotated data. Although recent advances in TTS model have enabled synthetic
dysfluency generation, existing synthetic datasets suffer from unnatural
prosody and limited contextual diversity. To address these limitations, we
propose LLM-Dys -- the most comprehensive dysfluent speech corpus with
LLM-enhanced dysfluency simulation. This dataset captures 11 dysfluency
categories spanning both word and phoneme levels. Building upon this resource,
we improve an end-to-end dysfluency detection framework. Experimental
validation demonstrates state-of-the-art performance. All data, models, and
code are open-sourced at https://github.com/Berkeley-Speech-Group/LLM-Dys.
\\ ( https://arxiv.org/abs/2505.22029 ,  1619kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22038 (*cross-listing*)
Date: Wed, 28 May 2025 07:00:50 GMT   (7586kb)

Title: Balanced Token Pruning: Accelerating Vision Language Models Beyond Local
  Optimization
Authors: Kaiyuan Li and Xiaoyue Chen and Chen Gao and Yong Li and Xinlei Chen
Categories: cs.CV cs.AI
\\
  Large Vision-Language Models (LVLMs) have shown impressive performance across
multi-modal tasks by encoding images into thousands of tokens. However, the
large number of image tokens results in significant computational overhead, and
the use of dynamic high-resolution inputs further increases this burden.
Previous approaches have attempted to reduce the number of image tokens through
token pruning, typically by selecting tokens based on attention scores or image
token diversity. Through empirical studies, we observe that existing methods
often overlook the joint impact of pruning on both the current layer's output
(local) and the outputs of subsequent layers (global), leading to suboptimal
pruning decisions. To address this challenge, we propose Balanced Token Pruning
(BTP), a plug-and-play method for pruning vision tokens. Specifically, our
method utilizes a small calibration set to divide the pruning process into
multiple stages. In the early stages, our method emphasizes the impact of
pruning on subsequent layers, whereas in the deeper stages, the focus shifts
toward preserving the consistency of local outputs. Extensive experiments
across various LVLMs demonstrate the broad effectiveness of our approach on
multiple benchmarks. Our method achieves a 78% compression rate while
preserving 96.7% of the original models' performance on average.
\\ ( https://arxiv.org/abs/2505.22038 ,  7586kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22067 (*cross-listing*)
Date: Wed, 28 May 2025 07:46:19 GMT   (25229kb)

Title: From Failures to Fixes: LLM-Driven Scenario Repair for Self-Evolving
  Autonomous Driving
Authors: Xinyu Xia, Xingjun Ma, Yunfeng Hu, Ting Qu, Hong Chen, Xun Gong
Categories: cs.CV cs.AI cs.RO
\\
  Ensuring robust and generalizable autonomous driving requires not only broad
scenario coverage but also efficient repair of failure cases, particularly
those related to challenging and safety-critical scenarios. However, existing
scenario generation and selection methods often lack adaptivity and semantic
relevance, limiting their impact on performance improvement. In this paper, we
propose \textbf{SERA}, an LLM-powered framework that enables autonomous driving
systems to self-evolve by repairing failure cases through targeted scenario
recommendation. By analyzing performance logs, SERA identifies failure patterns
and dynamically retrieves semantically aligned scenarios from a structured
bank. An LLM-based reflection mechanism further refines these recommendations
to maximize relevance and diversity. The selected scenarios are used for
few-shot fine-tuning, enabling targeted adaptation with minimal data.
Experiments on the benchmark show that SERA consistently improves key metrics
across multiple autonomous driving baselines, demonstrating its effectiveness
and generalizability under safety-critical conditions.
\\ ( https://arxiv.org/abs/2505.22067 ,  25229kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22068 (*cross-listing*)
Date: Wed, 28 May 2025 07:47:46 GMT   (2193kb)

Title: Beyond path selection: Better LLMs for Scientific Information Extraction
  with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO
Authors: Ran Li, Shimin Di, Yuchen Liu, Chen Jing, Yu Qiu, Lei Chen
Categories: cs.CL cs.AI cs.IR
\\
  Previous study suggest that powerful Large Language Models (LLMs) trained
with Reinforcement Learning with Verifiable Rewards (RLVR) only refines
reasoning path without improving the reasoning capacity in math tasks while
supervised-finetuning(SFT) with distillation can. We study this from the view
of Scientific information extraction (SciIE) where LLMs and reasoning LLMs
underperforms small Bert-based models. SciIE require both the reasoning and
memorization. We argue that both SFT and RLVR can refine the reasoning path and
improve reasoning capacity in a simple way based on SciIE. We propose two-stage
training with 1. MimicSFT, using structured reasoning templates without needing
high-quality chain-of-thought data, 2. R$^2$GRPO with relevance and
rule-induced rewards. Experiments on scientific IE benchmarks show that both
methods can improve the reasoning capacity. R$^2$GRPO with mimicSFT surpasses
baseline LLMs and specialized supervised models in relation extraction. Our
code is available at https://github.com/ranlislz/R2GRPO.
\\ ( https://arxiv.org/abs/2505.22068 ,  2193kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22086 (*cross-listing*)
Date: Wed, 28 May 2025 08:08:57 GMT   (2629kb)

Title: iDSE: Navigating Design Space Exploration in High-Level Synthesis Using
  LLMs
Authors: Runkai Li, Jia Xiong, Xi Wang
Categories: cs.AR cs.AI
\\
  High-Level Synthesis (HLS) serves as an agile hardware development tool that
streamlines the circuit design by abstracting the register transfer level into
behavioral descriptions, while allowing designers to customize the generated
microarchitectures through optimization directives. However, the combinatorial
explosion of possible directive configurations yields an intractable design
space. Traditional design space exploration (DSE) methods, despite adopting
heuristics or constructing predictive models to accelerate Pareto-optimal
design acquisition, still suffer from prohibitive exploration costs and
suboptimal results. Addressing these concerns, we introduce iDSE, the first
LLM-aided DSE framework that leverages HLS design quality perception to
effectively navigate the design space. iDSE intelligently pruns the design
space to guide LLMs in calibrating representative initial sampling designs,
expediting convergence toward the Pareto front. By exploiting the convergent
and divergent thinking patterns inherent in LLMs for hardware optimization,
iDSE achieves multi-path refinement of the design quality and diversity.
Extensive experiments demonstrate that iDSE outperforms heuristic-based DSE
methods by 5.1$\times$$\sim$16.6$\times$ in proximity to the reference Pareto
front, matching NSGA-II with only 4.6% of the explored designs. Our work
demonstrates the transformative potential of LLMs in scalable and efficient HLS
design optimization, offering new insights into multiobjective optimization
challenges.
\\ ( https://arxiv.org/abs/2505.22086 ,  2629kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22096 (*cross-listing*)
Date: Wed, 28 May 2025 08:17:58 GMT   (260kb)

Title: Knowledge Base Construction for Knowledge-Augmented Text-to-SQL
Authors: Jinheon Baek, Horst Samulowitz, Oktie Hassanzadeh, Dharmashankar
  Subramanian, Sola Shirai, Alfio Gliozzo, and Debarun Bhattacharjya
Categories: cs.CL cs.AI cs.LG
Comments: ACL Findings 2025
\\
  Text-to-SQL aims to translate natural language queries into SQL statements,
which is practical as it enables anyone to easily retrieve the desired
information from databases. Recently, many existing approaches tackle this
problem with Large Language Models (LLMs), leveraging their strong capability
in understanding user queries and generating corresponding SQL code. Yet, the
parametric knowledge in LLMs might be limited to covering all the diverse and
domain-specific queries that require grounding in various database schemas,
which makes generated SQLs less accurate oftentimes. To tackle this, we propose
constructing the knowledge base for text-to-SQL, a foundational source of
knowledge, from which we retrieve and generate the necessary knowledge for
given queries. In particular, unlike existing approaches that either manually
annotate knowledge or generate only a few pieces of knowledge for each query,
our knowledge base is comprehensive, which is constructed based on a
combination of all the available questions and their associated database
schemas along with their relevant knowledge, and can be reused for unseen
databases from different datasets and domains. We validate our approach on
multiple text-to-SQL datasets, considering both the overlapping and
non-overlapping database scenarios, where it outperforms relevant baselines
substantially.
\\ ( https://arxiv.org/abs/2505.22096 ,  260kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22106 (*cross-listing*)
Date: Wed, 28 May 2025 08:33:58 GMT   (222kb)

Title: AudioTurbo: Fast Text-to-Audio Generation with Rectified Diffusion
Authors: Junqi Zhao, Jinzheng Zhao, Haohe Liu, Yun Chen, Lu Han, Xubo Liu, Mark
  Plumbley, Wenwu Wang
Categories: cs.SD cs.AI eess.AS
\\
  Diffusion models have significantly improved the quality and diversity of
audio generation but are hindered by slow inference speed. Rectified flow
enhances inference speed by learning straight-line ordinary differential
equation (ODE) paths. However, this approach requires training a flow-matching
model from scratch and tends to perform suboptimally, or even poorly, at low
step counts. To address the limitations of rectified flow while leveraging the
advantages of advanced pre-trained diffusion models, this study integrates
pre-trained models with the rectified diffusion method to improve the
efficiency of text-to-audio (TTA) generation. Specifically, we propose
AudioTurbo, which learns first-order ODE paths from deterministic noise sample
pairs generated by a pre-trained TTA model. Experiments on the AudioCaps
dataset demonstrate that our model, with only 10 sampling steps, outperforms
prior models and reduces inference to 3 steps compared to a flow-matching-based
acceleration model.
\\ ( https://arxiv.org/abs/2505.22106 ,  222kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22116 (*cross-listing*)
Date: Wed, 28 May 2025 08:44:55 GMT   (3521kb,D)

Title: Multimodal Forecasting of Sparse Intraoperative Hypotension Events
  Powered by Language Model
Authors: Jintao Zhang and Zirui Liu and Mingyue Cheng and Shilong Zhang and
  Tingyue Pan and Qi Liu and Yanhu Xie
Categories: cs.CL cs.AI
\\
  Intraoperative hypotension (IOH) frequently occurs under general anesthesia
and is strongly linked to adverse outcomes such as myocardial injury and
increased mortality. Despite its significance, IOH prediction is hindered by
event sparsity and the challenge of integrating static and dynamic data across
diverse patients. In this paper, we propose \textbf{IOHFuseLM}, a multimodal
language model framework. To accurately identify and differentiate sparse
hypotensive events, we leverage a two-stage training strategy. The first stage
involves domain adaptive pretraining on IOH physiological time series augmented
through diffusion methods, thereby enhancing the model sensitivity to patterns
associated with hypotension. Subsequently, task fine-tuning is performed on the
original clinical dataset to further enhance the ability to distinguish
normotensive from hypotensive states. To enable multimodal fusion for each
patient, we align structured clinical descriptions with the corresponding
physiological time series at the token level. Such alignment enables the model
to capture individualized temporal patterns alongside their corresponding
clinical semantics. In addition, we convert static patient attributes into
structured text to enrich personalized information. Experimental evaluations on
two intraoperative datasets demonstrate that IOHFuseLM outperforms established
baselines in accurately identifying IOH events, highlighting its applicability
in clinical decision support scenarios. Our code is publicly available to
promote reproducibility at https://github.com/zjt-gpu/IOHFuseLM.
\\ ( https://arxiv.org/abs/2505.22116 ,  3521kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22126 (*cross-listing*)
Date: Wed, 28 May 2025 08:51:01 GMT   (12541kb)

Title: SridBench: Benchmark of Scientific Research Illustration Drawing of
  Image Generation Model
Authors: Yifan Chang, Yukang Feng, Jianwen Sun, Jiaxin Ai, Chuanhao Li, S.
  Kevin Zhou, Kaipeng Zhang
Categories: cs.CV cs.AI
\\
  Recent years have seen rapid advances in AI-driven image generation. Early
diffusion models emphasized perceptual quality, while newer multimodal models
like GPT-4o-image integrate high-level reasoning, improving semantic
understanding and structural composition. Scientific illustration generation
exemplifies this evolution: unlike general image synthesis, it demands accurate
interpretation of technical content and transformation of abstract ideas into
clear, standardized visuals. This task is significantly more
knowledge-intensive and laborious, often requiring hours of manual work and
specialized tools. Automating it in a controllable, intelligent manner would
provide substantial practical value. Yet, no benchmark currently exists to
evaluate AI on this front. To fill this gap, we introduce SridBench, the first
benchmark for scientific figure generation. It comprises 1,120 instances
curated from leading scientific papers across 13 natural and computer science
disciplines, collected via human experts and MLLMs. Each sample is evaluated
along six dimensions, including semantic fidelity and structural accuracy.
Experimental results reveal that even top-tier models like GPT-4o-image lag
behind human performance, with common issues in text/visual clarity and
scientific correctness. These findings highlight the need for more advanced
reasoning-driven visual generation capabilities.
\\ ( https://arxiv.org/abs/2505.22126 ,  12541kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22128 (*cross-listing*)
Date: Wed, 28 May 2025 08:52:38 GMT   (3674kb)

Title: Real-Time Blind Defocus Deblurring for Earth Observation: The IMAGIN-e
  Mission Approach
Authors: Alejandro D. Mousist
Categories: cs.CV cs.AI
\\
  This work addresses mechanical defocus in Earth observation images from the
IMAGIN-e mission aboard the ISS, proposing a blind deblurring approach adapted
to space-based edge computing constraints. Leveraging Sentinel-2 data, our
method estimates the defocus kernel and trains a restoration model within a GAN
framework, effectively operating without reference images.
  On Sentinel-2 images with synthetic degradation, SSIM improved by 72.47% and
PSNR by 25.00%, confirming the model's ability to recover lost details when the
original clean image is known. On IMAGIN-e, where no reference images exist,
perceptual quality metrics indicate a substantial enhancement, with NIQE
improving by 60.66% and BRISQUE by 48.38%, validating real-world onboard
restoration. The approach is currently deployed aboard the IMAGIN-e mission,
demonstrating its practical application in an operational space environment.
  By efficiently handling high-resolution images under edge computing
constraints, the method enables applications such as water body segmentation
and contour detection while maintaining processing viability despite resource
limitations.
\\ ( https://arxiv.org/abs/2505.22128 ,  3674kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22137 (*cross-listing*)
Date: Wed, 28 May 2025 09:00:56 GMT   (9426kb)

Title: Limited Generalizability in Argument Mining: State-Of-The-Art Models
  Learn Datasets, Not Arguments
Authors: Marc Feger, Katarina Boland and Stefan Dietze
Categories: cs.CL cs.AI cs.LG
Comments: This paper has been accepted to ACL 2025 and will be published after
  27.07.2025
\\
  Identifying arguments is a necessary prerequisite for various tasks in
automated discourse analysis, particularly within contexts such as political
debates, online discussions, and scientific reasoning. In addition to
theoretical advances in understanding the constitution of arguments, a
significant body of research has emerged around practical argument mining,
supported by a growing number of publicly available datasets. On these
benchmarks, BERT-like transformers have consistently performed best,
reinforcing the belief that such models are broadly applicable across diverse
contexts of debate. This study offers the first large-scale re-evaluation of
such state-of-the-art models, with a specific focus on their ability to
generalize in identifying arguments. We evaluate four transformers, three
standard and one enhanced with contrastive pre-training for better
generalization, on 17 English sentence-level datasets as most relevant to the
task. Our findings show that, to varying degrees, these models tend to rely on
lexical shortcuts tied to content words, suggesting that apparent progress may
often be driven by dataset-specific cues rather than true task alignment. While
the models achieve strong results on familiar benchmarks, their performance
drops markedly when applied to unseen datasets. Nonetheless, incorporating both
task-specific pre-training and joint benchmark training proves effective in
enhancing both robustness and generalization.
\\ ( https://arxiv.org/abs/2505.22137 ,  9426kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22141 (*cross-listing*)
Date: Wed, 28 May 2025 09:04:00 GMT   (33481kb,D)

Title: FaceEditTalker: Interactive Talking Head Generation with Facial
  Attribute Editing
Authors: Guanwen Feng, Zhiyuan Ma, Yunan Li, Junwei Jing, Jiahao Yang, Qiguang
  Miao
Categories: cs.CV cs.AI
\\
  Recent advances in audio-driven talking head generation have achieved
impressive results in lip synchronization and emotional expression. However,
they largely overlook the crucial task of facial attribute editing. This
capability is crucial for achieving deep personalization and expanding the
range of practical applications, including user-tailored digital avatars,
engaging online education content, and brand-specific digital customer service.
In these key domains, the flexible adjustment of visual attributes-such as
hairstyle, accessories, and subtle facial features is essential for aligning
with user preferences, reflecting diverse brand identities, and adapting to
varying contextual demands. In this paper, we present FaceEditTalker, a unified
framework that enables controllable facial attribute manipulation while
generating high-quality, audio-synchronized talking head videos. Our method
consists of two key components: an image feature space editing module, which
extracts semantic and detail features and allows flexible control over
attributes like expression, hairstyle, and accessories; and an audio-driven
video generation module, which fuses these edited features with audio-guided
facial landmarks to drive a diffusion-based generator. This design ensures
temporal coherence, visual fidelity, and identity preservation across frames.
Extensive experiments on public datasets demonstrate that our method
outperforms state-of-the-art approaches in lip-sync accuracy, video quality,
and attribute controllability. Project page:
https://peterfanfan.github.io/FaceEditTalker/
\\ ( https://arxiv.org/abs/2505.22141 ,  33481kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22146 (*cross-listing*)
Date: Wed, 28 May 2025 09:06:04 GMT   (46866kb,A)

Title: Flexible Tool Selection through Low-dimensional Attribute Alignment of
  Vision and Language
Authors: Guangfu Hao, Haojie Wen, Liangxuna Guo, Yang Chen, Yanchao Bi, Shan Yu
Categories: cs.CV cs.AI cs.CL q-bio.NC
\\
  Flexible tool selection reflects a complex cognitive ability that
distinguishes humans from other species, yet computational models that capture
this ability remain underdeveloped. We developed a framework using
low-dimensional attribute representations to bridge visual tool perception and
linguistic task understanding. We constructed a comprehensive dataset (ToolNet)
containing 115 common tools labeled with 13 carefully designed attributes
spanning physical, functional, and psychological properties, paired with
natural language scenarios describing tool usage. Visual encoders (ResNet or
ViT) extract attributes from tool images while fine-tuned language models
(GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our
approach achieves 74% accuracy in tool selection tasks-significantly
outperforming direct tool matching (20%) and smaller multimodal models
(21%-58%), while approaching performance of much larger models like GPT-4o
(73%) with substantially fewer parameters. Ablation studies revealed that
manipulation-related attributes (graspability, hand-relatedness, elongation)
consistently prove most critical across modalities. This work provides a
parameter-efficient, interpretable solution that mimics human-like tool
cognition, advancing both cognitive science understanding and practical
applications in tool selection tasks.
\\ ( https://arxiv.org/abs/2505.22146 ,  46866kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22165 (*cross-listing*)
Date: Wed, 28 May 2025 09:28:52 GMT   (268kb)

Title: Unifying Continuous and Discrete Text Diffusion with Non-simultaneous
  Diffusion Processes
Authors: Bocheng Li, Zhujin Gao, Linli Xu
Categories: cs.CL cs.AI cs.LG
Comments: Accepted to ACL 2025 Main Conference
\\
  Diffusion models have emerged as a promising approach for text generation,
with recent works falling into two main categories: discrete and continuous
diffusion models. Discrete diffusion models apply token corruption
independently using categorical distributions, allowing for different diffusion
progress across tokens but lacking fine-grained control. Continuous diffusion
models map tokens to continuous spaces and apply fine-grained noise, but the
diffusion progress is uniform across tokens, limiting their ability to capture
semantic nuances. To address these limitations, we propose
\textbf{\underline{N}}on-simultan\textbf{\underline{e}}ous
C\textbf{\underline{o}}ntinuous \textbf{\underline{Diff}}usion Models
(NeoDiff), a novel diffusion model that integrates the strengths of both
discrete and continuous approaches. NeoDiff introduces a Poisson diffusion
process for the forward process, enabling a flexible and fine-grained noising
paradigm, and employs a time predictor for the reverse process to adaptively
modulate the denoising progress based on token semantics. Furthermore, NeoDiff
utilizes an optimized schedule for inference to ensure more precise noise
control and improved performance. Our approach unifies the theories of discrete
and continuous diffusion models, offering a more principled and effective
framework for text generation. Experimental results on several text generation
tasks demonstrate NeoDiff's superior performance compared to baselines of
non-autoregressive continuous and discrete diffusion models, iterative-based
methods and autoregressive diffusion-based methods. These results highlight
NeoDiff's potential as a powerful tool for generating high-quality text and
advancing the field of diffusion-based text generation.
\\ ( https://arxiv.org/abs/2505.22165 ,  268kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22179 (*cross-listing*)
Date: Wed, 28 May 2025 09:55:08 GMT   (45kb)

Title: Speculative Decoding Meets Quantization: Compatibility Evaluation and
  Hierarchical Framework Design
Authors: Yudi Zhang, Weilin Zhao, Xu Han, Tiejun Zhao, Wang Xu, Hailong Cao,
  Conghui Zhu
Categories: cs.CL cs.AI cs.LG
Comments: 12 pages, 5 figures
\\
  Speculative decoding and quantization effectively accelerate memory-bound
inference of large language models. Speculative decoding mitigates the memory
bandwidth bottleneck by verifying multiple tokens within a single forward pass,
which increases computational effort. Quantization achieves this optimization
by compressing weights and activations into lower bit-widths and also reduces
computations via low-bit matrix multiplications. To further leverage their
strengths, we investigate the integration of these two techniques.
Surprisingly, experiments applying the advanced speculative decoding method
EAGLE-2 to various quantized models reveal that the memory benefits from 4-bit
weight quantization are diminished by the computational load from speculative
decoding. Specifically, verifying a tree-style draft incurs significantly more
time overhead than a single-token forward pass on 4-bit weight quantized
models. This finding led to our new speculative decoding design: a hierarchical
framework that employs a small model as an intermediate stage to turn
tree-style drafts into sequence drafts, leveraging the memory access benefits
of the target quantized model. Experimental results show that our hierarchical
approach achieves a 2.78$\times$ speedup across various tasks for the 4-bit
weight Llama-3-70B model on an A100 GPU, outperforming EAGLE-2 by 1.31$\times$.
Code available at https://github.com/AI9Stars/SpecMQuant.
\\ ( https://arxiv.org/abs/2505.22179 ,  45kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22184 (*cross-listing*)
Date: Wed, 28 May 2025 09:58:15 GMT   (464kb)

Title: Breaking the Cloak! Unveiling Chinese Cloaked Toxicity with Homophone
  Graph and Toxic Lexicon
Authors: Xuchen Ma, Jianxiang Yu, Wenming Shao, Bo Pang, Xiang Li
Categories: cs.CL cs.AI
Comments: 25 pages, 5 figures, 9 tables
\\
  Social media platforms have experienced a significant rise in toxic content,
including abusive language and discriminatory remarks, presenting growing
challenges for content moderation. Some users evade censorship by deliberately
disguising toxic words through homophonic cloak, which necessitates the task of
unveiling cloaked toxicity. Existing methods are mostly designed for English
texts, while Chinese cloaked toxicity unveiling has not been solved yet. To
tackle the issue, we propose C$^2$TU, a novel training-free and prompt-free
method for Chinese cloaked toxic content unveiling. It first employs substring
matching to identify candidate toxic words based on Chinese homo-graph and
toxic lexicon. Then it filters those candidates that are non-toxic and corrects
cloaks to be their corresponding toxicities. Specifically, we develop two model
variants for filtering, which are based on BERT and LLMs, respectively. For
LLMs, we address the auto-regressive limitation in computing word occurrence
probability and utilize the full semantic contexts of a text sequence to reveal
cloaked toxic words. Extensive experiments demonstrate that C$^2$TU can achieve
superior performance on two Chinese toxic datasets. In particular, our method
outperforms the best competitor by up to 71% on the F1 score and 35% on
accuracy, respectively.
\\ ( https://arxiv.org/abs/2505.22184 ,  464kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22193 (*cross-listing*)
Date: Wed, 28 May 2025 10:11:48 GMT   (4347kb)

Title: Physics-inspired Generative AI models via real hardware-based noisy
  quantum diffusion
Authors: Marco Parigi, Stefano Martina, Francesco Aldo Venturelli and Filippo
  Caruso
Categories: quant-ph cond-mat.dis-nn cs.AI cs.CV cs.LG
Comments: 17 pages, 9 figures. Supplementary materials: 2 pages, 2 figures
MSC-class: 81P68, 81P40, 81P47, 68Q12, 68T07,
ACM-class: I.2.6; I.3.3; J.2
\\
  Quantum Diffusion Models (QDMs) are an emerging paradigm in Generative AI
that aims to use quantum properties to improve the performances of their
classical counterparts. However, existing algorithms are not easily scalable
due to the limitations of near-term quantum devices. Following our previous
work on QDMs, here we propose and implement two physics-inspired protocols. In
the first, we use the formalism of quantum stochastic walks, showing that a
specific interplay of quantum and classical dynamics in the forward process
produces statistically more robust models generating sets of MNIST images with
lower Fr\'echet Inception Distance (FID) than using totally classical dynamics.
In the second approach, we realize an algorithm to generate images by
exploiting the intrinsic noise of real IBM quantum hardware with only four
qubits. Our work could be a starting point to pave the way for new scenarios
for large-scale algorithms in quantum Generative AI, where quantum noise is
neither mitigated nor corrected, but instead exploited as a useful resource.
\\ ( https://arxiv.org/abs/2505.22193 ,  4347kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22200 (*cross-listing*)
Date: Wed, 28 May 2025 10:25:43 GMT   (239kb)

Title: Investigating Mechanisms for In-Context Vision Language Binding
Authors: Darshana Saravanan, Makarand Tapaswi, Vineet Gandhi
Categories: cs.CV cs.AI
Comments: Accepted to MIV at CVPRW 2025 (Oral)
\\
  To understand a prompt, Vision-Language models (VLMs) must perceive the
image, comprehend the text, and build associations within and across both
modalities. For instance, given an 'image of a red toy car', the model should
associate this image to phrases like 'car', 'red toy', 'red object', etc. Feng
and Steinhardt propose the Binding ID mechanism in LLMs, suggesting that the
entity and its corresponding attribute tokens share a Binding ID in the model
activations. We investigate this for image-text binding in VLMs using a
synthetic dataset and task that requires models to associate 3D objects in an
image with their descriptions in the text. Our experiments demonstrate that
VLMs assign a distinct Binding ID to an object's image tokens and its textual
references, enabling in-context association.
\\ ( https://arxiv.org/abs/2505.22200 ,  239kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22202 (*cross-listing*)
Date: Wed, 28 May 2025 10:28:35 GMT   (4599kb)

Title: Let's Predict Sentence by Sentence
Authors: Hyeonbin Hwang, Byeongguk Jeon, Seungone Kim, Jiyeon Kim, Hoyeon
  Chang, Sohee Yang, Seungpil Won, Dohaeng Lee, Youbin Ahn, Minjoon Seo
Categories: cs.CL cs.AI
Comments: Work In Progress
\\
  Autoregressive language models (LMs) generate one token at a time, yet human
reasoning operates over higher-level abstractions - sentences, propositions,
and concepts. This contrast raises a central question- Can LMs likewise learn
to reason over structured semantic units rather than raw token sequences? In
this work, we investigate whether pretrained LMs can be lifted into such
abstract reasoning spaces by building on their learned representations. We
present a framework that adapts a pretrained token-level LM to operate in
sentence space by autoregressively predicting continuous embeddings of next
sentences. We explore two embedding paradigms inspired by classical
representation learning: 1) semantic embeddings, learned via autoencoding to
preserve surface meaning; and 2) contextual embeddings, trained via
next-sentence prediction to encode anticipatory structure. We evaluate both
under two inference regimes: Discretized, which decodes each predicted
embedding into text before re-encoding; and Continuous, which reasons entirely
in embedding space for improved efficiency. Across four domains - mathematics,
logic, commonsense, and planning - contextual embeddings under continuous
inference show competitive performance with Chain-of-Thought (CoT) while
reducing inference-time FLOPs on average by half. We also present early signs
of scalability and modular adaptation. Finally, to visualize latent
trajectories, we introduce SentenceLens, a diagnostic tool that decodes
intermediate model states into interpretable sentences. Together, our results
indicate that pretrained LMs can effectively transition to abstract, structured
reasoning within latent embedding spaces.
\\ ( https://arxiv.org/abs/2505.22202 ,  4599kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22232 (*cross-listing*)
Date: Wed, 28 May 2025 11:06:54 GMT   (6265kb)

Title: Judging Quality Across Languages: A Multilingual Approach to Pretraining
  Data Filtering with Language Models
Authors: Mehdi Ali, Manuel Brack, Max L\"ubbering, Elias Wendt, Abbas Goher
  Khan, Richard Rutmann, Alex Jude, Maurice Kraus, Alexander Arno Weber, Felix
  Stollenwerk, David Kacz\'er, Florian Mai, Lucie Flek, Rafet Sifa, Nicolas
  Flores-Herr, Joachim K\"ohler, Patrick Schramowski, Michael Fromm, Kristian
  Kersting
Categories: cs.CL cs.AI cs.LG
Comments: Project page available at https://huggingface.co/spaces/Jackal-AI/JQL
\\
  High-quality multilingual training data is essential for effectively
pretraining large language models (LLMs). Yet, the availability of suitable
open-source multilingual datasets remains limited. Existing state-of-the-art
datasets mostly rely on heuristic filtering methods, restricting both their
cross-lingual transferability and scalability. Here, we introduce JQL, a
systematic approach that efficiently curates diverse and high-quality
multilingual data at scale while significantly reducing computational demands.
JQL distills LLMs' annotation capabilities into lightweight annotators based on
pretrained multilingual embeddings. These models exhibit robust multilingual
and cross-lingual performance, even for languages and scripts unseen during
training. Evaluated empirically across 35 languages, the resulting annotation
pipeline substantially outperforms current heuristic filtering methods like
Fineweb2. JQL notably enhances downstream model training quality and increases
data retention rates. Our research provides practical insights and valuable
resources for multilingual data curation, raising the standards of multilingual
dataset development.
\\ ( https://arxiv.org/abs/2505.22232 ,  6265kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22264 (*cross-listing*)
Date: Wed, 28 May 2025 11:50:22 GMT   (72kb,D)

Title: MRT at SemEval-2025 Task 8: Maximizing Recovery from Tables with
  Multiple Steps
Authors: Maximiliano Hormaz\'abal Lagos,\'Alvaro Bueno Saez, H\'ector
  Cerezo-Costas, Pedro Alonso Doval, Jorge Alcalde Vesteiro
Categories: cs.CL cs.AI cs.IR
Comments: 7 pages, 6 tables
\\
  In this paper we expose our approach to solve the \textit{SemEval 2025 Task
8: Question-Answering over Tabular Data} challenge. Our strategy leverages
Python code generation with LLMs to interact with the table and get the answer
to the questions. The process is composed of multiple steps: understanding the
content of the table, generating natural language instructions in the form of
steps to follow in order to get the answer, translating these instructions to
code, running it and handling potential errors or exceptions. These steps use
open source LLMs and fine grained optimized prompts for each task (step). With
this approach, we achieved a score of $70.50\%$ for subtask 1.
\\ ( https://arxiv.org/abs/2505.22264 ,  72kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22271 (*cross-listing*)
Date: Wed, 28 May 2025 11:57:46 GMT   (2045kb)

Title: Test-Time Immunization: A Universal Defense Framework Against Jailbreaks
  for (Multimodal) Large Language Models
Authors: Yongcan Yu, Yanbo Wang, Ran He, Jian Liang
Categories: cs.CR cs.AI cs.CL
Comments: Under Review
\\
  While (multimodal) large language models (LLMs) have attracted widespread
attention due to their exceptional capabilities, they remain vulnerable to
jailbreak attacks. Various defense methods are proposed to defend against
jailbreak attacks, however, they are often tailored to specific types of
jailbreak attacks, limiting their effectiveness against diverse adversarial
strategies. For instance, rephrasing-based defenses are effective against text
adversarial jailbreaks but fail to counteract image-based attacks. To overcome
these limitations, we propose a universal defense framework, termed Test-time
IMmunization (TIM), which can adaptively defend against various jailbreak
attacks in a self-evolving way. Specifically, TIM initially trains a gist token
for efficient detection, which it subsequently applies to detect jailbreak
activities during inference. When jailbreak attempts are identified, TIM
implements safety fine-tuning using the detected jailbreak instructions paired
with refusal answers. Furthermore, to mitigate potential performance
degradation in the detector caused by parameter updates during safety
fine-tuning, we decouple the fine-tuning process from the detection module.
Extensive experiments on both LLMs and multimodal LLMs demonstrate the efficacy
of TIM.
\\ ( https://arxiv.org/abs/2505.22271 ,  2045kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22280 (*cross-listing*)
Date: Wed, 28 May 2025 12:17:01 GMT   (905kb,D)

Title: Natural Language Processing in Support of Evidence-based Medicine: A
  Scoping Review
Authors: Zihan Xu, Haotian Ma, Gongbo Zhang, Yihao Ding, Chunhua Weng, Yifan
  Peng
Categories: cs.CL cs.AI
Comments: Accepted by ACL 2025 Findings
\\
  Evidence-based medicine (EBM) is at the forefront of modern healthcare,
emphasizing the use of the best available scientific evidence to guide clinical
decisions. Due to the sheer volume and rapid growth of medical literature and
the high cost of curation, there is a critical need to investigate Natural
Language Processing (NLP) methods to identify, appraise, synthesize, summarize,
and disseminate evidence in EBM. This survey presents an in-depth review of 129
research studies on leveraging NLP for EBM, illustrating its pivotal role in
enhancing clinical decision-making processes. The paper systematically explores
how NLP supports the five fundamental steps of EBM -- Ask, Acquire, Appraise,
Apply, and Assess. The review not only identifies current limitations within
the field but also proposes directions for future research, emphasizing the
potential for NLP to revolutionize EBM by refining evidence extraction,
evidence synthesis, appraisal, summarization, enhancing data comprehensibility,
and facilitating a more efficient clinical workflow.
\\ ( https://arxiv.org/abs/2505.22280 ,  905kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22291 (*cross-listing*)
Date: Wed, 28 May 2025 12:28:35 GMT   (1643kb,D)

Title: Neural Restoration of Greening Defects in Historical Autochrome
  Photographs Based on Purely Synthetic Data
Authors: Saptarshi Neil Sinha and P. Julius Kuehn and Johannes Koppe and Arjan
  Kuijper and Michael Weinmann
Categories: cs.CV cs.AI
\\
  The preservation of early visual arts, particularly color photographs, is
challenged by deterioration caused by aging and improper storage, leading to
issues like blurring, scratches, color bleeding, and fading defects. In this
paper, we present the first approach for the automatic removal of greening
color defects in digitized autochrome photographs. Our main contributions
include a method based on synthetic dataset generation and the use of
generative AI with a carefully designed loss function for the restoration of
visual arts. To address the lack of suitable training datasets for analyzing
greening defects in damaged autochromes, we introduce a novel approach for
accurately simulating such defects in synthetic data. We also propose a
modified weighted loss function for the ChaIR method to account for color
imbalances between defected and non-defected areas. While existing methods
struggle with accurately reproducing original colors and may require
significant manual effort, our method allows for efficient restoration with
reduced time requirements.
\\ ( https://arxiv.org/abs/2505.22291 ,  1643kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22334 (*cross-listing*)
Date: Wed, 28 May 2025 13:21:38 GMT   (452kb,D)

Title: Advancing Multimodal Reasoning via Reinforcement Learning with Cold
  Start
Authors: Lai Wei, Yuting Li, Kaipeng Zheng, Chen Wang, Yue Wang, Linghe Kong,
  Lichao Sun, Weiran Huang
Categories: cs.CL cs.AI cs.CV cs.LG
\\
  Recent advancements in large language models (LLMs) have demonstrated
impressive chain-of-thought reasoning capabilities, with reinforcement learning
(RL) playing a crucial role in this progress. While "aha moment"
patterns--where models exhibit self-correction through reflection--are often
attributed to emergent properties from RL, we first demonstrate that these
patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not
necessarily correlate with improved reasoning performance. Building on these
insights, we present a comprehensive study on enhancing multimodal reasoning
through a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start
with structured chain-of-thought reasoning patterns, followed by (2)
reinforcement learning via GRPO to further refine these capabilities. Our
extensive experiments show that this combined approach consistently outperforms
both SFT-only and RL-only methods across challenging multimodal reasoning
benchmarks. The resulting models achieve state-of-the-art performance among
open-source MLLMs at both 3B and 7B scales, with our 7B model showing
substantial improvements over base models (e.g., 66.3 %$\rightarrow$73.4 % on
MathVista, 62.9 %$\rightarrow$70.4 % on We-Math) and our 3B model achieving
performance competitive with several 7B models. Overall, this work provides
practical guidance for building advanced multimodal reasoning models. Our code
is available at https://github.com/waltonfuture/RL-with-Cold-Start.
\\ ( https://arxiv.org/abs/2505.22334 ,  452kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22338 (*cross-listing*)
Date: Wed, 28 May 2025 13:23:49 GMT   (867kb)

Title: Text2Grad: Reinforcement Learning from Natural Language Feedback
Authors: Hanyang Wang, Lu Wang, Chaoyun Zhang, Tianjun Mao, Si Qin, Qingwei
  Lin, Saravan Rajmohan, Dongmei Zhang
Categories: cs.CL cs.AI
Comments: The code for our method is available at
  https://github.com/microsoft/Text2Grad
\\
  Traditional RLHF optimizes language models with coarse, scalar rewards that
mask the fine-grained reasons behind success or failure, leading to slow and
opaque learning. Recent work augments RL with textual critiques through
prompting or reflection, improving interpretability but leaving model
parameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm
that turns free-form textual feedback into span-level gradients. Given human
(or programmatic) critiques, Text2Grad aligns each feedback phrase with the
relevant token spans, converts these alignments into differentiable reward
signals, and performs gradient updates that directly refine the offending
portions of the model's policy. This yields precise, feedback-conditioned
adjustments instead of global nudges. Text2Grad is realized through three
components: (1) a high-quality feedback-annotation pipeline that pairs
critiques with token spans; (2) a fine-grained reward model that predicts
span-level reward on answer while generating explanatory critiques; and (3) a
span-level policy optimizer that back-propagates natural-language gradients.
Across summarization, code generation, and question answering, Text2Grad
consistently surpasses scalar-reward RL and prompt-only baselines, providing
both higher task metrics and richer interpretability. Our results demonstrate
that natural-language feedback, when converted to gradients, is a powerful
signal for fine-grained policy optimization. The code for our method is
available at https://github.com/microsoft/Text2Grad
\\ ( https://arxiv.org/abs/2505.22338 ,  867kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22343 (*cross-listing*)
Date: Wed, 28 May 2025 13:27:07 GMT   (13001kb,D)

Title: Empowering Intelligent Low-altitude Economy with Large AI Model
  Deployment
Authors: Zhonghao Lyu, Yulan Gao, Junting Chen, Hongyang Du, Jie Xu, Kaibin
  Huang, and Dong In Kim
Categories: eess.SP cs.AI
\\
  Low-altitude economy (LAE) represents an emerging economic paradigm that
redefines commercial and social aerial activities. Large artificial
intelligence models (LAIMs) offer transformative potential to further enhance
the intelligence of LAE services. However, deploying LAIMs in LAE poses several
challenges, including the significant gap between their computational/storage
demands and the limited onboard resources of LAE entities, the mismatch between
lab-trained LAIMs and dynamic physical environments, and the inefficiencies of
traditional decoupled designs for sensing, communication, and computation. To
address these issues, we first propose a hierarchical system architecture
tailored for LAIM deployment and present representative LAE application
scenarios. Next, we explore key enabling techniques that facilitate the mutual
co-evolution of LAIMs and low-altitude systems, and introduce a task-oriented
execution pipeline for scalable and adaptive service delivery. Then, the
proposed framework is validated through real-world case studies. Finally, we
outline open challenges to inspire future research.
\\ ( https://arxiv.org/abs/2505.22343 ,  13001kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22349 (*cross-listing*)
Date: Wed, 28 May 2025 13:31:08 GMT   (2802kb)

Title: ChatPD: An LLM-driven Paper-Dataset Networking System
Authors: Anjie Xu, Ruiqing Ding, Leye Wang
Categories: cs.DB cs.AI cs.IR
Comments: Accepted by KDD Applied Data Science Track 2025
DOI: 10.1145/3711896.3737202
\\
  Scientific research heavily depends on suitable datasets for method
validation, but existing academic platforms with dataset management like
PapersWithCode suffer from inefficiencies in their manual workflow. To overcome
this bottleneck, we present a system, called ChatPD, that utilizes Large
Language Models (LLMs) to automate dataset information extraction from academic
papers and construct a structured paper-dataset network. Our system consists of
three key modules: \textit{paper collection}, \textit{dataset information
extraction}, and \textit{dataset entity resolution} to construct paper-dataset
networks. Specifically, we propose a \textit{Graph Completion and Inference}
strategy to map dataset descriptions to their corresponding entities. Through
extensive experiments, we demonstrate that ChatPD not only outperforms the
existing platform PapersWithCode in dataset usage extraction but also achieves
about 90\% precision and recall in entity resolution tasks. Moreover, we have
deployed ChatPD to continuously extract which datasets are used in papers, and
provide a dataset discovery service, such as task-specific dataset queries and
similar dataset recommendations. We open source ChatPD and the current
paper-dataset network on this [GitHub
repository]{https://github.com/ChatPD-web/ChatPD}.
\\ ( https://arxiv.org/abs/2505.22349 ,  2802kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22353 (*cross-listing*)
Date: Wed, 28 May 2025 13:34:05 GMT   (4734kb)

Title: VME: A Satellite Imagery Dataset and Benchmark for Detecting Vehicles in
  the Middle East and Beyond
Authors: Noora Al-Emadi, Ingmar Weber, Yin Yang, and Ferda Ofli
Categories: cs.CV cs.AI
Journal-ref: Article number:500; 2025; 12
DOI: 10.1038/s41597-025-04567-y
\\
  Detecting vehicles in satellite images is crucial for traffic management,
urban planning, and disaster response. However, current models struggle with
real-world diversity, particularly across different regions. This challenge is
amplified by geographic bias in existing datasets, which often focus on
specific areas and overlook regions like the Middle East. To address this gap,
we present the Vehicles in the Middle East (VME) dataset, designed explicitly
for vehicle detection in high-resolution satellite images from Middle Eastern
countries. Sourced from Maxar, the VME dataset spans 54 cities across 12
countries, comprising over 4,000 image tiles and more than 100,000 vehicles,
annotated using both manual and semi-automated methods. Additionally, we
introduce the largest benchmark dataset for Car Detection in Satellite Imagery
(CDSI), combining images from multiple sources to enhance global car detection.
Our experiments demonstrate that models trained on existing datasets perform
poorly on Middle Eastern images, while the VME dataset significantly improves
detection accuracy in this region. Moreover, state-of-the-art models trained on
CDSI achieve substantial improvements in global car detection.
\\ ( https://arxiv.org/abs/2505.22353 ,  4734kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22384 (*cross-listing*)
Date: Wed, 28 May 2025 14:11:14 GMT   (343kb)

Title: Exact Algorithms and Lower Bounds for Forming Coalitions of Constrained
  Maximum Size
Authors: Foivos Fioravantes and Harmender Gahlawat and Nikolaos Melissinos
Categories: cs.DS cs.AI
Comments: a preliminary version appeared in AAAI 2025
\\
  Imagine we want to split a group of agents into teams in the most
\emph{efficient} way, considering that each agent has their own preferences
about their teammates. This scenario is modeled by the extensively studied
\textsc{Coalition Formation} problem. Here, we study a version of this problem
where each team must additionally be of bounded size.
  We conduct a systematic algorithmic study, providing several intractability
results as well as multiple exact algorithms that scale well as the input grows
(FPT), which could prove useful in practice.
  Our main contribution is an algorithm that deals efficiently with tree-like
structures (bounded \emph{treewidth}) for ``small'' teams. We complement this
result by proving that our algorithm is asymptotically optimal. Particularly,
there can be no algorithm that vastly outperforms the one we present, under
reasonable theoretical assumptions, even when considering star-like structures
(bounded \emph{vertex cover number}).
\\ ( https://arxiv.org/abs/2505.22384 ,  343kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22387 (*cross-listing*)
Date: Wed, 28 May 2025 14:13:38 GMT   (6388kb)

Title: DAM: Domain-Aware Module for Multi-Domain Dataset Condensation
Authors: Jaehyun Choi, Gyojin Han, Dong-Jae Lee, Sunghyun Baek, Junmo Kim
Categories: cs.CV cs.AI cs.LG
\\
  Dataset Condensation (DC) has emerged as a promising solution to mitigate the
computational and storage burdens associated with training deep learning
models. However, existing DC methods largely overlook the multi-domain nature
of modern datasets, which are increasingly composed of heterogeneous images
spanning multiple domains. In this paper, we extend DC and introduce
Multi-Domain Dataset Condensation (MDDC), which aims to condense data that
generalizes across both single-domain and multi-domain settings. To this end,
we propose the Domain-Aware Module (DAM), a training-time module that embeds
domain-related features into each synthetic image via learnable spatial masks.
As explicit domain labels are mostly unavailable in real-world datasets, we
employ frequency-based pseudo-domain labeling, which leverages low-frequency
amplitude statistics. DAM is only active during the condensation process, thus
preserving the same images per class (IPC) with prior methods. Experiments show
that DAM consistently improves in-domain, out-of-domain, and cross-architecture
performance over baseline dataset condensation methods.
\\ ( https://arxiv.org/abs/2505.22387 ,  6388kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22441 (*cross-listing*)
Date: Wed, 28 May 2025 15:04:46 GMT   (9720kb)

Title: Can NeRFs See without Cameras?
Authors: Chaitanya Amballa, Sattwik Basu, Yu-Lin Wei, Zhijian Yang, Mehmet
  Ergezer, Romit Roy Choudhury
Categories: cs.CV cs.AI
\\
  Neural Radiance Fields (NeRFs) have been remarkably successful at
synthesizing novel views of 3D scenes by optimizing a volumetric scene
function. This scene function models how optical rays bring color information
from a 3D object to the camera pixels. Radio frequency (RF) or audio signals
can also be viewed as a vehicle for delivering information about the
environment to a sensor. However, unlike camera pixels, an RF/audio sensor
receives a mixture of signals that contain many environmental reflections (also
called "multipath"). Is it still possible to infer the environment using such
multipath signals? We show that with redesign, NeRFs can be taught to learn
from multipath signals, and thereby "see" the environment. As a grounding
application, we aim to infer the indoor floorplan of a home from sparse WiFi
measurements made at multiple locations inside the home. Although a difficult
inverse problem, our implicitly learnt floorplans look promising, and enables
forward applications, such as indoor signal prediction and basic ray tracing.
\\ ( https://arxiv.org/abs/2505.22441 ,  9720kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22445 (*cross-listing*)
Date: Wed, 28 May 2025 15:08:49 GMT   (10628kb,D)

Title: NFR: Neural Feature-Guided Non-Rigid Shape Registration
Authors: Puhua Jiang, Zhangquan Chen, Mingze Sun, Ruqi Huang
Categories: cs.CV cs.AI
Comments: 20 pages, 9 figures. arXiv admin note: substantial text overlap with
  arXiv:2311.04494
ACM-class: I.4.m; I.2.6
\\
  In this paper, we propose a novel learning-based framework for 3D shape
registration, which overcomes the challenges of significant non-rigid
deformation and partiality undergoing among input shapes, and, remarkably,
requires no correspondence annotation during training. Our key insight is to
incorporate neural features learned by deep learning-based shape matching
networks into an iterative, geometric shape registration pipeline. The
advantage of our approach is two-fold -- On one hand, neural features provide
more accurate and semantically meaningful correspondence estimation than
spatial features (e.g., coordinates), which is critical in the presence of
large non-rigid deformations; On the other hand, the correspondences are
dynamically updated according to the intermediate registrations and filtered by
consistency prior, which prominently robustify the overall pipeline. Empirical
results show that, with as few as dozens of training shapes of limited
variability, our pipeline achieves state-of-the-art results on several
benchmarks of non-rigid point cloud matching and partial shape matching across
varying settings, but also delivers high-quality correspondences between unseen
challenging shape pairs that undergo both significant extrinsic and intrinsic
deformations, in which case neither traditional registration methods nor
intrinsic methods work.
\\ ( https://arxiv.org/abs/2505.22445 ,  10628kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22453 (*cross-listing*)
Date: Wed, 28 May 2025 15:11:16 GMT   (1341kb,D)

Title: Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO
Authors: Lai Wei, Yuting Li, Chen Wang, Yue Wang, Linghe Kong, Weiran Huang,
  Lichao Sun
Categories: cs.CL cs.AI cs.CV cs.LG
\\
  Improving Multi-modal Large Language Models (MLLMs) in the post-training
stage typically relies on supervised fine-tuning (SFT) or reinforcement
learning (RL). However, these supervised methods require expensive and manually
annotated multi-modal data--an ultimately unsustainable resource. While recent
efforts have explored unsupervised post-training, their methods are complex and
difficult to iterate. In this work, we are the first to investigate the use of
GRPO, a stable and scalable online RL algorithm, for enabling continual
self-improvement without any external supervision. We propose MM-UPT, a simple
yet effective framework for unsupervised post-training of MLLMs. MM-UPT builds
upon GRPO, replacing traditional reward signals with a self-rewarding mechanism
based on majority voting over multiple sampled responses. Our experiments
demonstrate that MM-UPT significantly improves the reasoning ability of
Qwen2.5-VL-7B (e.g., 66.3 %$\rightarrow$72.9 % on MathVista, 62.9
%$\rightarrow$68.7 % on We-Math), using standard dataset without ground truth
labels. MM-UPT also outperforms prior unsupervised baselines and even
approaches the results of supervised GRPO. Furthermore, we show that
incorporating synthetic questions, generated solely by MLLM itself, can boost
performance as well, highlighting a promising approach for scalable
self-improvement. Overall, MM-UPT offers a new paradigm for continual,
autonomous enhancement of MLLMs in the absence of external supervision. Our
code is available at https://github.com/waltonfuture/MM-UPT.
\\ ( https://arxiv.org/abs/2505.22453 ,  1341kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22457 (*cross-listing*)
Date: Wed, 28 May 2025 15:13:34 GMT   (4624kb)

Title: Fostering Video Reasoning via Next-Event Prediction
Authors: Haonan Wang, Hongfu Liu, Xiangyan Liu, Chao Du, Kenji Kawaguchi, Ye
  Wang, Tianyu Pang
Categories: cs.CV cs.AI cs.CL
\\
  Next-token prediction serves as the foundational learning task enabling
reasoning in LLMs. But what should the learning task be when aiming to equip
MLLMs with temporal reasoning capabilities over video inputs? Existing tasks
such as video question answering often rely on annotations from humans or much
stronger MLLMs, while video captioning tends to entangle temporal reasoning
with spatial information. To address this gap, we propose next-event prediction
(NEP), a learning task that harnesses future video segments as a rich,
self-supervised signal to foster temporal reasoning. We segment each video into
past and future frames: the MLLM takes the past frames as input and predicts a
summary of events derived from the future frames, thereby encouraging the model
to reason temporally in order to complete the task. To support this task, we
curate V1-33K, a dataset comprising 33,000 automatically extracted video
segments spanning diverse real-world scenarios. We further explore a range of
video instruction-tuning strategies to study their effects on temporal
reasoning. To evaluate progress, we introduce FutureBench to assess coherence
in predicting unseen future events. Experiments validate that NEP offers a
scalable and effective training paradigm for fostering temporal reasoning in
MLLMs.
\\ ( https://arxiv.org/abs/2505.22457 ,  4624kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22525 (*cross-listing*)
Date: Wed, 28 May 2025 16:12:45 GMT   (18984kb)

Title: Thinking with Generated Images
Authors: Ethan Chern, Zhulin Hu, Steffi Chern, Siqi Kou, Jiadi Su, Yan Ma,
  Zhijie Deng, Pengfei Liu
Categories: cs.CV cs.AI cs.CL
\\
  We present Thinking with Generated Images, a novel paradigm that
fundamentally transforms how large multimodal models (LMMs) engage with visual
reasoning by enabling them to natively think across text and vision modalities
through spontaneous generation of intermediate visual thinking steps. Current
visual reasoning with LMMs is constrained to either processing fixed
user-provided images or reasoning solely through text-based chain-of-thought
(CoT). Thinking with Generated Images unlocks a new dimension of cognitive
capability where models can actively construct intermediate visual thoughts,
critique their own visual hypotheses, and refine them as integral components of
their reasoning process. We demonstrate the effectiveness of our approach
through two complementary mechanisms: (1) vision generation with intermediate
visual subgoals, where models decompose complex visual tasks into manageable
components that are generated and integrated progressively, and (2) vision
generation with self-critique, where models generate an initial visual
hypothesis, analyze its shortcomings through textual reasoning, and produce
refined outputs based on their own critiques. Our experiments on vision
generation benchmarks show substantial improvements over baseline approaches,
with our models achieving up to 50% (from 38% to 57%) relative improvement in
handling complex multi-object scenarios. From biochemists exploring novel
protein structures, and architects iterating on spatial designs, to forensic
analysts reconstructing crime scenes, and basketball players envisioning
strategic plays, our approach enables AI models to engage in the kind of visual
imagination and iterative refinement that characterizes human creative,
analytical, and strategic thinking. We release our open-source suite at
https://github.com/GAIR-NLP/thinking-with-generated-images.
\\ ( https://arxiv.org/abs/2505.22525 ,  18984kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22543 (*cross-listing*)
Date: Wed, 28 May 2025 16:24:52 GMT   (9778kb)

Title: Scaling-up Perceptual Video Quality Assessment
Authors: Ziheng Jia, Zicheng Zhang, Zeyu Zhang, Yingji Liang, Xiaorong Zhu,
  Chunyi Li, Jinliang Han, Haoning Wu, Bin Wang, Haoran Zhang, Guanyu Zhu,
  Qiyong Zhao, Xiaohong Liu, Guangtao Zhai, Xiongkuo Min
Categories: cs.CV cs.AI
\\
  The data scaling law has been shown to significantly enhance the performance
of large multi-modal models (LMMs) across various downstream tasks. However, in
the domain of perceptual video quality assessment (VQA), the potential of
scaling law remains unprecedented due to the scarcity of labeled resources and
the insufficient scale of datasets. To address this, we propose
\textbf{OmniVQA}, an efficient framework designed to efficiently build
high-quality, human-in-the-loop VQA multi-modal instruction databases (MIDBs).
We then scale up to create \textbf{OmniVQA-Chat-400K}, the largest MIDB in the
VQA field concurrently. Our focus is on the technical and aesthetic quality
dimensions, with abundant in-context instruction data to provide fine-grained
VQA knowledge. Additionally, we have built the \textbf{OmniVQA-MOS-20K} dataset
to enhance the model's quantitative quality rating capabilities. We then
introduce a \textbf{complementary} training strategy that effectively leverages
the knowledge from datasets for quality understanding and quality rating tasks.
Furthermore, we propose the \textbf{OmniVQA-FG (fine-grain)-Benchmark} to
evaluate the fine-grained performance of the models. Our results demonstrate
that our models achieve state-of-the-art performance in both quality
understanding and rating tasks.
\\ ( https://arxiv.org/abs/2505.22543 ,  9778kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22552 (*cross-listing*)
Date: Wed, 28 May 2025 16:34:14 GMT   (620kb)

Title: ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation
  with Lightweight Specialized LLM
Authors: Hoang Pham, Thanh-Do Nguyen and Khac-Hoai Nam Bui
Categories: cs.CL cs.AI cs.DB
Comments: Accepted by ACL 2025 findings
Journal-ref: ACL 2025
\\
  Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of
large language models (LLMs) is an emerging research challenge in claim
verification. While KGs provide structured, semantically rich representations
well-suited for reasoning, most existing verification methods rely on
unstructured text corpora, limiting their ability to effectively leverage KGs.
Additionally, despite possessing strong reasoning abilities, modern LLMs
struggle with multi-step modular pipelines and reasoning over KGs without
adaptation. To address these challenges, we propose ClaimPKG, an end-to-end
framework that seamlessly integrates LLM reasoning with structured knowledge
from KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight,
specialized LLM to represent the input claim as pseudo-subgraphs, guiding a
dedicated subgraph retrieval module to identify relevant KG subgraphs. These
retrieved subgraphs are then processed by a general-purpose LLM to produce the
final verdict and justification. Extensive experiments on the FactKG dataset
demonstrate that ClaimPKG achieves state-of-the-art performance, outperforming
strong baselines in this research field by 9%-12% accuracy points across
multiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability
to unstructured datasets such as HoVer and FEVEROUS, effectively combining
structured knowledge from KGs with LLM reasoning across various LLM backbones.
\\ ( https://arxiv.org/abs/2505.22552 ,  620kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22564 (*cross-listing*)
Date: Wed, 28 May 2025 16:42:10 GMT   (1828kb)

Title: PRISM: Video Dataset Condensation with Progressive Refinement and
  Insertion for Sparse Motion
Authors: Jaehyun Choi, Jiwan Hur, Gyojin Han, Jaemyung Yu, Junmo Kim
Categories: cs.CV cs.AI cs.LG
\\
  Video dataset condensation has emerged as a critical technique for addressing
the computational challenges associated with large-scale video data processing
in deep learning applications. While significant progress has been made in
image dataset condensation, the video domain presents unique challenges due to
the complex interplay between spatial content and temporal dynamics. This paper
introduces PRISM, Progressive Refinement and Insertion for Sparse Motion, for
video dataset condensation, a novel approach that fundamentally reconsiders how
video data should be condensed. Unlike the previous method that separates
static content from dynamic motion, our method preserves the essential
interdependence between these elements. Our approach progressively refines and
inserts frames to fully accommodate the motion in an action while achieving
better performance but less storage, considering the relation of gradients for
each frame. Extensive experiments across standard video action recognition
benchmarks demonstrate that PRISM outperforms existing disentangled approaches
while maintaining compact representations suitable for resource-constrained
environments.
\\ ( https://arxiv.org/abs/2505.22564 ,  1828kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22566 (*cross-listing*)
Date: Wed, 28 May 2025 16:43:01 GMT   (1685kb)

Title: Universal Visuo-Tactile Video Understanding for Embodied Interaction
Authors: Yifan Xie, Mingyang Li, Shoujie Li, Xingting Li, Guangyu Chen, Fei Ma,
  Fei Richard Yu, Wenbo Ding
Categories: cs.CV cs.AI
Comments: 13 pages, 5 figures
\\
  Tactile perception is essential for embodied agents to understand physical
attributes of objects that cannot be determined through visual inspection
alone. While existing approaches have made progress in visual and language
modalities for physical understanding, they fail to effectively incorporate
tactile information that provides crucial haptic feedback for real-world
interaction. In this paper, we present VTV-LLM, the first multi-modal large
language model for universal Visuo-Tactile Video (VTV) understanding that
bridges the gap between tactile perception and natural language. To address the
challenges of cross-sensor and cross-modal integration, we contribute VTV150K,
a comprehensive dataset comprising 150,000 video frames from 100 diverse
objects captured across three different tactile sensors (GelSight Mini, DIGIT,
and Tac3D), annotated with four fundamental tactile attributes (hardness,
protrusion, elasticity, and friction). We develop a novel three-stage training
paradigm that includes VTV enhancement for robust visuo-tactile representation,
VTV-text alignment for cross-modal correspondence, and text prompt finetuning
for natural language generation. Our framework enables sophisticated tactile
reasoning capabilities including feature assessment, comparative analysis,
scenario-based decision making and so on. Experimental evaluations demonstrate
that VTV-LLM achieves superior performance in tactile video understanding
tasks, establishing a foundation for more intuitive human-machine interaction
in tactile domains.
\\ ( https://arxiv.org/abs/2505.22566 ,  1685kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22571 (*cross-listing*)
Date: Wed, 28 May 2025 16:46:31 GMT   (611kb)

Title: Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified
  Retrieval-Augmented Generation Systems
Authors: Hoang Pham and Khac-Hoai Nam Bui
Categories: cs.CL cs.AI cs.DB cs.IR
\\
  This paper presents a novel approach for unified retrieval-augmented
generation (RAG) systems using the recent emerging large language model (LLM)
agent concept. Specifically, Agent LLM, which utilizes LLM as fundamental
controllers, has become a promising approach to enable the interpretability of
RAG tasks, especially for complex reasoning question-answering systems (e.g.,
multi-hop queries). Nonetheless, previous works mainly focus on solving RAG
systems with either single-hop or multi-hop approaches separately, which limits
the application of those approaches to real-world applications. In this study,
we propose a trainable agent framework called Agent-UniRAG for unified
retrieval-augmented LLM systems, which enhances the effectiveness and
interpretability of RAG systems. The main idea is to design an LLM agent
framework to solve RAG tasks step-by-step based on the complexity of the
inputs, simultaneously including single-hop and multi-hop queries in an
end-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset
to enable the proposed agent framework for small open-source LLMs (e.g.,
Llama-3-8B). The results show comparable performances with closed-source and
larger open-source LLMs across various RAG benchmarks. Our source code and
dataset are publicly available for further exploitation.
\\ ( https://arxiv.org/abs/2505.22571 ,  611kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22572 (*cross-listing*)
Date: Wed, 28 May 2025 16:46:55 GMT   (1803kb)

Title: Fusion Steering: Prompt-Specific Activation Control
Authors: Waldemar Chang, Alhassan Yasin
Categories: cs.CL cs.AI
Comments: 14 pages, 4 figures, 2 tables
\\
  We present Fusion Steering, an activation steering methodology that improves
factual accuracy in large language models (LLMs) for question-answering (QA)
tasks. This approach introduces flexible steering configurations, including
full-layer steering and segmented steering. Unlike traditional methods
constrained to single-layer or fixed-layer operations, Fusion Steering employs
dynamic injection of prompt-specific activation deltas across all transformer
layers. These activation deltas are derived from reference completions that
combine the ground-truth answer with a model-generated explanation to
facilitate semantically enriched, example-specific steering. The injection
weights are optimized per prompt using Optuna, targeting a joint objective that
balances token overlap (factual alignment) and perplexity (fluency proxy).
Evaluation employs a composite score integrating token overlap and LLM-graded
quality, encompassing factual accuracy, coherence, and relevance. Empirical
results on 260 SimpleQA prompts (selected from 500 where the baseline failed)
showcase the efficacy of segmented steering. Using Gemma-2-2B-IT with 8-bit
quantization, segmented steering achieves an accuracy of 25.4% (outputs scoring
$\geq 0.6$), outperforming the baseline at 3.5% and full-layer steering at
16.2%. Under the stricter SimpleQA rubric, segmented steering boosts fully
correct responses from 0.0% to 13.1%. These findings highlight the strengths of
segmented, dynamic intervention strategies and the promise of per-prompt,
full-network activation control. Fusion Steering is also amenable to sparse
representations, such as Neuronpedia or sparse crosscoders, suggesting a
promising direction for interpretable and scalable activation-level control in
LLMs.
\\ ( https://arxiv.org/abs/2505.22572 ,  1803kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22581 (*cross-listing*)
Date: Wed, 28 May 2025 16:54:36 GMT   (9530kb,D)

Title: Tell me Habibi, is it Real or Fake?
Authors: Kartik Kuckreja, Parul Gupta, Injy Hamed, Thamar Solorio, Muhammad
  Haris Khan, Abhinav Dhall
Categories: cs.CV cs.AI
Comments: 9 pages, 2 figures, 12 tables
\\
  Deepfake generation methods are evolving fast, making fake media harder to
detect and raising serious societal concerns. Most deepfake detection and
dataset creation research focuses on monolingual content, often overlooking the
challenges of multilingual and code-switched speech, where multiple languages
are mixed within the same discourse. Code-switching, especially between Arabic
and English, is common in the Arab world and is widely used in digital
communication. This linguistic mixing poses extra challenges for deepfake
detection, as it can confuse models trained mostly on monolingual data. To
address this, we introduce \textbf{ArEnAV}, the first large-scale
Arabic-English audio-visual deepfake dataset featuring intra-utterance
code-switching, dialectal variation, and monolingual Arabic content. It
\textbf{contains 387k videos and over 765 hours of real and fake videos}. Our
dataset is generated using a novel pipeline integrating four Text-To-Speech and
two lip-sync models, enabling comprehensive analysis of multilingual multimodal
deepfake detection. We benchmark our dataset against existing monolingual and
multilingual datasets, state-of-the-art deepfake detection models, and a human
evaluation, highlighting its potential to advance deepfake research. The
dataset can be accessed
\href{https://huggingface.co/datasets/kartik060702/ArEnAV-Full}{here}.
\\ ( https://arxiv.org/abs/2505.22581 ,  9530kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22583 (*cross-listing*)
Date: Wed, 28 May 2025 16:56:11 GMT   (512kb)

Title: GitGoodBench: A Novel Benchmark For Evaluating Agentic Performance On
  Git
Authors: Tobias Lindenbauer, Egor Bogomolov, Yaroslav Zharov
Categories: cs.SE cs.AI
Comments: Short Paper, 5 pages
\\
  Benchmarks for Software Engineering (SE) AI agents, most notably SWE-bench,
have catalyzed progress in programming capabilities of AI agents. However, they
overlook critical developer workflows such as Version Control System (VCS)
operations. To address this issue, we present GitGoodBench, a novel benchmark
for evaluating AI agent performance on VCS tasks. GitGoodBench covers three
core Git scenarios extracted from permissive open-source Python, Java, and
Kotlin repositories. Our benchmark provides three datasets: a comprehensive
evaluation suite (900 samples), a rapid prototyping version (120 samples), and
a training corpus (17,469 samples). We establish baseline performance on the
prototyping version of our benchmark using GPT-4o equipped with custom tools,
achieving a 21.11% solve rate overall. We expect GitGoodBench to serve as a
crucial stepping stone toward truly comprehensive SE agents that go beyond mere
programming.
\\ ( https://arxiv.org/abs/2505.22583 ,  512kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22591 (*cross-listing*)
Date: Wed, 28 May 2025 17:02:47 GMT   (9582kb)

Title: Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical
  Reasoning
Authors: Erxin Yu, Jing Li, Ming Liao, Qi Zhu, Boyang Xue, Minghui Xu, Baojun
  Wang, Lanqing Hong, Fei Mi, Lifeng Shang
Categories: cs.CL cs.AI cs.LG
Comments: 16 pages, 9 figures
\\
  Although large language models demonstrate strong performance across various
domains, they still struggle with numerous bad cases in mathematical reasoning.
Previous approaches to learning from errors synthesize training data by solely
extrapolating from isolated bad cases, thereby failing to generalize the
extensive patterns inherent within these cases. This paper presents
Self-Error-Instruct (SEI), a framework that addresses these model weaknesses
and synthesizes more generalized targeted training data. Specifically, we
explore a target model on two mathematical datasets, GSM8K and MATH, to
pinpoint bad cases. Then, we generate error keyphrases for these cases based on
the instructor model's (GPT-4o) analysis and identify error types by clustering
these keyphrases. Next, we sample a few bad cases during each generation for
each identified error type and input them into the instructor model, which
synthesizes additional training data using a self-instruct approach. This new
data is refined through a one-shot learning process to ensure that only the
most effective examples are kept. Finally, we use these curated data to
fine-tune the target model, iteratively repeating the process to enhance
performance. We apply our framework to various models and observe improvements
in their reasoning abilities across both in-domain and out-of-domain
mathematics datasets. These results demonstrate the effectiveness of self-error
instruction in improving LLMs' mathematical reasoning through error
generalization.
\\ ( https://arxiv.org/abs/2505.22591 ,  9582kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22598 (*cross-listing*)
Date: Wed, 28 May 2025 17:13:11 GMT   (224kb,D)

Title: On the performance of machine-learning assisted Monte Carlo in sampling
  from simple statistical physics models
Authors: Luca Maria Del Bono, Federico Ricci-Tersenghi, Francesco Zamponi
Categories: cond-mat.dis-nn cond-mat.stat-mech cs.AI cs.LG physics.comp-ph
Comments: 16 pages, 9 figures
\\
  Recent years have seen a rise in the application of machine learning
techniques to aid the simulation of hard-to-sample systems that cannot be
studied using traditional methods. Despite the introduction of many different
architectures and procedures, a wide theoretical understanding is still
lacking, with the risk of suboptimal implementations. As a first step to
address this gap, we provide here a complete analytic study of the widely-used
Sequential Tempering procedure applied to a shallow MADE architecture for the
Curie-Weiss model. The contribution of this work is twofold: firstly, we give a
description of the optimal weights and of the training under Gradient Descent
optimization. Secondly, we compare what happens in Sequential Tempering with
and without the addition of local Metropolis Monte Carlo steps. We are thus
able to give theoretical predictions on the best procedure to apply in this
case. This work establishes a clear theoretical basis for the integration of
machine learning techniques into Monte Carlo sampling and optimization.
\\ ( https://arxiv.org/abs/2505.22598 ,  224kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22608 (*cross-listing*)
Date: Wed, 28 May 2025 17:24:21 GMT   (1672kb)

Title: Effective and Efficient One-pass Compression of Speech Foundation Models
  Using Sparsity-aware Self-pinching Gates
Authors: Haoning Xu, Zhaoqing Li, Youjun Chen, Huimeng Wang, Guinan Li, Mengzhe
  Geng, Chengxi Deng, Xunying Liu
Categories: cs.SD cs.AI eess.AS
Comments: Submitted to Interspeech 2025
\\
  This paper presents a novel approach for speech foundation models compression
that tightly integrates model pruning and parameter update into a single stage.
Highly compact layer-level tied self-pinching gates each containing only a
single learnable threshold are jointly trained with uncompressed models and
used in fine-grained neuron level pruning. Experiments conducted on the
LibriSpeech-100hr corpus suggest that our approach reduces the number of
parameters of wav2vec2.0-base and HuBERT-large models by 65% and 60%
respectively, while incurring no statistically significant word error rate
(WER) increase on the test-clean dataset. Compared to previously published
methods on the same task, our approach not only achieves the lowest WER of
7.05% on the test-clean dataset under a comparable model compression ratio of
4.26x, but also operates with at least 25% less model compression time.
\\ ( https://arxiv.org/abs/2505.22608 ,  1672kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22613 (*cross-listing*)
Date: Wed, 28 May 2025 17:29:34 GMT   (2001kb,D)

Title: RICO: Improving Accuracy and Completeness in Image Recaptioning via
  Visual Reconstruction
Authors: Yuchi Wang, Yishuo Cai, Shuhuai Ren, Sihan Yang, Linli Yao, Yuanxin
  Liu, Yuanxing Zhang, Pengfei Wan, Xu Sun
Categories: cs.CV cs.AI cs.CL
Comments: code: https://github.com/wangyuchi369/RICO
\\
  Image recaptioning is widely used to generate training datasets with enhanced
quality for various multimodal tasks. Existing recaptioning methods typically
rely on powerful multimodal large language models (MLLMs) to enhance textual
descriptions, but often suffer from inaccuracies due to hallucinations and
incompleteness caused by missing fine-grained details. To address these
limitations, we propose RICO, a novel framework that refines captions through
visual reconstruction. Specifically, we leverage a text-to-image model to
reconstruct a caption into a reference image, and prompt an MLLM to identify
discrepancies between the original and reconstructed images to refine the
caption. This process is performed iteratively, further progressively promoting
the generation of more faithful and comprehensive descriptions. To mitigate the
additional computational cost induced by the iterative process, we introduce
RICO-Flash, which learns to generate captions like RICO using DPO. Extensive
experiments demonstrate that our approach significantly improves caption
accuracy and completeness, outperforms most baselines by approximately 10% on
both CapsBench and CompreCap. Code released at
https://github.com/wangyuchi369/RICO.
\\ ( https://arxiv.org/abs/2505.22613 ,  2001kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22633 (*cross-listing*)
Date: Wed, 28 May 2025 17:50:21 GMT   (5194kb,D)

Title: Spatial Knowledge Graph-Guided Multimodal Synthesis
Authors: Yida Xue, Zhen Bi, Jinnan Yang, Jungang Lou, Huajun Chen, Ningyu Zhang
Categories: cs.CL cs.AI cs.CV cs.LG cs.MM
Comments: Ongoing work
\\
  Recent advances in multimodal large language models (MLLMs) have
significantly enhanced their capabilities; however, their spatial perception
abilities remain a notable limitation. To address this challenge, multimodal
data synthesis offers a promising solution. Yet, ensuring that synthesized data
adhere to spatial common sense is a non-trivial task. In this work, we
introduce SKG2Data, a novel multimodal synthesis approach guided by spatial
knowledge graphs, grounded in the concept of knowledge-to-data generation.
SKG2Data automatically constructs a Spatial Knowledge Graph (SKG) to emulate
human-like perception of spatial directions and distances, which is
subsequently utilized to guide multimodal data synthesis. Extensive experiments
demonstrate that data synthesized from diverse types of spatial knowledge,
including direction and distance, not only enhance the spatial perception and
reasoning abilities of MLLMs but also exhibit strong generalization
capabilities. We hope that the idea of knowledge-based data synthesis can
advance the development of spatial intelligence.
\\ ( https://arxiv.org/abs/2505.22633 ,  5194kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22635 (*cross-listing*)
Date: Wed, 28 May 2025 17:51:10 GMT   (339kb)

Title: Learning Composable Chains-of-Thought
Authors: Fangcong Yin, Zeyu Leo Liu, Liu Leqi, Xi Ye, Greg Durrett
Categories: cs.CL cs.AI
\\
  A common approach for teaching large language models (LLMs) to reason is to
train on chain-of-thought (CoT) traces of in-distribution reasoning problems,
but such annotated data is costly to obtain for every problem of interest. We
want reasoning models to generalize beyond their training distribution, and
ideally to generalize compositionally: combine atomic reasoning skills to solve
harder, unseen reasoning tasks. We take a step towards compositional
generalization of reasoning skills when addressing a target compositional task
that has no labeled CoT data. We find that simply training models on CoT data
of atomic tasks leads to limited generalization, but minimally modifying CoT
formats of constituent atomic tasks to be composable can lead to improvements.
We can train "atomic CoT" models on the atomic tasks with Composable CoT data
and combine them with multitask learning or model merging for better zero-shot
performance on the target compositional task. Such a combined model can be
further bootstrapped on a small amount of compositional data using rejection
sampling fine-tuning (RFT). Results on string operations and natural language
skill compositions show that training LLMs on Composable CoT outperforms
multitask learning and continued fine-tuning baselines within a given training
data budget.
\\ ( https://arxiv.org/abs/2505.22635 ,  339kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22649 (*cross-listing*)
Date: Wed, 28 May 2025 17:57:11 GMT   (14310kb,D)

Title: Pre-training for Recommendation Unlearning
Authors: Guoxuan Chen, Lianghao Xia, Chao Huang
Categories: cs.IR cs.AI cs.LG
Comments: Accepted to SIGIR 2025 Oral
DOI: 10.1145/3726302.3730060
\\
  Modern recommender systems powered by Graph Neural Networks (GNNs) excel at
modeling complex user-item interactions, yet increasingly face scenarios
requiring selective forgetting of training data. Beyond user requests to remove
specific interactions due to privacy concerns or preference changes, regulatory
frameworks mandate recommender systems' ability to eliminate the influence of
certain user data from models. This recommendation unlearning challenge
presents unique difficulties as removing connections within interaction graphs
creates ripple effects throughout the model, potentially impacting
recommendations for numerous users. Traditional approaches suffer from
significant drawbacks: fragmentation methods damage graph structure and
diminish performance, while influence function techniques make assumptions that
may not hold in complex GNNs, particularly with self-supervised or random
architectures. To address these limitations, we propose a novel model-agnostic
pre-training paradigm UnlearnRec that prepares systems for efficient unlearning
operations. Our Influence Encoder takes unlearning requests together with
existing model parameters and directly produces updated parameters of unlearned
model with little fine-tuning, avoiding complete retraining while preserving
model performance characteristics. Extensive evaluation on public benchmarks
demonstrates that our method delivers exceptional unlearning effectiveness
while providing more than 10x speedup compared to retraining approaches. We
release our method implementation at: https://github.com/HKUDS/UnlearnRec.
\\ ( https://arxiv.org/abs/2505.22649 ,  14310kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22657 (*cross-listing*)
Date: Wed, 28 May 2025 17:59:13 GMT   (11939kb,D)

Title: 3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large
  Language Model
Authors: Wenbo Hu, Yining Hong, Yanjun Wang, Leison Gao, Zibu Wei, Xingcheng
  Yao, Nanyun Peng, Yonatan Bitton, Idan Szpektor, Kai-Wei Chang
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: demos at: https://3dllm-mem.github.io
\\
  Humans excel at performing complex tasks by leveraging long-term memory
across temporal and spatial experiences. In contrast, current Large Language
Models (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D
environments. We posit that part of this limitation is due to the lack of
proper 3D spatial-temporal memory modeling in LLMs. To address this, we first
introduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000
trajectories and 2,892 embodied tasks, question-answering and captioning,
designed to evaluate an agent's ability to reason over long-term memory in 3D
environments. Second, we propose 3DLLM-Mem, a novel dynamic memory management
and fusion model for embodied spatial-temporal reasoning and actions in LLMs.
Our model uses working memory tokens, which represents current observations, as
queries to selectively attend to and fuse the most useful spatial and temporal
features from episodic memory, which stores past observations and interactions.
Our approach allows the agent to focus on task-relevant information while
maintaining memory efficiency in complex, long-horizon environments.
Experimental results demonstrate that 3DLLM-Mem achieves state-of-the-art
performance across various tasks, outperforming the strongest baselines by
16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied
tasks.
\\ ( https://arxiv.org/abs/2505.22657 ,  11939kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21729 (*cross-listing*)
Date: Thu, 22 May 2025 16:53:52 GMT   (29431kb)

Title: Bridging the Narrative Divide: Cross-Platform Discourse Networks in
  Fragmented Ecosystems
Authors: Patrick Gerard, Hans W. A. Hanley, Luca Luceri, Emilio Ferrara
Categories: cs.SI cs.CY
Comments: 22 pages, 5 figures
\\
  Political discourse has grown increasingly fragmented across different social
platforms, making it challenging to trace how narratives spread and evolve
within such a fragmented information ecosystem. Reconstructing social graphs
and information diffusion networks is challenging, and available strategies
typically depend on platform-specific features and behavioral signals which are
often incompatible across systems and increasingly restricted. To address these
challenges, we present a platform-agnostic framework that allows to accurately
and efficiently reconstruct the underlying social graph of users'
cross-platform interactions, based on discovering latent narratives and users'
participation therein. Our method achieves state-of-the-art performance in key
network-based tasks: information operation detection, ideological stance
prediction, and cross-platform engagement
prediction$\unicode{x2013}$$\unicode{x2013}$while requiring significantly less
data than existing alternatives and capturing a broader set of users. When
applied to cross-platform information dynamics between Truth Social and X
(formerly Twitter), our framework reveals a small, mixed-platform group of
$\textit{bridge users}$, comprising just 0.33% of users and 2.14% of posts, who
introduce nearly 70% of $\textit{migrating narratives}$ to the receiving
platform. These findings offer a structural lens for anticipating how
narratives traverse fragmented information ecosystems, with implications for
cross-platform governance, content moderation, and policy interventions.
\\ ( https://arxiv.org/abs/2505.21729 ,  29431kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21733 (*cross-listing*)
Date: Tue, 27 May 2025 20:22:45 GMT   (162kb)

Title: Scrapers selectively respect robots.txt directives: evidence from a
  large-scale empirical study
Authors: Taein Kim, Karstan Bock, Claire Luo, Amanda Liswood, Emily Wenger
Categories: cs.NI cs.CR cs.CY
Comments: 13 pages
\\
  Online data scraping has taken on new dimensions in recent years, as
traditional scrapers have been joined by new AI-specific bots. To counteract
unwanted scraping, many sites use tools like the Robots Exclusion Protocol
(REP), which places a robots.txt file at the site root to dictate scraper
behavior. Yet, the efficacy of the REP is not well-understood. Anecdotal
evidence suggests some bots comply poorly with it, but no rigorous study exists
to support (or refute) this claim. To understand the merits and limits of the
REP, we conduct the first large-scale study of web scraper compliance with
robots.txt directives using anonymized web logs from our institution. We
analyze the behavior of 130 self-declared bots (and many anonymous ones) over
40 days, using a series of controlled robots.txt experiments. We find that bots
are less likely to comply with stricter robots.txt directives, and that certain
categories of bots, including AI search crawlers, rarely check robots.txt at
all. These findings suggest that relying on robots.txt files to prevent
unwanted scraping is risky and highlight the need for alternative approaches.
\\ ( https://arxiv.org/abs/2505.21733 ,  162kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22327 (*cross-listing*)
Date: Wed, 28 May 2025 13:14:44 GMT   (1500kb)

Title: NLP for Social Good: A Survey of Challenges, Opportunities, and
  Responsible Deployment
Authors: Antonia Karamolegkou, Angana Borah, Eunjung Cho, Sagnik Ray Choudhury,
  Martina Galletti, Rajarshi Ghosh, Pranav Gupta, Oana Ignat, Priyanka
  Kargupta, Neema Kotonya, Hemank Lamba, Sun-Joo Lee, Arushi Mangla, Ishani
  Mondal, Deniz Nazarova, Poli Nemkova, Dina Pisarevskaya, Naquee Rizwan,
  Nazanin Sabri, Dominik Stammbach, Anna Steinberg, David Tom\'as, Steven R
  Wilson, Bowen Yi, Jessica H Zhu, Arkaitz Zubiaga, Anders S{\o}gaard,
  Alexander Fraser, Zhijing Jin, Rada Mihalcea, Joel R. Tetreault, Daryna
  Dementieva
Categories: cs.CL cs.CY
\\
  Recent advancements in large language models (LLMs) have unlocked
unprecedented possibilities across a range of applications. However, as a
community, we believe that the field of Natural Language Processing (NLP) has a
growing need to approach deployment with greater intentionality and
responsibility. In alignment with the broader vision of AI for Social Good
(Toma\v{s}ev et al., 2020), this paper examines the role of NLP in addressing
pressing societal challenges. Through a cross-disciplinary analysis of social
goals and emerging risks, we highlight promising research directions and
outline challenges that must be addressed to ensure responsible and equitable
progress in NLP4SG research.
\\ ( https://arxiv.org/abs/2505.22327 ,  1500kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22645 (*cross-listing*)
Date: Wed, 28 May 2025 17:56:49 GMT   (2315kb)

Title: Characterizing Bias: Benchmarking Large Language Models in Simplified
  versus Traditional Chinese
Authors: Hanjia Lyu, Jiebo Luo, Jian Kang, Allison Koenecke
Categories: cs.CL cs.CY
Comments: To appear in the 2025 ACM Conference on Fairness, Accountability, and
  Transparency (FAccT '25)
\\
  While the capabilities of Large Language Models (LLMs) have been studied in
both Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit
differential performance when prompted in these two variants of written
Chinese. This understanding is critical, as disparities in the quality of LLM
responses can perpetuate representational harms by ignoring the different
cultural contexts underlying Simplified versus Traditional Chinese, and can
exacerbate downstream harms in LLM-facilitated decision-making in domains such
as education or hiring. To investigate potential LLM performance disparities,
we design two benchmark tasks that reflect real-world scenarios: regional term
choice (prompting the LLM to name a described item which is referred to
differently in Mainland China and Taiwan), and regional name choice (prompting
the LLM to choose who to hire from a list of names in both Simplified and
Traditional Chinese). For both tasks, we audit the performance of 11 leading
commercial LLM services and open-sourced models -- spanning those primarily
trained on English, Simplified Chinese, or Traditional Chinese. Our analyses
indicate that biases in LLM responses are dependent on both the task and
prompting language: while most LLMs disproportionately favored Simplified
Chinese responses in the regional term choice task, they surprisingly favored
Traditional Chinese names in the regional name choice task. We find that these
disparities may arise from differences in training data representation, written
character preferences, and tokenization of Simplified and Traditional Chinese.
These findings highlight the need for further analysis of LLM biases; as such,
we provide an open-sourced benchmark dataset to foster reproducible evaluations
of future LLM behavior across Chinese language variants
(https://github.com/brucelyu17/SC-TC-Bench).
\\ ( https://arxiv.org/abs/2505.22645 ,  2315kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21952 (*cross-listing*)
Date: Wed, 28 May 2025 04:06:04 GMT   (14kb)

Title: Properties of zero-determinant strategies in multichannel games
Authors: Masahiko Ueda
Categories: physics.soc-ph cs.GT cs.MA cs.SY eess.SY
Comments: 12 pages
\\
  Controlling payoffs in repeated games is one of the important topics in
control theory of multi-agent systems. Recently proposed zero-determinant
strategies enable players to unilaterally enforce linear relations between
payoffs. Furthermore, based on the mathematics of zero-determinant strategies,
regional payoff control, in which payoffs are enforced into some feasible
regions, has been discovered in social dilemma situations. More recently,
theory of payoff control was extended to multichannel games, where players
parallelly interact with each other in multiple channels. However, properties
of zero-determinant strategies specific to multichannel games are still not
clear. In this paper, we elucidate properties of zero-determinant strategies in
multichannel games. First, we relate the existence condition of
zero-determinant strategies in multichannel games to that of zero-determinant
strategies in each channel. We then show that the existence of zero-determinant
strategies in multichannel games requires the existence of zero-determinant
strategies in some channels. This result implies that the existence of
zero-determinant strategies in multichannel games is tightly restricted by
structure of games played in each channel.
\\ ( https://arxiv.org/abs/2505.21952 ,  14kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22032 (*cross-listing*)
Date: Wed, 28 May 2025 06:53:33 GMT   (7892kb)

Title: Retweets, Receipts, and Resistance: Discourse, Sentiment, and
  Credibility in Public Health Crisis Twitter
Authors: Tawfiq Ammari, Anna Gutowska, Jacob Ziff, Casey Randazzo, Harihan
  Subramonyam
Categories: cs.SI cs.HC
Comments: arXiv admin note: substantial text overlap with arXiv:2503.20262
\\
  As the COVID-19 pandemic evolved, the Centers for Disease Control and
Prevention (CDC) used Twitter to disseminate safety guidance and updates,
reaching millions of users. This study analyzes two years of tweets from, to,
and about the CDC using a mixed methods approach to examine discourse
characteristics, credibility, and user engagement. We found that the CDCs
communication remained largely one directional and did not foster reciprocal
interaction, while discussions around COVID19 were deeply shaped by political
and ideological polarization. Users frequently cited earlier CDC messages to
critique new and sometimes contradictory guidance. Our findings highlight the
role of sentiment, media richness, and source credibility in shaping the spread
of public health messages. We propose design strategies to help the CDC tailor
communications to diverse user groups and manage misinformation more
effectively during high-stakes health crises.
\\ ( https://arxiv.org/abs/2505.22032 ,  7892kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22142 (*cross-listing*)
Date: Wed, 28 May 2025 09:04:02 GMT   (86kb)

Title: Interpolation of Quantum Polar Codes and Quantum Reed-Muller Codes
Authors: Keita Hidaka, Dina Abdelhadi, Ruediger Urbanke
Categories: quant-ph cs.IT math.IT
\\
  Good quantum error-correcting codes that fulfill practical considerations,
such as simple encoding circuits and efficient decoders, are essential for
functional quantum information processing systems. Quantum polar codes satisfy
some of these requirements but lack certain critical features, thereby
hindering their widespread use. Existing constructions either require
entanglement assistance to produce valid quantum codes, suffer from poor
finite-size performance, or fail to tailor polar codes to the underlying
channel properties. Meanwhile, quantum Reed-Muller (RM) codes demonstrate
strong performance, though no known efficient decoding algorithm exists for
them. In this work, we propose strategies to interpolate between quantum polar
codes and quantum RM codes, thus addressing the challenges of designing valid
quantum polar codes without entanglement assistance and improving finite-size
code performance.
\\ ( https://arxiv.org/abs/2505.22142 ,  86kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22170 (*cross-listing*)
Date: Wed, 28 May 2025 09:41:10 GMT   (2302kb)

Title: Attention-Enhanced Prompt Decision Transformers for UAV-Assisted
  Communications with AoI
Authors: Chi Lu, Yiyang Ni, Zhe Wang, Xiaoli Shi, Jun Li, Shi Jin
Categories: eess.SP cs.IT math.IT
\\
  Decision Transformer (DT) has recently demonstrated strong generalizability
in dynamic resource allocation within unmanned aerial vehicle (UAV) networks,
compared to conventional deep reinforcement learning (DRL). However, its
performance is hindered due to zero-padding for varying state dimensions,
inability to manage long-term energy constraint, and challenges in acquiring
expert samples for few-shot fine-tuning in new scenarios. To overcome these
limitations, we propose an attention-enhanced prompt Decision Transformer
(APDT) framework to optimize trajectory planning and user scheduling, aiming to
minimize the average age of information (AoI) under long-term energy constraint
in UAV-assisted Internet of Things (IoT) networks. Specifically, we enhance the
convenional DT framework by incorporating an attention mechanism to accommodate
varying numbers of terrestrial users, introducing a prompt mechanism based on
short trajectory demonstrations for rapid adaptation to new scenarios, and
designing a token-assisted method to address the UAV's long-term energy
constraint. The APDT framework is first pre-trained on offline datasets and
then efficiently generalized to new scenarios. Simulations demonstrate that
APDT achieves twice faster in terms of convergence rate and reduces average AoI
by $8\%$ compared to conventional DT.
\\ ( https://arxiv.org/abs/2505.22170 ,  2302kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22565 (*cross-listing*)
Date: Wed, 28 May 2025 16:42:48 GMT   (248kb,A)

Title: The Ingleton inequality holds for metacyclic groups and fails for
  supersoluble groups
Authors: David A. Craven
Categories: math.GR cs.IT math.IT
Comments: 14 pages
\\
  The Ingleton inequality first appeared in matroid theory, where Ingleton
proved in 1971 that every rank function coming from a representable matroid on
four subsets satisfies a particular inequality. Because this inequality is not
implied by submodularity, Shannon-type axioms alone, it and various analogues
play a central role in separately linear and non-linear phenomena in a variety
of areas of mathematics. The Ingleton inequality for finite groups concerns the
various intersections of four subgroups. It holds for many quadruples of
subgroups of finite groups, but not all, the smallest example being four
subgroups of $S_5$, of order 120. Open questions are whether the Inlgeton
inequality always holds for metacycle and nilpotent groups. (There is a proof
in the literature due to Oggier and Stancu, but there is an already known issue
with their proof, which we address in this article.)
  In this paper we prove that the Ingleton inequality always holds for
metacycle groups, but that it fails for supersoluble groups, a class of groups
only a little larger than nilpotent groups. Although we do not resolve the
nilpotent case here we do make some reductions, and also prove that there are
no nilpotent violators of the Ingleton inequality of order less than 1024. We
end with a list of Ingleton inequality violating groups of order at most 1023.
  The article comes with a Magma package that allows reproduction of all
results in the paper and for the reader to check the Ingleton inequality for
any given finite group.
\\ ( https://arxiv.org/abs/2505.22565 ,  248kb)
------------------------------------------------------------------------------
\\
arXiv:2505.18478 (*cross-listing*)
Date: Sat, 24 May 2025 02:51:34 GMT   (1629kb)

Title: Provably Robust Training of Quantum Circuit Classifiers Against
  Parameter Noise
Authors: Lucas Tecot, Di Luo, Cho-Jui Hsieh
Categories: quant-ph cs.LG physics.comp-ph
Comments: 14 pages, 3 figures
\\
  Advancements in quantum computing have spurred significant interest in
harnessing its potential for speedups over classical systems. However, noise
remains a major obstacle to achieving reliable quantum algorithms. In this
work, we present a provably noise-resilient training theory and algorithm to
enhance the robustness of parameterized quantum circuit classifiers. Our
method, with a natural connection to Evolutionary Strategies, guarantees
resilience to parameter noise with minimal adjustments to commonly used
optimization algorithms. Our approach is function-agnostic and adaptable to
various quantum circuits, successfully demonstrated in quantum phase
classification tasks. By developing provably guaranteed optimization theory
with quantum circuits, our work opens new avenues for practical, robust
applications of near-term quantum computers.
\\ ( https://arxiv.org/abs/2505.18478 ,  1629kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20344 (*cross-listing*)
Date: Sun, 25 May 2025 03:59:00 GMT   (734kb)

Title: Genetic Influences on Brain Aging: Analyzing Sex Differences in the UK
  Biobank using Structural MRI
Authors: Karen Ardila, Aashka Mohite, Abdoljalil Addeh, Amanda V. Tyndall,
  Cindy K. Barha, Quan Long and M. Ethan MacDonald
Categories: q-bio.GN cs.LG
Comments: 7 pages, 5 figures, conference
Journal-ref: International Society for Magnetic Resonance in Medicine, ISMRM
  Annual Meeting, May 2025
\\
  Brain aging trajectories differ between males and females, yet the genetic
factors underlying these differences remain underexplored. Using structural MRI
and genotyping data from 40,940 UK Biobank participants (aged 45-83), we
computed Brain Age Gap Estimates (BrainAGE) for total brain, hippocampal, and
ventricular volumes. We conducted sex-stratified genome-wide association
studies (GWAS) and Post-GWAS analyses to identify genetic variants associated
with accelerated brain aging. Distinct gene sets emerged by sex: in females,
neurotransmitter transport and mitochondrial stress response genes were
implicated; in males, immune and inflammation-related genes dominated. Shared
genes, including GMNC and OSTN, were consistently linked to brain volumes
across sexes, suggesting core roles in neurostructural maintenance. Tissue
expression analyses revealed sex-specific enrichment in pathways tied to
neurodegeneration. These findings highlight the importance of sex-stratified
approaches in aging research and suggest genetic targets for personalized
interventions against age-related cognitive decline.
\\ ( https://arxiv.org/abs/2505.20344 ,  734kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21507 (*cross-listing*)
Date: Tue, 13 May 2025 07:07:24 GMT   (976kb)

Title: Automatic detection of abnormal clinical EEG: comparison of a finetuned
  foundation model with two deep learning models
Authors: Aurore Bussalb, Fran\c{c}ois Le Gac, Guillaume Jubien, Mohamed
  Rahmouni, Ruggero G. Bettinardi, Pedro Marinho R. de Oliveira, Phillipe
  Derambure, Nicolas Gaspard, Jacques Jonas, Louis Maillard, Laurent Vercueil,
  Herv\'e Vespignani, Philippe Laval, Laurent Koessler, Ulysse Gimenez
Categories: q-bio.NC cs.LG eess.SP
Comments: 20 pages, 7 figures
\\
  Electroencephalography (EEG) is commonly used by physicians for the diagnosis
of numerous neurological disorders. Due to the large volume of EEGs requiring
interpretation and the specific expertise involved, artificial
intelligence-based tools are being developed to assist in their visual
analysis. In this paper, we compare two deep learning models (CNN-LSTM and
Transformer-based) with BioSerenity-E1, a recently proposed foundation model,
in the task of classifying entire EEG recordings as normal or abnormal. The
three models were trained or finetuned on 2,500 EEG recordings and their
performances were evaluated on two private and one public datasets: a large
multicenter dataset annotated by a single specialist (dataset A composed of n =
4,480 recordings), a small multicenter dataset annotated by three specialists
(dataset B, n = 198), and the Temple University Abnormal (TUAB) EEG corpus
evaluation dataset (n = 276). On dataset A, the three models achieved at least
86% balanced accuracy, with BioSerenity-E1 finetuned achieving the highest
balanced accuracy (89.19% [88.36-90.41]). BioSerenity-E1 finetuned also
achieved the best performance on dataset B, with 94.63% [92.32-98.12] balanced
accuracy. The models were then validated on TUAB evaluation dataset, whose
corresponding training set was not used during training, where they achieved at
least 76% accuracy. Specifically, BioSerenity-E1 finetuned outperformed the
other two models, reaching an accuracy of 82.25% [78.27-87.48]. Our results
highlight the usefulness of leveraging pre-trained models for automatic EEG
classification: enabling robust and efficient interpretation of EEG data with
fewer resources and broader applicability.
\\ ( https://arxiv.org/abs/2505.21507 ,  976kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21524 (*cross-listing*)
Date: Fri, 23 May 2025 11:13:04 GMT   (48244kb)

Title: Learning Shared Representations from Unpaired Data
Authors: Amitai Yacobi, Nir Ben-Ari, Ronen Talmon and Uri Shaham
Categories: cs.CV cs.LG stat.ML
\\
  Learning shared representations is a primary area of multimodal
representation learning. The current approaches to achieve a shared embedding
space rely heavily on paired samples from each modality, which are
significantly harder to obtain than unpaired ones. In this work, we demonstrate
that shared representations can be learned almost exclusively from unpaired
data. Our arguments are grounded in the spectral embeddings of the random walk
matrices constructed independently from each unimodal representation. Empirical
results in computer vision and natural language processing domains support its
potential, revealing the effectiveness of unpaired data in capturing meaningful
cross-modal relations, demonstrating high capabilities in retrieval tasks,
generation, arithmetics, zero-shot, and cross-domain classification. This work,
to the best of our knowledge, is the first to demonstrate these capabilities
almost exclusively from unpaired samples, giving rise to a cross-modal
embedding that could be viewed as universal, i.e., independent of the specific
modalities of the data. Our code IS publicly available at
https://github.com/shaham-lab/SUE.
\\ ( https://arxiv.org/abs/2505.21524 ,  48244kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21533 (*cross-listing*)
Date: Fri, 23 May 2025 20:12:07 GMT   (6181kb)

Title: Self-Organizing Visual Prototypes for Non-Parametric Representation
  Learning
Authors: Thalles Silva, Helio Pedrini, Ad\'in Ram\'irez Rivera
Categories: cs.CV cs.LG
Comments: Accepted at ICML 2025, code at https://github.com/sthalles/sop
\\
  We present Self-Organizing Visual Prototypes (SOP), a new training technique
for unsupervised visual feature learning. Unlike existing prototypical
self-supervised learning (SSL) methods that rely on a single prototype to
encode all relevant features of a hidden cluster in the data, we propose the
SOP strategy. In this strategy, a prototype is represented by many semantically
similar representations, or support embeddings (SEs), each containing a
complementary set of features that together better characterize their region in
space and maximize training performance. We reaffirm the feasibility of
non-parametric SSL by introducing novel non-parametric adaptations of two loss
functions that implement the SOP strategy. Notably, we introduce the SOP Masked
Image Modeling (SOP-MIM) task, where masked representations are reconstructed
from the perspective of multiple non-parametric local SEs. We comprehensively
evaluate the representations learned using the SOP strategy on a range of
benchmarks, including retrieval, linear evaluation, fine-tuning, and object
detection. Our pre-trained encoders achieve state-of-the-art performance on
many retrieval benchmarks and demonstrate increasing performance gains with
more complex encoders.
\\ ( https://arxiv.org/abs/2505.21533 ,  6181kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21545 (*cross-listing*)
Date: Sat, 24 May 2025 20:11:14 GMT   (19426kb,D)

Title: Corruption-Aware Training of Latent Video Diffusion Models for Robust
  Text-to-Video Generation
Authors: Chika Maduabuchi, Hao Chen, Yujin Han, Jindong Wang
Categories: cs.CV cs.LG
Comments: Code: https://github.com/chikap421/catlvdm Models:
  https://huggingface.co/Chikap421/catlvdm-checkpoints/tree/main
\\
  Latent Video Diffusion Models (LVDMs) achieve high-quality generation but are
sensitive to imperfect conditioning, which causes semantic drift and temporal
incoherence on noisy, web-scale video-text datasets. We introduce CAT-LVDM, the
first corruption-aware training framework for LVDMs that improves robustness
through structured, data-aligned noise injection. Our method includes
Batch-Centered Noise Injection (BCNI), which perturbs embeddings along
intra-batch semantic directions to preserve temporal consistency. BCNI is
especially effective on caption-rich datasets like WebVid-2M, MSR-VTT, and
MSVD. We also propose Spectrum-Aware Contextual Noise (SACN), which injects
noise along dominant spectral directions to improve low-frequency smoothness,
showing strong results on UCF-101. On average, BCNI reduces FVD by 31.9% across
WebVid-2M, MSR-VTT, and MSVD, while SACN yields a 12.3% improvement on UCF-101.
Ablation studies confirm the benefit of low-rank, data-aligned noise. Our
theoretical analysis further explains how such perturbations tighten entropy,
Wasserstein, score-drift, mixing-time, and generalization bounds. CAT-LVDM
establishes a principled, scalable training approach for robust video diffusion
under multimodal noise. Code and models: https://github.com/chikap421/catlvdm
\\ ( https://arxiv.org/abs/2505.21545 ,  19426kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21561 (*cross-listing*)
Date: Tue, 27 May 2025 02:01:45 GMT   (1249kb)

Title: Knowledge Distillation Approach for SOS Fusion Staging: Towards Fully
  Automated Skeletal Maturity Assessment
Authors: Omid Halimi Milani, Amanda Nikho, Marouane Tliba, Lauren Mills, Ahmet
  Enis Cetin, Mohammed H Elnagar
Categories: cs.CV cs.LG
Comments: This paper has been accepted to the CVPR Workshop 2025, to be held in
  Nashville, Tennessee
\\
  We introduce a novel deep learning framework for the automated staging of
spheno-occipital synchondrosis (SOS) fusion, a critical diagnostic marker in
both orthodontics and forensic anthropology. Our approach leverages a
dual-model architecture wherein a teacher model, trained on manually cropped
images, transfers its precise spatial understanding to a student model that
operates on full, uncropped images. This knowledge distillation is facilitated
by a newly formulated loss function that aligns spatial logits as well as
incorporates gradient-based attention spatial mapping, ensuring that the
student model internalizes the anatomically relevant features without relying
on external cropping or YOLO-based segmentation. By leveraging expert-curated
data and feedback at each step, our framework attains robust diagnostic
accuracy, culminating in a clinically viable end-to-end pipeline. This
streamlined approach obviates the need for additional pre-processing tools and
accelerates deployment, thereby enhancing both the efficiency and consistency
of skeletal maturation assessment in diverse clinical settings.
\\ ( https://arxiv.org/abs/2505.21561 ,  1249kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21564 (*cross-listing*)
Date: Tue, 27 May 2025 04:10:28 GMT   (1775kb)

Title: Multi-instance Learning as Downstream Task of Self-Supervised
  Learning-based Pre-trained Model
Authors: Koki Matsuishi and Tsuyoshi Okita
Categories: cs.CV cs.LG
Comments: 8 pages, 6 figures
\\
  In deep multi-instance learning, the number of applicable instances depends
on the data set. In histopathology images, deep learning multi-instance
learners usually assume there are hundreds to thousands instances in a bag.
However, when the number of instances in a bag increases to 256 in brain
hematoma CT, learning becomes extremely difficult. In this paper, we address
this drawback. To overcome this problem, we propose using a pre-trained model
with self-supervised learning for the multi-instance learner as a downstream
task. With this method, even when the original target task suffers from the
spurious correlation problem, we show improvements of 5% to 13% in accuracy and
40% to 55% in the F1 measure for the hypodensity marker classification of brain
hematoma CT.
\\ ( https://arxiv.org/abs/2505.21564 ,  1775kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21566 (*cross-listing*)
Date: Tue, 27 May 2025 05:04:50 GMT   (2559kb)

Title: Diffusion Model-based Activity Completion for AI Motion Capture from
  Videos
Authors: Gao Huayu, Huang Tengjiu, Ye Xiaolong, Tsuyoshi Okita
Categories: cs.CV cs.LG
Comments: 32 pages, 16 figures
\\
  AI-based motion capture is an emerging technology that offers a
cost-effective alternative to traditional motion capture systems. However,
current AI motion capture methods rely entirely on observed video sequences,
similar to conventional motion capture. This means that all human actions must
be predefined, and movements outside the observed sequences are not possible.
To address this limitation, we aim to apply AI motion capture to virtual
humans, where flexible actions beyond the observed sequences are required. We
assume that while many action fragments exist in the training data, the
transitions between them may be missing. To bridge these gaps, we propose a
diffusion-model-based action completion technique that generates complementary
human motion sequences, ensuring smooth and continuous movements. By
introducing a gate module and a position-time embedding module, our approach
achieves competitive results on the Human3.6M dataset. Our experimental results
show that (1) MDC-Net outperforms existing methods in ADE, FDE, and MMADE but
is slightly less accurate in MMFDE, (2) MDC-Net has a smaller model size
(16.84M) compared to HumanMAC (28.40M), and (3) MDC-Net generates more natural
and coherent motion sequences. Additionally, we propose a method for extracting
sensor data, including acceleration and angular velocity, from human motion
sequences.
\\ ( https://arxiv.org/abs/2505.21566 ,  2559kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21567 (*cross-listing*)
Date: Tue, 27 May 2025 05:42:21 GMT   (782kb)

Title: EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models
Authors: Feng Jiang, Zihao Zheng, Xiuping Cui, Maoliang Li, JIayu Chen, Xiang
  Chen
Categories: cs.CV cs.LG
\\
  With the development of Embodied Artificial intelligence, the end-to-end
control policy such as Vision-Language-Action (VLA) model has become the
mainstream. Existing VLA models faces expensive computing/storage cost, which
need to be optimized. Quantization is considered as the most effective method
which can not only reduce the memory cost but also achieve computation
acceleration. However, we find the token alignment of VLA models hinders the
application of existing quantization methods. To address this, we proposed an
optimized framework called EaqVLA, which apply encoding-aligned quantization to
VLA models. Specifically, we propose an complete analysis method to find the
misalignment in various granularity. Based on the analysis results, we propose
a mixed precision quantization with the awareness of encoding alignment.
Experiments shows that the porposed EaqVLA achieves better quantization
performance (with the minimal quantization loss for end-to-end action control
and xxx times acceleration) than existing quantization methods.
\\ ( https://arxiv.org/abs/2505.21567 ,  782kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21574 (*cross-listing*)
Date: Tue, 27 May 2025 07:27:03 GMT   (175kb,D)

Title: Do We Need All the Synthetic Data? Towards Targeted Synthetic Image
  Augmentation via Diffusion Models
Authors: Dang Nguyen, Jiping Li, Jinghao Zheng, Baharan Mirzasoleiman
Categories: cs.CV cs.LG
\\
  Synthetically augmenting training datasets with diffusion models has been an
effective strategy for improving generalization of image classifiers. However,
existing techniques struggle to ensure the diversity of generation and increase
the size of the data by up to 10-30x to improve the in-distribution
performance. In this work, we show that synthetically augmenting part of the
data that is not learned early in training outperforms augmenting the entire
dataset. By analyzing a two-layer CNN, we prove that this strategy improves
generalization by promoting homogeneity in feature learning speed without
amplifying noise. Our extensive experiments show that by augmenting only
30%-40% of the data, our method boosts the performance by up to 2.8% in a
variety of scenarios, including training ResNet, ViT and DenseNet on CIFAR-10,
CIFAR-100, and TinyImageNet, with a range of optimizers including SGD and SAM.
Notably, our method applied with SGD outperforms the SOTA optimizer, SAM, on
CIFAR-100 and TinyImageNet. It can also easily stack with existing weak and
strong augmentation strategies to further boost the performance.
\\ ( https://arxiv.org/abs/2505.21574 ,  175kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21580 (*cross-listing*)
Date: Tue, 27 May 2025 09:06:28 GMT   (602kb,D)

Title: A Kernelised Stein Discrepancy for Assessing the Fit of Inhomogeneous
  Random Graph Models
Authors: Anum Fatima, Gesine Reinert
Categories: stat.ML cs.LG math.ST stat.TH
Comments: 43 pages, 24 figures
\\
  Complex data are often represented as a graph, which in turn can often be
viewed as a realisation of a random graph, such as of an inhomogeneous random
graph model (IRG). For general fast goodness-of-fit tests in high dimensions,
kernelised Stein discrepancy (KSD) tests are a powerful tool. Here, we develop,
test, and analyse a KSD-type goodness-of-fit test for IRG models that can be
carried out with a single observation of the network. The test is applicable to
a network of any size and does not depend on the asymptotic distribution of the
test statistic. We also provide theoretical guarantees.
\\ ( https://arxiv.org/abs/2505.21580 ,  602kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21592 (*cross-listing*)
Date: Tue, 27 May 2025 14:29:35 GMT   (4141kb)

Title: Taylor expansion-based Kolmogorov-Arnold network for blind image quality
  assessment
Authors: Ze Chen, Shaode Yu
Categories: eess.IV cs.CV cs.LG
Comments: under review
\\
  Kolmogorov-Arnold Network (KAN) has attracted growing interest for its strong
function approximation capability. In our previous work, KAN and its variants
were explored in score regression for blind image quality assessment (BIQA).
However, these models encounter challenges when processing high-dimensional
features, leading to limited performance gains and increased computational
cost. To address these issues, we propose TaylorKAN that leverages the Taylor
expansions as learnable activation functions to enhance local approximation
capability. To improve the computational efficiency, network depth reduction
and feature dimensionality compression are integrated into the TaylorKAN-based
score regression pipeline. On five databases (BID, CLIVE, KonIQ, SPAQ, and
FLIVE) with authentic distortions, extensive experiments demonstrate that
TaylorKAN consistently outperforms the other KAN-related models, indicating
that the local approximation via Taylor expansions is more effective than
global approximation using orthogonal functions. Its generalization capacity is
validated through inter-database experiments. The findings highlight the
potential of TaylorKAN as an efficient and robust model for high-dimensional
score regression.
\\ ( https://arxiv.org/abs/2505.21592 ,  4141kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21597 (*cross-listing*)
Date: Tue, 27 May 2025 16:53:58 GMT   (895kb,D)

Title: Optimizing Deep Learning for Skin Cancer Classification: A
  Computationally Efficient CNN with Minimal Accuracy Trade-Off
Authors: Abdullah Al Mamun, Pollob Chandra Ray, Md Rahat Ul Nasib, Akash Das,
  Jia Uddin, Md Nurul Absur
Categories: eess.IV cs.CV cs.LG
Comments: 6 pages, & 7 Images
\\
  The rapid advancement of deep learning in medical image analysis has greatly
enhanced the accuracy of skin cancer classification. However, current
state-of-the-art models, especially those based on transfer learning like
ResNet50, come with significant computational overhead, rendering them
impractical for deployment in resource-constrained environments. This study
proposes a custom CNN model that achieves a 96.7\% reduction in parameters
(from 23.9 million in ResNet50 to 692,000) while maintaining a classification
accuracy deviation of less than 0.022\%. Our empirical analysis of the HAM10000
dataset reveals that although transfer learning models provide a marginal
accuracy improvement of approximately 0.022\%, they result in a staggering
13,216.76\% increase in FLOPs, considerably raising computational costs and
inference latency. In contrast, our lightweight CNN architecture, which
encompasses only 30.04 million FLOPs compared to ResNet50's 4.00 billion,
significantly reduces energy consumption, memory footprint, and inference time.
These findings underscore the trade-off between the complexity of deep models
and their real-world feasibility, positioning our optimized CNN as a practical
solution for mobile and edge-based skin cancer diagnostics.
\\ ( https://arxiv.org/abs/2505.21597 ,  895kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21647 (*cross-listing*)
Date: Tue, 27 May 2025 18:21:48 GMT   (2872kb)

Title: QuARI: Query Adaptive Retrieval Improvement
Authors: Eric Xing and Abby Stylianou and Robert Pless and Nathan Jacobs
Categories: cs.CV cs.LG
Comments: 13 pages, 4 figures, 4 tables
\\
  Massive-scale pretraining has made vision-language models increasingly
popular for image-to-image and text-to-image retrieval across a broad
collection of domains. However, these models do not perform well when used for
challenging retrieval tasks, such as instance retrieval in very large-scale
image collections. Recent work has shown that linear transformations of VLM
features trained for instance retrieval can improve performance by emphasizing
subspaces that relate to the domain of interest. In this paper, we explore a
more extreme version of this specialization by learning to map a given query to
a query-specific feature space transformation. Because this transformation is
linear, it can be applied with minimal computational cost to millions of image
embeddings, making it effective for large-scale retrieval or re-ranking.
Results show that this method consistently outperforms state-of-the-art
alternatives, including those that require many orders of magnitude more
computation at query time.
\\ ( https://arxiv.org/abs/2505.21647 ,  2872kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21658 (*cross-listing*)
Date: Tue, 27 May 2025 18:32:54 GMT   (7740kb)

Title: STACI: Spatio-Temporal Aleatoric Conformal Inference
Authors: Brandon R. Feng, David Keetae Park, Xihaier Luo, Arantxa Urdangarin,
  Shinjae Yoo, Brian J. Reich
Categories: stat.ML cs.LG stat.ME
\\
  Fitting Gaussian Processes (GPs) provides interpretable aleatoric uncertainty
quantification for estimation of spatio-temporal fields. Spatio-temporal deep
learning models, while scalable, typically assume a simplistic independent
covariance matrix for the response, failing to capture the underlying
correlation structure. However, spatio-temporal GPs suffer from issues of
scalability and various forms of approximation bias resulting from restrictive
assumptions of the covariance kernel function. We propose STACI, a novel
framework consisting of a variational Bayesian neural network approximation of
non-stationary spatio-temporal GP along with a novel spatio-temporal conformal
inference algorithm. STACI is highly scalable, taking advantage of GPU training
capabilities for neural network models, and provides statistically valid
prediction intervals for uncertainty quantification. STACI outperforms
competing GPs and deep methods in accurately approximating spatio-temporal
processes and we show it easily scales to datasets with millions of
observations.
\\ ( https://arxiv.org/abs/2505.21658 ,  7740kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21686 (*cross-listing*)
Date: Tue, 27 May 2025 19:16:20 GMT   (17300kb)

Title: tenSVD algorithm for compression
Authors: Michele Gallo
Categories: stat.CO cs.CV cs.LG
\\
  Tensors provide a robust framework for managing high-dimensional data.
Consequently, tensor analysis has emerged as an active research area in various
domains, including machine learning, signal processing, computer vision, graph
analysis, and data mining. This study introduces an efficient image storage
approach utilizing tensors, aiming to minimize memory to store, bandwidth to
transmit and energy to processing. The proposed method organizes original data
into a higher-order tensor and applies the Tucker model for compression.
Implemented in R, this method is compared to a baseline algorithm. The
evaluation focuses on efficient of algorithm measured in term of computational
time and the quality of information preserved, using both simulated and real
datasets. A detailed analysis of the results is conducted, employing
established quantitative metrics, with significant attention paid to
sustainability in terms of energy consumption across algorithms.
\\ ( https://arxiv.org/abs/2505.21686 ,  17300kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21692 (*cross-listing*)
Date: Tue, 27 May 2025 19:28:22 GMT   (1491kb)

Title: What Data Enables Optimal Decisions? An Exact Characterization for
  Linear Optimization
Authors: Omar Bennouna, Amine Bennouna, Saurabh Amin, Asuman Ozdaglar
Categories: math.OC cs.LG
\\
  We study the fundamental question of how informative a dataset is for solving
a given decision-making task. In our setting, the dataset provides partial
information about unknown parameters that influence task outcomes. Focusing on
linear programs, we characterize when a dataset is sufficient to recover an
optimal decision, given an uncertainty set on the cost vector. Our main
contribution is a sharp geometric characterization that identifies the
directions of the cost vector that matter for optimality, relative to the task
constraints and uncertainty set. We further develop a practical algorithm that,
for a given task, constructs a minimal or least-costly sufficient dataset. Our
results reveal that small, well-chosen datasets can often fully determine
optimal decisions -- offering a principled foundation for task-aware data
selection.
\\ ( https://arxiv.org/abs/2505.21692 ,  1491kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21706 (*cross-listing*)
Date: Tue, 27 May 2025 19:43:32 GMT   (48kb)

Title: Network classification through random walks
Authors: Gonzalo Travieso, Joao Merenda, Odemir M. Bruno
Categories: cs.SI cs.LG physics.soc-ph
Comments: 11 pages, 2 figures
\\
  Network models have been widely used to study diverse systems and analyze
their dynamic behaviors. Given the structural variability of networks, an
intriguing question arises: Can we infer the type of system represented by a
network based on its structure? This classification problem involves extracting
relevant features from the network. Existing literature has proposed various
methods that combine structural measurements and dynamical processes for
feature extraction. In this study, we introduce a novel approach to
characterize networks using statistics from random walks, which can be
particularly informative about network properties. We present the employed
statistical metrics and compare their performance on multiple datasets with
other state-of-the-art feature extraction methods. Our results demonstrate that
the proposed method is effective in many cases, often outperforming existing
approaches, although some limitations are observed across certain datasets.
\\ ( https://arxiv.org/abs/2505.21706 ,  48kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21721 (*cross-listing*)
Date: Tue, 27 May 2025 20:08:28 GMT   (590kb)

Title: Nearly Dimension-Independent Convergence of Mean-Field Black-Box
  Variational Inference
Authors: Kyurae Kim, Yi-An Ma, Trevor Campbell, Jacob R. Gardner
Categories: stat.ML cs.LG math.OC stat.CO
\\
  We prove that, given a mean-field location-scale variational family,
black-box variational inference (BBVI) with the reparametrization gradient
converges at an almost dimension-independent rate. Specifically, for strongly
log-concave and log-smooth targets, the number of iterations for BBVI with a
sub-Gaussian family to achieve an objective $\epsilon$-close to the global
optimum is $\mathrm{O}(\log d)$, which improves over the $\mathrm{O}(d)$
dependence of full-rank location-scale families. For heavy-tailed families, we
provide a weaker $\mathrm{O}(d^{2/k})$ dimension dependence, where $k$ is the
number of finite moments. Additionally, if the Hessian of the target
log-density is constant, the complexity is free of any explicit dimension
dependence. We also prove that our bound on the gradient variance, which is key
to our result, cannot be improved using only spectral bounds on the Hessian of
the target log-density.
\\ ( https://arxiv.org/abs/2505.21721 ,  590kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21723 (*cross-listing*)
Date: Tue, 27 May 2025 20:11:21 GMT   (16627kb)

Title: Are Statistical Methods Obsolete in the Era of Deep Learning?
Authors: Skyler Wu, Shihao Yang, S. C. Kou
Categories: stat.CO cs.LG stat.ML
Comments: 35 pages, 11 figures (main text)
\\
  In the era of AI, neural networks have become increasingly popular for
modeling, inference, and prediction, largely due to their potential for
universal approximation. With the proliferation of such deep learning models, a
question arises: are leaner statistical methods still relevant? To shed insight
on this question, we employ the mechanistic nonlinear ordinary differential
equation (ODE) inverse problem as a testbed, using physics-informed neural
network (PINN) as a representative of the deep learning paradigm and
manifold-constrained Gaussian process inference (MAGI) as a representative of
statistically principled methods. Through case studies involving the SEIR model
from epidemiology and the Lorenz model from chaotic dynamics, we demonstrate
that statistical methods are far from obsolete, especially when working with
sparse and noisy observations. On tasks such as parameter inference and
trajectory reconstruction, statistically principled methods consistently
achieve lower bias and variance, while using far fewer parameters and requiring
less hyperparameter tuning. Statistical methods can also decisively outperform
deep learning models on out-of-sample future prediction, where the absence of
relevant data often leads overparameterized models astray. Additionally, we
find that statistically principled approaches are more robust to accumulation
of numerical imprecision and can represent the underlying system more faithful
to the true governing ODEs.
\\ ( https://arxiv.org/abs/2505.21723 ,  16627kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21736 (*cross-listing*)
Date: Tue, 27 May 2025 20:27:00 GMT   (2483kb)

Title: Moment kernels: a simple and scalable approach for equivariance to
  rotations and reflections in deep convolutional networks
Authors: Zachary Schlamowitz, Andrew Bennecke, Daniel J. Tward
Categories: cs.CV cs.LG
\\
  The principle of translation equivariance (if an input image is translated an
output image should be translated by the same amount), led to the development
of convolutional neural networks that revolutionized machine vision. Other
symmetries, like rotations and reflections, play a similarly critical role,
especially in biomedical image analysis, but exploiting these symmetries has
not seen wide adoption. We hypothesize that this is partially due to the
mathematical complexity of methods used to exploit these symmetries, which
often rely on representation theory, a bespoke concept in differential geometry
and group theory. In this work, we show that the same equivariance can be
achieved using a simple form of convolution kernels that we call ``moment
kernels,'' and prove that all equivariant kernels must take this form. These
are a set of radially symmetric functions of a spatial position $x$, multiplied
by powers of the components of $x$ or the identity matrix. We implement
equivariant neural networks using standard convolution modules, and provide
architectures to execute several biomedical image analysis tasks that depend on
equivariance principles: classification (outputs are invariant under orthogonal
transforms), 3D image registration (outputs transform like a vector), and cell
segmentation (quadratic forms defining ellipses transform like a matrix).
\\ ( https://arxiv.org/abs/2505.21736 ,  2483kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21742 (*cross-listing*)
Date: Tue, 27 May 2025 20:32:28 GMT   (44037kb)

Title: What is Adversarial Training for Diffusion Models?
Authors: Briglia Maria Rosaria, Mujtaba Hussain Mirza, Giuseppe Lisanti, Iacopo
  Masi
Categories: cs.CV cs.LG
Comments: 40 pages
\\
  We answer the question in the title, showing that adversarial training (AT)
for diffusion models (DMs) fundamentally differs from classifiers: while AT in
classifiers enforces output invariance, AT in DMs requires equivariance to keep
the diffusion process aligned with the data distribution. AT is a way to
enforce smoothness in the diffusion flow, improving robustness to outliers and
corrupted data. Unlike prior art, our method makes no assumptions about the
noise model and integrates seamlessly into diffusion training by adding random
noise, similar to randomized smoothing, or adversarial noise, akin to AT. This
enables intrinsic capabilities such as handling noisy data, dealing with
extreme variability such as outliers, preventing memorization, and improving
robustness. We rigorously evaluate our approach with proof-of-concept datasets
with known distributions in low- and high-dimensional space, thereby taking a
perfect measure of errors; we further evaluate on standard benchmarks such as
CIFAR-10, CelebA and LSUN Bedroom, showing strong performance under severe
noise, data corruption, and iterative adversarial attacks.
\\ ( https://arxiv.org/abs/2505.21742 ,  44037kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21767 (*cross-listing*)
Date: Tue, 27 May 2025 21:00:46 GMT   (3357kb)

Title: Beyond 1D: Vision Transformers and Multichannel Signal Images for
  PPG-to-ECG Reconstruction
Authors: Xiaoyan Li and Shixin Xu and Faisal Habib and Arvind Gupta and
  Huaxiong Huang
Categories: eess.IV cs.LG eess.SP
\\
  Reconstructing ECG from PPG is a promising yet challenging task. While recent
advancements in generative models have significantly improved ECG
reconstruction, accurately capturing fine-grained waveform features remains a
key challenge. To address this, we propose a novel PPG-to-ECG reconstruction
method that leverages a Vision Transformer (ViT) as the core network. Unlike
conventional approaches that rely on single-channel PPG, our method employs a
four-channel signal image representation, incorporating the original PPG, its
first-order difference, second-order difference, and area under the curve. This
multi-channel design enriches feature extraction by preserving both temporal
and physiological variations within the PPG. By leveraging the self-attention
mechanism in ViT, our approach effectively captures both inter-beat and
intra-beat dependencies, leading to more robust and accurate ECG
reconstruction. Experimental results demonstrate that our method consistently
outperforms existing 1D convolution-based approaches, achieving up to 29%
reduction in PRD and 15% reduction in RMSE. The proposed approach also produces
improvements in other evaluation metrics, highlighting its robustness and
effectiveness in reconstructing ECG signals. Furthermore, to ensure a
clinically relevant evaluation, we introduce new performance metrics, including
QRS area error, PR interval error, RT interval error, and RT amplitude
difference error. Our findings suggest that integrating a four-channel signal
image representation with the self-attention mechanism of ViT enables more
effective extraction of informative PPG features and improved modeling of
beat-to-beat variations for PPG-to-ECG mapping. Beyond demonstrating the
potential of PPG as a viable alternative for heart activity monitoring, our
approach opens new avenues for cyclic signal analysis and prediction.
\\ ( https://arxiv.org/abs/2505.21767 ,  3357kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21791 (*cross-listing*)
Date: Tue, 27 May 2025 21:46:27 GMT   (9762kb,D)

Title: Global Minimizers of $\ell^p$-Regularized Objectives Yield the Sparsest
  ReLU Neural Networks
Authors: Julia Nakhleh, Robert D. Nowak
Categories: stat.ML cs.LG
\\
  Overparameterized neural networks can interpolate a given dataset in many
different ways, prompting the fundamental question: which among these solutions
should we prefer, and what explicit regularization strategies will provably
yield these solutions? This paper addresses the challenge of finding the
sparsest interpolating ReLU network -- i.e., the network with the fewest
nonzero parameters or neurons -- a goal with wide-ranging implications for
efficiency, generalization, interpretability, theory, and model compression.
Unlike post hoc pruning approaches, we propose a continuous, almost-everywhere
differentiable training objective whose global minima are guaranteed to
correspond to the sparsest single-hidden-layer ReLU networks that fit the data.
This result marks a conceptual advance: it recasts the combinatorial problem of
sparse interpolation as a smooth optimization task, potentially enabling the
use of gradient-based training methods. Our objective is based on minimizing
$\ell^p$ quasinorms of the weights for $0 < p < 1$, a classical
sparsity-promoting strategy in finite-dimensional settings. However, applying
these ideas to neural networks presents new challenges: the function class is
infinite-dimensional, and the weights are learned using a highly nonconvex
objective. We prove that, under our formulation, global minimizers correspond
exactly to sparsest solutions. Our work lays a foundation for understanding
when and how continuous sparsity-inducing objectives can be leveraged to
recover sparse networks through training.
\\ ( https://arxiv.org/abs/2505.21791 ,  9762kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21796 (*cross-listing*)
Date: Tue, 27 May 2025 21:58:35 GMT   (43kb)

Title: A General-Purpose Theorem for High-Probability Bounds of Stochastic
  Approximation with Polyak Averaging
Authors: Sajad Khodadadian, Martin Zubeldia
Categories: stat.ML cs.LG math.PR
Comments: 37 pages
\\
  Polyak-Ruppert averaging is a widely used technique to achieve the optimal
asymptotic variance of stochastic approximation (SA) algorithms, yet its
high-probability performance guarantees remain underexplored in general
settings. In this paper, we present a general framework for establishing
non-asymptotic concentration bounds for the error of averaged SA iterates. Our
approach assumes access to individual concentration bounds for the unaveraged
iterates and yields a sharp bound on the averaged iterates. We also construct
an example, showing the tightness of our result up to constant multiplicative
factors. As direct applications, we derive tight concentration bounds for
contractive SA algorithms and for algorithms such as temporal difference
learning and Q-learning with averaging, obtaining new bounds in settings where
traditional analysis is challenging.
\\ ( https://arxiv.org/abs/2505.21796 ,  43kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21799 (*cross-listing*)
Date: Tue, 27 May 2025 22:11:21 GMT   (10052kb)

Title: PolarGrad: A Class of Matrix-Gradient Optimizers from a Unifying
  Preconditioning Perspective
Authors: Tim Tsz-Kit Lau, Qi Long, Weijie Su
Categories: math.OC cs.LG stat.ML
\\
  The ever-growing scale of deep learning models and datasets underscores the
critical importance of efficient optimization methods. While preconditioned
gradient methods such as Adam and AdamW are the de facto optimizers for
training neural networks and large language models, structure-aware
preconditioned optimizers like Shampoo and Muon, which utilize the matrix
structure of gradients, have demonstrated promising evidence of faster
convergence. In this paper, we introduce a unifying framework for analyzing
"matrix-aware" preconditioned methods, which not only sheds light on the
effectiveness of Muon and related optimizers but also leads to a class of new
structure-aware preconditioned methods. A key contribution of this framework is
its precise distinction between preconditioning strategies that treat neural
network weights as vectors (addressing curvature anisotropy) versus those that
consider their matrix structure (addressing gradient anisotropy). This
perspective provides new insights into several empirical phenomena in language
model pre-training, including Adam's training instabilities, Muon's accelerated
convergence, and the necessity of learning rate warmup for Adam. Building upon
this framework, we introduce PolarGrad, a new class of preconditioned
optimization methods based on the polar decomposition of matrix-valued
gradients. As a special instance, PolarGrad includes Muon with updates scaled
by the nuclear norm of the gradients. We provide numerical implementations of
these methods, leveraging efficient numerical polar decomposition algorithms
for enhanced convergence. Our extensive evaluations across diverse matrix
optimization problems and language model pre-training tasks demonstrate that
PolarGrad outperforms both Adam and Muon.
\\ ( https://arxiv.org/abs/2505.21799 ,  10052kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21809 (*cross-listing*)
Date: Tue, 27 May 2025 22:30:56 GMT   (830kb)

Title: Voice Quality Dimensions as Interpretable Primitives for Speaking Style
  for Atypical Speech and Affect
Authors: Jaya Narain, Vasudha Kowtha, Colin Lea, Lauren Tooley, Dianna Yee,
  Vikramjit Mitra, Zifang Huang, Miquel Espi Marques, Jon Huang, Carlos
  Avendano, Shirley Ren
Categories: cs.SD cs.LG eess.AS
Comments: accepted for Interspeech 2025
\\
  Perceptual voice quality dimensions describe key characteristics of atypical
speech and other speech modulations. Here we develop and evaluate voice quality
models for seven voice and speech dimensions (intelligibility, imprecise
consonants, harsh voice, naturalness, monoloudness, monopitch, and
breathiness). Probes were trained on the public Speech Accessibility (SAP)
project dataset with 11,184 samples from 434 speakers, using embeddings from
frozen pre-trained models as features. We found that our probes had both strong
performance and strong generalization across speech elicitation categories in
the SAP dataset. We further validated zero-shot performance on additional
datasets, encompassing unseen languages and tasks: Italian atypical speech,
English atypical speech, and affective speech. The strong zero-shot performance
and the interpretability of results across an array of evaluations suggests the
utility of using voice quality dimensions in speaking style-related tasks.
\\ ( https://arxiv.org/abs/2505.21809 ,  830kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21819 (*cross-listing*)
Date: Tue, 27 May 2025 23:02:54 GMT   (41kb)

Title: Representative Language Generation
Authors: Charlotte Peale, Vinod Raman, Omer Reingold
Categories: cs.CL cs.LG
Comments: Accepted to ICML 2025
\\
  We introduce "representative generation," extending the theoretical framework
for generation proposed by Kleinberg et al. (2024) and formalized by Li et al.
(2024), to additionally address diversity and bias concerns in generative
models. Our notion requires outputs of a generative model to proportionally
represent groups of interest from the training data. We characterize
representative uniform and non-uniform generation, introducing the "group
closure dimension" as a key combinatorial quantity. For representative
generation in the limit, we analyze both information-theoretic and
computational aspects, demonstrating feasibility for countably infinite
hypothesis classes and collections of groups under certain conditions, but
proving a negative result for computability using only membership queries. This
contrasts with Kleinberg et al.'s (2024) positive results for standard
generation in the limit. Our findings provide a rigorous foundation for
developing more diverse and representative generative models.
\\ ( https://arxiv.org/abs/2505.21819 ,  41kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21837 (*cross-listing*)
Date: Wed, 28 May 2025 00:03:39 GMT   (13336kb)

Title: UniMoGen: Universal Motion Generation
Authors: Aliasghar Khani, Arianna Rampini, Evan Atherton, Bruno Roy
Categories: cs.CV cs.LG
\\
  Motion generation is a cornerstone of computer graphics, animation, gaming,
and robotics, enabling the creation of realistic and varied character
movements. A significant limitation of existing methods is their reliance on
specific skeletal structures, which restricts their versatility across
different characters. To overcome this, we introduce UniMoGen, a novel
UNet-based diffusion model designed for skeleton-agnostic motion generation.
UniMoGen can be trained on motion data from diverse characters, such as humans
and animals, without the need for a predefined maximum number of joints. By
dynamically processing only the necessary joints for each character, our model
achieves both skeleton agnosticism and computational efficiency. Key features
of UniMoGen include controllability via style and trajectory inputs, and the
ability to continue motions from past frames. We demonstrate UniMoGen's
effectiveness on the 100style dataset, where it outperforms state-of-the-art
methods in diverse character motion generation. Furthermore, when trained on
both the 100style and LAFAN1 datasets, which use different skeletons, UniMoGen
achieves high performance and improved efficiency across both skeletons. These
results highlight UniMoGen's potential to advance motion generation by
providing a flexible, efficient, and controllable solution for a wide range of
character animations.
\\ ( https://arxiv.org/abs/2505.21837 ,  13336kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21842 (*cross-listing*)
Date: Wed, 28 May 2025 00:21:49 GMT   (1984kb)

Title: A Physics-Informed Learning Framework to Solve the Infinite-Horizon
  Optimal Control Problem
Authors: Filippos Fotiadis, Kyriakos G. Vamvoudakis
Categories: eess.SY cs.LG cs.SY
Comments: Accepted with minor revisions at International Journal of Robust and
  Nonlinear Control
\\
  We propose a physics-informed neural networks (PINNs) framework to solve the
infinite-horizon optimal control problem of nonlinear systems. In particular,
since PINNs are generally able to solve a class of partial differential
equations (PDEs), they can be employed to learn the value function of the
infinite-horizon optimal control problem via solving the associated
steady-state Hamilton-Jacobi-Bellman (HJB) equation. However, an issue here is
that the steady-state HJB equation generally yields multiple solutions; hence
if PINNs are directly employed to it, they may end up approximating a solution
that is different from the optimal value function of the problem. We tackle
this by instead applying PINNs to a finite-horizon variant of the steady-state
HJB that has a unique solution, and which uniformly approximates the optimal
value function as the horizon increases. An algorithm to verify if the chosen
horizon is large enough is also given, as well as a method to extend it -- with
reduced computations and robustness to approximation errors -- in case it is
not. Unlike many existing methods, the proposed technique works well with
non-polynomial basis functions, does not require prior knowledge of a
stabilizing controller, and does not perform iterative policy evaluations.
Simulations are performed, which verify and clarify theoretical findings.
\\ ( https://arxiv.org/abs/2505.21842 ,  1984kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21845 (*cross-listing*)
Date: Wed, 28 May 2025 00:25:10 GMT   (237kb)

Title: Spectral clustering for dependent community Hawkes process models of
  temporal networks
Authors: Lingfei Zhao, Hadeel Soliman, Kevin S. Xu, Subhadeep Paul
Categories: stat.ML cs.LG cs.SI stat.ME
\\
  Temporal networks observed continuously over time through timestamped
relational events data are commonly encountered in application settings
including online social media communications, financial transactions, and
international relations. Temporal networks often exhibit community structure
and strong dependence patterns among node pairs. This dependence can be modeled
through mutual excitations, where an interaction event from a sender to a
receiver node increases the possibility of future events among other node
pairs.
  We provide statistical results for a class of models that we call dependent
community Hawkes (DCH) models, which combine the stochastic block model with
mutually exciting Hawkes processes for modeling both community structure and
dependence among node pairs, respectively. We derive a non-asymptotic upper
bound on the misclustering error of spectral clustering on the event count
matrix as a function of the number of nodes and communities, time duration, and
the amount of dependence in the model. Our result leverages recent results on
bounding an appropriate distance between a multivariate Hawkes process count
vector and a Gaussian vector, along with results from random matrix theory. We
also propose a DCH model that incorporates only self and reciprocal excitation
along with highly scalable parameter estimation using a Generalized Method of
Moments (GMM) estimator that we demonstrate to be consistent for growing
network size and time duration.
\\ ( https://arxiv.org/abs/2505.21845 ,  237kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21872 (*cross-listing*)
Date: Wed, 28 May 2025 01:36:57 GMT   (5297kb,D)

Title: Targeted Unlearning Using Perturbed Sign Gradient Methods With
  Applications On Medical Images
Authors: George R. Nahass, Zhu Wang, Homa Rashidisabet, Won Hwa Kim, Sasha
  Hubschman, Jeffrey C. Peterson, Ghasem Yazdanpanah, Chad A. Purnell, Pete
  Setabutr, Ann Q. Tran, Darvin Yi, Sathya N. Ravi
Categories: eess.IV cs.LG
Comments: 39 pages, 12 figures, 11 tables, 3 algorithms
\\
  Machine unlearning aims to remove the influence of specific training samples
from a trained model without full retraining. While prior work has largely
focused on privacy-motivated settings, we recast unlearning as a
general-purpose tool for post-deployment model revision. Specifically, we focus
on utilizing unlearning in clinical contexts where data shifts, device
deprecation, and policy changes are common. To this end, we propose a bilevel
optimization formulation of boundary-based unlearning that can be solved using
iterative algorithms. We provide convergence guarantees when first-order
algorithms are used to unlearn. Our method introduces tunable loss design for
controlling the forgetting-retention tradeoff and supports novel model
composition strategies that merge the strengths of distinct unlearning runs.
Across benchmark and real-world clinical imaging datasets, our approach
outperforms baselines on both forgetting and retention metrics, including
scenarios involving imaging devices and anatomical outliers. This work
establishes machine unlearning as a modular, practical alternative to
retraining for real-world model maintenance in clinical applications.
\\ ( https://arxiv.org/abs/2505.21872 ,  5297kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21892 (*cross-listing*)
Date: Wed, 28 May 2025 02:10:11 GMT   (787kb)

Title: Almost Linear Convergence under Minimal Score Assumptions: Quantized
  Transition Diffusion
Authors: Xunpeng Huang, Yingyu Lin, Nikki Lijing Kuang, Hanze Dong, Difan Zou,
  Yian Ma, Tong Zhang
Categories: stat.ML cs.LG
Comments: 37 pages, 3 figures, 3 tables
\\
  Continuous diffusion models have demonstrated remarkable performance in data
generation across various domains, yet their efficiency remains constrained by
two critical limitations: (1) the local adjacency structure of the forward
Markov process, which restricts long-range transitions in the data space, and
(2) inherent biases introduced during the simulation of time-inhomogeneous
reverse denoising processes. To address these challenges, we propose Quantized
Transition Diffusion (QTD), a novel approach that integrates data quantization
with discrete diffusion dynamics. Our method first transforms the continuous
data distribution $p_*$ into a discrete one $q_*$ via histogram approximation
and binary encoding, enabling efficient representation in a structured discrete
latent space. We then design a continuous-time Markov chain (CTMC) with Hamming
distance-based transitions as the forward process, which inherently supports
long-range movements in the original data space. For reverse-time sampling, we
introduce a \textit{truncated uniformization} technique to simulate the reverse
CTMC, which can provably provide unbiased generation from $q_*$ under minimal
score assumptions. Through a novel KL dynamic analysis of the reverse CTMC, we
prove that QTD can generate samples with $O(d\ln^2(d/\epsilon))$ score
evaluations in expectation to approximate the $d$--dimensional target
distribution $p_*$ within an $\epsilon$ error tolerance. Our method not only
establishes state-of-the-art inference efficiency but also advances the
theoretical foundations of diffusion-based generative modeling by unifying
discrete and continuous diffusion paradigms.
\\ ( https://arxiv.org/abs/2505.21892 ,  787kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21925 (*cross-listing*)
Date: Wed, 28 May 2025 03:20:46 GMT   (15064kb)

Title: RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with
  Global Illumination
Authors: Chong Zeng and Yue Dong and Pieter Peers and Hongzhi Wu and Xin Tong
Categories: cs.GR cs.CV cs.LG
Comments: Accepted to SIGGRAPH 2025. Project page:
  https://microsoft.github.io/renderformer
Journal-ref: ACM SIGGRAPH 2025 Conference Papers
DOI: 10.1145/3721238.3730595
\\
  We present RenderFormer, a neural rendering pipeline that directly renders an
image from a triangle-based representation of a scene with full global
illumination effects and that does not require per-scene training or
fine-tuning. Instead of taking a physics-centric approach to rendering, we
formulate rendering as a sequence-to-sequence transformation where a sequence
of tokens representing triangles with reflectance properties is converted to a
sequence of output tokens representing small patches of pixels. RenderFormer
follows a two stage pipeline: a view-independent stage that models
triangle-to-triangle light transport, and a view-dependent stage that
transforms a token representing a bundle of rays to the corresponding pixel
values guided by the triangle-sequence from the view-independent stage. Both
stages are based on the transformer architecture and are learned with minimal
prior constraints. We demonstrate and evaluate RenderFormer on scenes with
varying complexity in shape and light transport.
\\ ( https://arxiv.org/abs/2505.21925 ,  15064kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21932 (*cross-listing*)
Date: Wed, 28 May 2025 03:37:10 GMT   (4174kb)

Title: Higher-Order Group Synchronization
Authors: Adriana L. Duncan, Joe Kileel
Categories: stat.ML cs.CV cs.LG math.CO math.OC
Comments: 40 pages
\\
  Group synchronization is the problem of determining reliable global estimates
from noisy local measurements on networks. The typical task for group
synchronization is to assign elements of a group to the nodes of a graph in a
way that respects group elements given on the edges which encode information
about local pairwise relationships between the nodes. In this paper, we
introduce a novel higher-order group synchronization problem which operates on
a hypergraph and seeks to synchronize higher-order local measurements on the
hyperedges to obtain global estimates on the nodes. Higher-order group
synchronization is motivated by applications to computer vision and image
processing, among other computational problems. First, we define the problem of
higher-order group synchronization and discuss its mathematical foundations.
Specifically, we give necessary and sufficient synchronizability conditions
which establish the importance of cycle consistency in higher-order group
synchronization. Then, we propose the first computational framework for general
higher-order group synchronization; it acts globally and directly on
higher-order measurements using a message passing algorithm. We discuss
theoretical guarantees for our framework, including convergence analyses under
outliers and noise. Finally, we show potential advantages of our method through
numerical experiments. In particular, we show that in certain cases our
higher-order method applied to rotational and angular synchronization
outperforms standard pairwise synchronization methods and is more robust to
outliers. We also show that our method has comparable performance on simulated
cryo-electron microscopy (cryo-EM) data compared to a standard cryo-EM
reconstruction package.
\\ ( https://arxiv.org/abs/2505.21932 ,  4174kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21994 (*cross-listing*)
Date: Wed, 28 May 2025 05:52:03 GMT   (4411kb)

Title: Locking-Free Training of Physics-Informed Neural Network for Solving
  Nearly Incompressible Elasticity Equations
Authors: Josef Dick, Seungchan Ko, Kassem Mustapha and Sanghyeon Park
Categories: math.NA cs.LG cs.NA
\\
  Due to divergence instability, the accuracy of low-order conforming finite
element methods for nearly incompressible homogeneous elasticity equations
deteriorates as the Lam\'e coefficient $\lambda\to\infty$, or equivalently as
the Poisson ratio $\nu\to1/2$. This phenomenon, known as locking or
non-robustness, remains not fully understood despite extensive investigation.
In this paper, we propose a robust method based on a fundamentally different,
machine-learning-driven approach. Leveraging recently developed
Physics-Informed Neural Networks (PINNs), we address the numerical solution of
linear elasticity equations governing nearly incompressible materials. The core
idea of our method is to appropriately decompose the given equations to
alleviate the extreme imbalance in the coefficients, while simultaneously
solving both the forward and inverse problems to recover the solutions of the
decomposed systems as well as the associated external conditions. Through
various numerical experiments, including constant, variable and parametric
Lam\'e coefficients, we illustrate the efficiency of the proposed methodology.
\\ ( https://arxiv.org/abs/2505.21994 ,  4411kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22008 (*cross-listing*)
Date: Wed, 28 May 2025 06:15:41 GMT   (9724kb)

Title: Align-DA: Align Score-based Atmospheric Data Assimilation with Multiple
  Preferences
Authors: Jing-An Sun, Hang Fan, Junchao Gong, Ben Fei, Kun Chen, Fenghua Ling,
  Wenlong Zhang, Wanghan Xu, Li Yan, Pierre Gentine, Lei Bai
Categories: physics.ao-ph cs.LG
\\
  Data assimilation (DA) aims to estimate the full state of a dynamical system
by combining partial and noisy observations with a prior model forecast,
commonly referred to as the background. In atmospheric applications, this
problem is fundamentally ill-posed due to the sparsity of observations relative
to the high-dimensional state space. Traditional methods address this challenge
by simplifying background priors to regularize the solution, which are
empirical and require continual tuning for application. Inspired by alignment
techniques in text-to-image diffusion models, we propose Align-DA, which
formulates DA as a generative process and uses reward signals to guide
background priors, replacing manual tuning with data-driven alignment.
Specifically, we train a score-based model in the latent space to approximate
the background-conditioned prior, and align it using three complementary reward
signals for DA: (1) assimilation accuracy, (2) forecast skill initialized from
the assimilated state, and (3) physical adherence of the analysis fields.
Experiments with multiple reward signals demonstrate consistent improvements in
analysis quality across different evaluation metrics and observation-guidance
strategies. These results show that preference alignment, implemented as a soft
constraint, can automatically adapt complex background priors tailored to DA,
offering a promising new direction for advancing the field.
\\ ( https://arxiv.org/abs/2505.22008 ,  9724kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22048 (*cross-listing*)
Date: Wed, 28 May 2025 07:16:11 GMT   (1145kb)

Title: Learning Curves of Stochastic Gradient Descent in Kernel Regression
Authors: Haihan Zhang, Weicheng Lin, Yuanshi Liu, Cong Fang
Categories: stat.ML cs.LG
\\
  This paper considers a canonical problem in kernel regression: how good are
the model performances when it is trained by the popular online first-order
algorithms, compared to the offline ones, such as ridge and ridgeless
regression? In this paper, we analyze the foundational single-pass Stochastic
Gradient Descent (SGD) in kernel regression under source condition where the
optimal predictor can even not belong to the RKHS, i.e. the model is
misspecified. Specifically, we focus on the inner product kernel over the
sphere and characterize the exact orders of the excess risk curves under
different scales of sample sizes $n$ concerning the input dimension $d$.
Surprisingly, we show that SGD achieves min-max optimal rates up to constants
among all the scales, without suffering the saturation, a prevalent phenomenon
observed in (ridge) regression, except when the model is highly misspecified
and the learning is in a final stage where $n\gg d^{\gamma}$ with any constant
$\gamma >0$. The main reason for SGD to overcome the curse of saturation is the
exponentially decaying step size schedule, a common practice in deep neural
network training. As a byproduct, we provide the \emph{first} provable
advantage of the scheme over the iterative averaging method in the common
setting.
\\ ( https://arxiv.org/abs/2505.22048 ,  1145kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22083 (*cross-listing*)
Date: Wed, 28 May 2025 08:06:25 GMT   (2139kb,D)

Title: Hyperbolic recurrent neural network as the first type of non-Euclidean
  neural quantum state ansatz
Authors: H. L. Dao
Categories: quant-ph cond-mat.dis-nn cs.LG physics.comp-ph
\\
  In this work, we introduce the first type of non-Euclidean neural quantum
state (NQS) ansatz, in the form of the hyperbolic GRU (a variant of recurrent
neural networks (RNNs)), to be used in the Variational Monte Carlo method of
approximating the ground state wavefunction for quantum many-body systems. In
particular, we examine the performances of NQS ansatzes constructed from both
conventional or Euclidean RNN/GRU and from hyperbolic GRU in the prototypical
settings of the one- and two-dimensional transverse field Ising models (TFIM)
of up to 100 spins and the one-dimensional Heisenberg $J_1J_2$ and $J_1J_2J_3$
systems of up 50 spins. By virtue of the fact that, for all of the experiments
performed in this work, hyperbolic GRU can yield performances comparable to or
better than Euclidean RNNs, which have been extensively studied in these
settings in the literature, our work is a proof-of-concept for the viability of
hyperbolic GRU as the first type of non-Euclidean NQS ansatz for quantum
many-body systems. Furthermore, in settings where the Hamiltonian displays a
clear hierarchical interaction structure, such as the 1D Heisenberg $J_1J_2$ &
$J_1J_2J_3$ systems with the 1st, 2nd and even 3rd nearest neighbor
interactions, our results show that hyperbolic GRU definitively outperforms its
Euclidean version in all instances. The fact that these results are reminiscent
of the established ones from natural language processing where hyperbolic GRU
almost always outperforms Euclidean RNNs when the training data exhibit a
tree-like or hierarchical structure leads us to hypothesize that hyperbolic GRU
NQS ansatz would likely outperform Euclidean RNN/GRU NQS ansatz in quantum spin
systems that involve different degrees of nearest neighbor interactions.
Finally, with this work, we hope to initiate future studies of other types of
non-Euclidean NQS beyond hyperbolic GRU.
\\ ( https://arxiv.org/abs/2505.22083 ,  2139kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22085 (*cross-listing*)
Date: Wed, 28 May 2025 08:07:34 GMT   (629kb,D)

Title: PADAM: Parallel averaged Adam reduces the error for stochastic
  optimization in scientific machine learning
Authors: Arnulf Jentzen and Julian Kranz and Adrian Riekert
Categories: math.OC cs.LG cs.NA math.NA
Comments: 38 pages, 13 figures
\\
  Averaging techniques such as Ruppert--Polyak averaging and exponential
movering averaging (EMA) are powerful approaches to accelerate optimization
procedures of stochastic gradient descent (SGD) optimization methods such as
the popular ADAM optimizer. However, depending on the specific optimization
problem under consideration, the type and the parameters for the averaging need
to be adjusted to achieve the smallest optimization error. In this work we
propose an averaging approach, which we refer to as parallel averaged ADAM
(PADAM), in which we compute parallely different averaged variants of ADAM and
during the training process dynamically select the variant with the smallest
optimization error. A central feature of this approach is that this procedure
requires no more gradient evaluations than the usual ADAM optimizer as each of
the averaged trajectories relies on the same underlying ADAM trajectory and
thus on the same underlying gradients. We test the proposed PADAM optimizer in
13 stochastic optimization and deep neural network (DNN) learning problems and
compare its performance with known optimizers from the literature such as
standard SGD, momentum SGD, Adam with and without EMA, and ADAMW. In
particular, we apply the compared optimizers to physics-informed neural
network, deep Galerkin, deep backward stochastic differential equation and deep
Kolmogorov approximations for boundary value partial differential equation
problems from scientific machine learning, as well as to DNN approximations for
optimal control and optimal stopping problems. In nearly all of the considered
examples PADAM achieves, sometimes among others and sometimes exclusively,
essentially the smallest optimization error. This work thus strongly suggest to
consider PADAM for scientific machine learning problems and also motivates
further research for adaptive averaging procedures within the training of DNNs.
\\ ( https://arxiv.org/abs/2505.22085 ,  629kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22090 (*cross-listing*)
Date: Wed, 28 May 2025 08:14:12 GMT   (4425kb)

Title: High Volume Rate 3D Ultrasound Reconstruction with Diffusion Models
Authors: Tristan S.W. Stevens, Ois\'in Nolan, Oudom Somphone, Jean-Luc Robert
  and Ruud J.G. van Sloun
Categories: eess.IV cs.LG
Comments: 10 pages, 10 figures, preprint
\\
  Three-dimensional ultrasound enables real-time volumetric visualization of
anatomical structures. Unlike traditional 2D ultrasound, 3D imaging reduces the
reliance on precise probe orientation, potentially making ultrasound more
accessible to clinicians with varying levels of experience and improving
automated measurements and post-exam analysis. However, achieving both high
volume rates and high image quality remains a significant challenge. While 3D
diverging waves can provide high volume rates, they suffer from limited tissue
harmonic generation and increased multipath effects, which degrade image
quality. One compromise is to retain the focusing in elevation while leveraging
unfocused diverging waves in the lateral direction to reduce the number of
transmissions per elevation plane. Reaching the volume rates achieved by full
3D diverging waves, however, requires dramatically undersampling the number of
elevation planes. Subsequently, to render the full volume, simple interpolation
techniques are applied. This paper introduces a novel approach to 3D ultrasound
reconstruction from a reduced set of elevation planes by employing diffusion
models (DMs) to achieve increased spatial and temporal resolution. We compare
both traditional and supervised deep learning-based interpolation methods on a
3D cardiac ultrasound dataset. Our results show that DM-based reconstruction
consistently outperforms the baselines in image quality and downstream task
performance. Additionally, we accelerate inference by leveraging the temporal
consistency inherent to ultrasound sequences. Finally, we explore the
robustness of the proposed method by exploiting the probabilistic nature of
diffusion posterior sampling to quantify reconstruction uncertainty and
demonstrate improved recall on out-of-distribution data with synthetic
anomalies under strong subsampling.
\\ ( https://arxiv.org/abs/2505.22090 ,  4425kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22099 (*cross-listing*)
Date: Wed, 28 May 2025 08:24:43 GMT   (4996kb)

Title: On the Transferability and Discriminability of Repersentation Learning
  in Unsupervised Domain Adaptation
Authors: Wenwen Qiang, Ziyin Gu, Lingyu Si, Jiangmeng Li, Changwen Zheng,
  Fuchun Sun, Hui Xiong
Categories: cs.CV cs.LG
\\
  In this paper, we addressed the limitation of relying solely on distribution
alignment and source-domain empirical risk minimization in Unsupervised Domain
Adaptation (UDA). Our information-theoretic analysis showed that this standard
adversarial-based framework neglects the discriminability of target-domain
features, leading to suboptimal performance. To bridge this
theoretical-practical gap, we defined "good representation learning" as
guaranteeing both transferability and discriminability, and proved that an
additional loss term targeting target-domain discriminability is necessary.
Building on these insights, we proposed a novel adversarial-based UDA framework
that explicitly integrates a domain alignment objective with a
discriminability-enhancing constraint. Instantiated as Domain-Invariant
Representation Learning with Global and Local Consistency (RLGLC), our method
leverages Asymmetrically-Relaxed Wasserstein of Wasserstein Distance (AR-WWD)
to address class imbalance and semantic dimension weighting, and employs a
local consistency mechanism to preserve fine-grained target-domain
discriminative information. Extensive experiments across multiple benchmark
datasets demonstrate that RLGLC consistently surpasses state-of-the-art
methods, confirming the value of our theoretical perspective and underscoring
the necessity of enforcing both transferability and discriminability in
adversarial-based UDA.
\\ ( https://arxiv.org/abs/2505.22099 ,  4996kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22107 (*cross-listing*)
Date: Wed, 28 May 2025 08:34:46 GMT   (729kb,D)

Title: Curse of High Dimensionality Issue in Transformer for Long-context
  Modeling
Authors: Shuhai Zhang, Zeng You, Yaofo Chen, Zhiquan Wen, Qianyue Wang, Zhijie
  Qiu, Yuanqing Li and Mingkui Tan
Categories: cs.CL cs.LG
Comments: Accepted at ICML 2025
\\
  Transformer-based large language models (LLMs) excel in natural language
processing tasks by capturing long-range dependencies through self-attention
mechanisms. However, long-context modeling faces significant computational
inefficiencies due to \textit{redundant} attention computations: while
attention weights are often \textit{sparse}, all tokens consume \textit{equal}
computational resources. In this paper, we reformulate traditional
probabilistic sequence modeling as a \textit{supervised learning task},
enabling the separation of relevant and irrelevant tokens and providing a
clearer understanding of redundancy. Based on this reformulation, we
theoretically analyze attention sparsity, revealing that only a few tokens
significantly contribute to predictions. Building on this, we formulate
attention optimization as a linear coding problem and propose a \textit{group
coding strategy}, theoretically showing its ability to improve robustness
against random noise and enhance learning efficiency. Motivated by this, we
propose \textit{Dynamic Group Attention} (DGA), which leverages the group
coding to explicitly reduce redundancy by aggregating less important tokens
during attention computation. Empirical results show that our DGA significantly
reduces computational costs while maintaining competitive performance.Code is
available at https://github.com/bolixinyu/DynamicGroupAttention.
\\ ( https://arxiv.org/abs/2505.22107 ,  729kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22135 (*cross-listing*)
Date: Wed, 28 May 2025 08:59:02 GMT   (823kb)

Title: RAD: Redundancy-Aware Distillation for Hybrid Models via
  Self-Speculative Decoding
Authors: Yuichiro Hoshino, Hideyuki Tachibana, Muneyoshi Inahara, Hiroto
  Takegawa
Categories: cs.CL cs.LG
Comments: 26 pages
\\
  Hybrid models combining Transformers and State Space Models (SSMs) are
promising for balancing performance and efficiency. However, optimizing these
hybrid models, particularly by addressing the potential redundancy inherent
within the Transformer components, remains a significant challenge. In this
paper, we propose RAD (Redundancy-Aware Distillation), a novel framework that
uses self-speculative decoding as a diagnostic tool to identify redundant
attention layers within the model. These identified layers are then selectively
replaced with SSM components, followed by targeted (self-)distillation.
Specifically, RAD focuses knowledge transfer on the components identified as
redundant, considering architectural changes and specific weight initialization
strategies. We experimentally demonstrate that self-distillation using RAD
significantly surpasses the performance of the original base model on
mathematical and coding tasks. Furthermore, RAD is also effective in standard
knowledge distillation settings, achieving up to approximately 2x faster
convergence compared to baseline methods. Notably, while a baseline model
distilled from a Llama-3.1 70B teacher achieves scores of 46.17 on GSM8K and
22.75 on CRUX, RAD achieves significantly higher scores of 71.27 on GSM8K and
28.25 on CRUX, even when using a much smaller Llama-3.1 8B teacher. RAD offers
a new pathway for efficient optimization and performance enhancement in the
distillation of hybrid models.
\\ ( https://arxiv.org/abs/2505.22135 ,  823kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22238 (*cross-listing*)
Date: Wed, 28 May 2025 11:12:57 GMT   (352kb)

Title: Yambda-5B -- A Large-Scale Multi-modal Dataset for Ranking And Retrieval
Authors: A. Ploshkin, V. Tytskiy, A. Pismenny, V. Baikalov, E. Taychinov, A.
  Permiakov, D. Burlakov, E. Krofto, N. Savushkin
Categories: cs.IR cs.LG
\\
  We present Yambda-5B, a large-scale open dataset sourced from the
Yandex.Music streaming platform. Yambda-5B contains 4.79 billion user-item
interactions from 1 million users across 9.39 million tracks. The dataset
includes two primary types of interactions: implicit feedback (listening
events) and explicit feedback (likes, dislikes, unlikes and undislikes). In
addition, we provide audio embeddings for most tracks, generated by a
convolutional neural network trained on audio spectrograms. A key
distinguishing feature of Yambda-5B is the inclusion of the is_organic flag,
which separates organic user actions from recommendation-driven events. This
distinction is critical for developing and evaluating machine learning
algorithms, as Yandex.Music relies on recommender systems to personalize track
selection for users. To support rigorous benchmarking, we introduce an
evaluation protocol based on a Global Temporal Split, allowing recommendation
algorithms to be assessed in conditions that closely mirror real-world use. We
report benchmark results for standard baselines (ItemKNN, iALS) and advanced
models (SANSA, SASRec) using a variety of evaluation metrics. By releasing
Yambda-5B to the community, we aim to provide a readily accessible,
industrial-scale resource to advance research, foster innovation, and promote
reproducible results in recommender systems.
\\ ( https://arxiv.org/abs/2505.22238 ,  352kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22243 (*cross-listing*)
Date: Wed, 28 May 2025 11:25:50 GMT   (1011kb,D)

Title: UDuo: Universal Dual Optimization Framework for Online Matching
Authors: Bin Li, Diwei Liu, Zehong Hu, Jia Jia
Categories: cs.IR cs.LG
\\
  Online resource allocation under budget constraints critically depends on
proper modeling of user arrival dynamics. Classical approaches employ
stochastic user arrival models to derive near-optimal solutions through
fractional matching formulations of exposed users for downstream allocation
tasks. However, this is no longer a reasonable assumption when the environment
changes dynamically. In this work, We propose the Universal Dual optimization
framework UDuo, a novel paradigm that fundamentally rethinks online allocation
through three key innovations: (i) a temporal user arrival representation
vector that explicitly captures distribution shifts in user arrival patterns
and resource consumption dynamics, (ii) a resource pacing learner with adaptive
allocation policies that generalize to heterogeneous constraint scenarios, and
(iii) an online time-series forecasting approach for future user arrival
distributions that achieves asymptotically optimal solutions with constraint
feasibility guarantees in dynamic environments. Experimental results show that
UDuo achieves higher efficiency and faster convergence than the traditional
stochastic arrival model in real-world pricing while maintaining rigorous
theoretical validity for general online allocation problems.
\\ ( https://arxiv.org/abs/2505.22243 ,  1011kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22296 (*cross-listing*)
Date: Wed, 28 May 2025 12:33:46 GMT   (58kb)

Title: 360-LLaMA-Factory: Plug & Play Sequence Parallelism for Long
  Post-Training
Authors: Haosheng Zou, Xiaowei Lv, Shousheng Jia, Xiangzheng Zhang
Categories: cs.CL cs.LG
Comments: code at https://github.com/Qihoo360/360-LLaMA-Factory
\\
  Adding sequence parallelism into LLaMA-Factory, we open-sourced
360-LLaMA-Factory at https://github.com/Qihoo360/360-LLaMA-Factory.
360-LLaMA-Factory has received wide recognition and used in models such as
Light-R1 arXiv:2503.10460, TinyR1 arXiv:2503.04872, Kaggle AIMO math models and
also in large companies' training frameworks. This technical report delves
deeper into the different sequence parallel modes behind 360-LLaMA-Factory and
discusses our implementation insights.
\\ ( https://arxiv.org/abs/2505.22296 ,  58kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22318 (*cross-listing*)
Date: Wed, 28 May 2025 13:03:18 GMT   (1026kb)

Title: If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?
Authors: Ishwar B Balappanawar, Vamshi Krishna Bonagiri, Anish R Joishy, Manas
  Gaur, Krishnaprasad Thirunarayan, Ponnurangam Kumaraguru
Categories: cs.CL cs.LG
Comments: 16 pages, 5 figures
\\
  Large Language Models (LLMs) demonstrate impressive reasoning capabilities in
familiar contexts, but struggle when the context conflicts with their
parametric knowledge. To investigate this phenomenon, we introduce
CounterLogic, a dataset containing 1,800 examples across 9 logical schemas,
explicitly designed to evaluate logical reasoning through counterfactual
(hypothetical knowledge-conflicting) scenarios. Our systematic evaluation of 11
LLMs across 6 different datasets reveals a consistent performance degradation,
with accuracies dropping by 27% on average when reasoning through
counterfactual information. We propose Self-Segregate, a prompting method
enabling metacognitive awareness (explicitly identifying knowledge conflicts)
before reasoning. Our method dramatically narrows the average performance gaps
from 27% to just 11%, while significantly increasing the overall accuracy
(+7.5%). We discuss the implications of these findings and draw parallels to
human cognitive processes, particularly on how humans disambiguate conflicting
information during reasoning tasks. Our findings offer practical insights for
understanding and enhancing LLMs reasoning capabilities in real-world
applications, especially where models must logically reason independently of
their factual knowledge.
\\ ( https://arxiv.org/abs/2505.22318 ,  1026kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22326 (*cross-listing*)
Date: Wed, 28 May 2025 13:13:52 GMT   (5658kb)

Title: Individualised Counterfactual Examples Using Conformal Prediction
  Intervals
Authors: James M. Adams and Gesine Reinert and Lukasz Szpruch and Carsten Maple
  and Andrew Elliott
Categories: stat.ML cs.LG
Comments: Submitted to Conformal and Probabilistic Predictions With
  Applications (COPA) 2025
\\
  Counterfactual explanations for black-box models aim to pr ovide insight into
an algorithmic decision to its recipient. For a binary classification problem
an individual counterfactual details which features might be changed for the
model to infer the opposite class. High-dimensional feature spaces that are
typical of machine learning classification models admit many possible
counterfactual examples to a decision, and so it is important to identify
additional criteria to select the most useful counterfactuals. In this paper,
we explore the idea that the counterfactuals should be maximally informative
when considering the knowledge of a specific individual about the underlying
classifier. To quantify this information gain we explicitly model the knowledge
of the individual, and assess the uncertainty of predictions which the
individual makes by the width of a conformal prediction interval. Regions of
feature space where the prediction interval is wide correspond to areas where
the confidence in decision making is low, and an additional counterfactual
example might be more informative to an individual. To explore and evaluate our
individualised conformal prediction interval counterfactuals (CPICFs), first we
present a synthetic data set on a hypercube which allows us to fully visualise
the decision boundary, conformal intervals via three different methods, and
resultant CPICFs. Second, in this synthetic data set we explore the impact of a
single CPICF on the knowledge of an individual locally around the original
query. Finally, in both our synthetic data set and a complex real world dataset
with a combination of continuous and discrete variables, we measure the utility
of these counterfactuals via data augmentation, testing the performance on a
held out set.
\\ ( https://arxiv.org/abs/2505.22326 ,  5658kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22332 (*cross-listing*)
Date: Wed, 28 May 2025 13:20:20 GMT   (1292kb)

Title: Credal Prediction based on Relative Likelihood
Authors: Timo L\"ohr, Paul Hofman, Felix Mohr, Eyke H\"ullermeier
Categories: stat.ML cs.LG
\\
  Predictions in the form of sets of probability distributions, so-called
credal sets, provide a suitable means to represent a learner's epistemic
uncertainty. In this paper, we propose a theoretically grounded approach to
credal prediction based on the statistical notion of relative likelihood: The
target of prediction is the set of all (conditional) probability distributions
produced by the collection of plausible models, namely those models whose
relative likelihood exceeds a specified threshold. This threshold has an
intuitive interpretation and allows for controlling the trade-off between
correctness and precision of credal predictions. We tackle the problem of
approximating credal sets defined in this way by means of suitably modified
ensemble learning techniques. To validate our approach, we illustrate its
effectiveness by experiments on benchmark datasets demonstrating superior
uncertainty representation without compromising predictive performance. We also
compare our method against several state-of-the-art baselines in credal
prediction.
\\ ( https://arxiv.org/abs/2505.22332 ,  1292kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22342 (*cross-listing*)
Date: Wed, 28 May 2025 13:26:52 GMT   (1327kb,D)

Title: Progressive Data Dropout: An Embarrassingly Simple Approach to Faster
  Training
Authors: Shriram M S, Xinyue Hao, Shihao Hou, Yang Lu, Laura Sevilla-Lara,
  Anurag Arnab, Shreyank N Gowda
Categories: cs.CV cs.LG
\\
  The success of the machine learning field has reliably depended on training
on large datasets. While effective, this trend comes at an extraordinary cost.
This is due to two deeply intertwined factors: the size of models and the size
of datasets. While promising research efforts focus on reducing the size of
models, the other half of the equation remains fairly mysterious. Indeed, it is
surprising that the standard approach to training remains to iterate over and
over, uniformly sampling the training dataset. In this paper we explore a
series of alternative training paradigms that leverage insights from
hard-data-mining and dropout, simple enough to implement and use that can
become the new training standard. The proposed Progressive Data Dropout reduces
the number of effective epochs to as little as 12.4% of the baseline. This
savings actually do not come at any cost for accuracy. Surprisingly, the
proposed method improves accuracy by up to 4.82%. Our approach requires no
changes to model architecture or optimizer, and can be applied across standard
training pipelines, thus posing an excellent opportunity for wide adoption.
Code can be found here: https://github.com/bazyagami/LearningWithRevision
\\ ( https://arxiv.org/abs/2505.22342 ,  1327kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22364 (*cross-listing*)
Date: Wed, 28 May 2025 13:46:07 GMT   (2095kb)

Title: Computing Optimal Transport Maps and Wasserstein Barycenters Using
  Conditional Normalizing Flows
Authors: Gabriele Visentin, Patrick Cheridito
Categories: stat.ML cs.LG
MSC-class: 65K99 (Primary) 68T07, 68T99 (Secondary)
\\
  We present a novel method for efficiently computing optimal transport maps
and Wasserstein barycenters in high-dimensional spaces. Our approach uses
conditional normalizing flows to approximate the input distributions as
invertible pushforward transformations from a common latent space. This makes
it possible to directly solve the primal problem using gradient-based
minimization of the transport cost, unlike previous methods that rely on dual
formulations and complex adversarial optimization. We show how this approach
can be extended to compute Wasserstein barycenters by solving a conditional
variance minimization problem. A key advantage of our conditional architecture
is that it enables the computation of barycenters for hundreds of input
distributions, which was computationally infeasible with previous methods. Our
numerical experiments illustrate that our approach yields accurate results
across various high-dimensional tasks and compares favorably with previous
state-of-the-art methods.
\\ ( https://arxiv.org/abs/2505.22364 ,  2095kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22454 (*cross-listing*)
Date: Wed, 28 May 2025 15:11:53 GMT   (444kb)

Title: Depth-Based Matrix Classification for the HHL Quantum Algorithm
Authors: Mark Danza and Sonia Lopez Alarcon and Cory Merkel
Categories: quant-ph cs.LG
\\
  Under the nearing error-corrected era of quantum computing, it is necessary
to understand the suitability of certain post-NISQ algorithms for practical
problems. One of the most promising, applicable and yet difficult to implement
in practical terms is the Harrow, Hassidim and Lloyd (HHL) algorithm for linear
systems of equations. An enormous number of problems can be expressed as linear
systems of equations, from Machine Learning to fluid dynamics. However, in most
cases, HHL will not be able to provide a practical, reasonable solution to
these problems. This paper's goal inquires about whether problems can be
labeled using Machine Learning classifiers as suitable or unsuitable for HHL
implementation when some numerical information about the problem is known
beforehand. This work demonstrates that training on significantly
representative data distributions is critical to achieve good classifications
of the problems based on the numerical properties of the matrix representing
the system of equations. Accurate classification is possible through
Multi-Layer Perceptrons, although with careful design of the training data
distribution and classifier parameters.
\\ ( https://arxiv.org/abs/2505.22454 ,  444kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22469 (*cross-listing*)
Date: Wed, 28 May 2025 15:22:15 GMT   (7950kb)

Title: CPINN-ABPI: Physics-Informed Neural Networks for Accurate Power
  Estimation in MPSoCs
Authors: Mohamed R. Elshamy, Mehdi Elahi, Ahmad Patooghy, and Abdel-Hameed A.
  Badawy
Categories: cs.PF cs.LG
\\
  Efficient thermal and power management in modern multiprocessor
systems-on-chip (MPSoCs) demands accurate power consumption estimation. One of
the state-of-the-art approaches, Alternative Blind Power Identification (ABPI),
theoretically eliminates the dependence on steady-state temperatures,
addressing a major shortcoming of previous approaches. However, ABPI
performance has remained unverified in actual hardware implementations. In this
study, we conduct the first empirical validation of ABPI on commercial hardware
using the NVIDIA Jetson Xavier AGX platform. Our findings reveal that, while
ABPI provides computational efficiency and independence from steady-state
temperature, it exhibits considerable accuracy deficiencies in real-world
scenarios. To overcome these limitations, we introduce a novel approach that
integrates Custom Physics-Informed Neural Networks (CPINNs) with the underlying
thermal model of ABPI. Our approach employs a specialized loss function that
harmonizes physical principles with data-driven learning, complemented by
multi-objective genetic algorithm optimization to balance estimation accuracy
and computational cost. In experimental validation, CPINN-ABPI achieves a
reduction of 84.7\% CPU and 73.9\% GPU in the mean absolute error (MAE)
relative to ABPI, with the weighted mean absolute percentage error (WMAPE)
improving from 47\%--81\% to $\sim$12\%. The method maintains real-time
performance with 195.3~$\mu$s of inference time, with similar 85\%--99\%
accuracy gains across heterogeneous SoCs.
\\ ( https://arxiv.org/abs/2505.22469 ,  7950kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22481 (*cross-listing*)
Date: Wed, 28 May 2025 15:29:43 GMT   (6193kb,D)

Title: Hypothesis Testing in Imaging Inverse Problems
Authors: Yiming Xi, Konstantinos Zygalakis, Marcelo Pereyra
Categories: stat.ML cs.LG
\\
  This paper proposes a framework for semantic hypothesis testing tailored to
imaging inverse problems. Modern imaging methods struggle to support hypothesis
testing, a core component of the scientific method that is essential for the
rigorous interpretation of experiments and robust interfacing with
decision-making processes. There are three main reasons why image-based
hypothesis testing is challenging. First, the difficulty of using a single
observation to simultaneously reconstruct an image, formulate hypotheses, and
quantify their statistical significance. Second, the hypotheses encountered in
imaging are mostly of semantic nature, rather than quantitative statements
about pixel values. Third, it is challenging to control test error
probabilities because the null and alternative distributions are often unknown.
Our proposed approach addresses these difficulties by leveraging concepts from
self-supervised computational imaging, vision-language models, and
non-parametric hypothesis testing with e-values. We demonstrate our proposed
framework through numerical experiments related to image-based phenotyping,
where we achieve excellent power while robustly controlling Type I errors.
\\ ( https://arxiv.org/abs/2505.22481 ,  6193kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22502 (*cross-listing*)
Date: Wed, 28 May 2025 15:50:56 GMT   (177kb)

Title: Assessing Quantum Advantage for Gaussian Process Regression
Authors: Dominic Lowe, M.S. Kim, Roberto Bondesan
Categories: quant-ph cs.LG
Comments: 18 pages, 2 figures
\\
  Gaussian Process Regression is a well-known machine learning technique for
which several quantum algorithms have been proposed. We show here that in a
wide range of scenarios these algorithms show no exponential speedup. We
achieve this by rigorously proving that the condition number of a kernel matrix
scales at least linearly with the matrix size under general assumptions on the
data and kernel. We additionally prove that the sparsity and Frobenius norm of
a kernel matrix scale linearly under similar assumptions. The implications for
the quantum algorithms runtime are independent of the complexity of loading
classical data on a quantum computer and also apply to dequantised algorithms.
We supplement our theoretical analysis with numerical verification for popular
kernels in machine learning.
\\ ( https://arxiv.org/abs/2505.22502 ,  177kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22518 (*cross-listing*)
Date: Wed, 28 May 2025 16:04:17 GMT   (66kb,D)

Title: IGNIS: A Neural Network Framework for Robust Parameter Estimation in
  Archimedean Copulas
Authors: Agnideep Aich, Ashit Baran Aich and Bruce Wade
Categories: stat.ML cs.LG
Comments: Under review
MSC-class: 62H05, 62H12, 62F10, 68T07, 62-08
\\
  Parameter estimation for Archimedean copulas remains a challenging problem,
particularly for the recently developed A1 and A2 families that exhibit complex
dependency structures. Traditional methods, such as the Method of Moments
(MoM), Maximum Likelihood Estimation (MLE), and Maximum Pseudo-Likelihood
(MPL), often struggle due to issues of non-monotonic relationship with
dependency measures such as Kendall's tau (as in the case of A1) and numerical
instability. In this paper, we present the IGNIS Network, a novel, unified
neural framework that learns a direct mapping from observable dependency
measures to copula parameters, thereby overcoming the limitations of classical
approaches. Our approach is trained on simulated data spanning five Archimedean
copula families including Clayton, Gumbel, Frank, A1, and A2, ensuring its
general applicability across the entire family. Extensive simulation studies
demonstrate that the IGNIS Network reduces estimation errors compared to MoM,
while inherently enforcing parameter constraints through theory-guided
post-processing. We further validate the practical utility of our method on
diverse real-world datasets, including financial returns (AAPL-MSFT),
healthcare metrics (CDC Diabetes indicators), and environmental measurements
(PM2.5 air quality). Our results underscore the transformative potential of
neural methods for robust and accurate dependence modeling in modern
applications.
\\ ( https://arxiv.org/abs/2505.22518 ,  66kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22527 (*cross-listing*)
Date: Wed, 28 May 2025 16:13:36 GMT   (28kb)

Title: Symplectic Generative Networks (SGNs): A Hamiltonian Framework for
  Invertible Deep Generative Modeling
Authors: Agnideep Aich, Ashit Aich and Bruce Wade
Categories: stat.ML cs.LG
Comments: Submitted
MSC-class: 68T07, 37J39, 65P10, 62B10, 53D22, 94A17
\\
  We introduce the Symplectic Generative Network (SGN), a deep generative model
that leverages Hamiltonian mechanics to construct an invertible,
volume-preserving mapping between a latent space and the data space. By
endowing the latent space with a symplectic structure and modeling data
generation as the time evolution of a Hamiltonian system, SGN achieves exact
likelihood evaluation without incurring the computational overhead of Jacobian
determinant calculations. In this work, we provide a rigorous mathematical
foundation for SGNs through a comprehensive theoretical framework that
includes: (i) complete proofs of invertibility and volume preservation, (ii) a
formal complexity analysis with theoretical comparisons to Variational
Autoencoders and Normalizing Flows, (iii) strengthened universal approximation
results with quantitative error bounds, (iv) an information-theoretic analysis
based on the geometry of statistical manifolds, and (v) an extensive stability
analysis with adaptive integration guarantees. These contributions highlight
the fundamental advantages of SGNs and establish a solid foundation for future
empirical investigations and applications to complex, high-dimensional data.
\\ ( https://arxiv.org/abs/2505.22527 ,  28kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22535 (*cross-listing*)
Date: Wed, 28 May 2025 16:21:58 GMT   (38008kb,D)

Title: RiverMamba: A State Space Model for Global River Discharge and Flood
  Forecasting
Authors: Mohamad Hakam Shams Eddin, Yikui Zahng, Stefan Kollet, Juergen Gall
Categories: cs.CV cs.LG
Comments: Main paper 10 pages, Appendix 53 pages
\\
  Recent deep learning approaches for river discharge forecasting have improved
the accuracy and efficiency in flood forecasting, enabling more reliable early
warning systems for risk management. Nevertheless, existing deep learning
approaches in hydrology remain largely confined to local-scale applications and
do not leverage the inherent spatial connections of bodies of water. Thus,
there is a strong need for new deep learning methodologies that are capable of
modeling spatio-temporal relations to improve river discharge and flood
forecasting for scientific and operational applications. To address this, we
present RiverMamba, a novel deep learning model that is pretrained with
long-term reanalysis data and that can forecast global river discharge and
floods on a $0.05^\circ$ grid up to 7 days lead time, which is of high
relevance in early warning. To achieve this, RiverMamba leverages efficient
Mamba blocks that enable the model to capture global-scale channel network
routing and enhance its forecast capability for longer lead times. The forecast
blocks integrate ECMWF HRES meteorological forecasts, while accounting for
their inaccuracies through spatio-temporal modeling. Our analysis demonstrates
that RiverMamba delivers reliable predictions of river discharge, including
extreme floods across return periods and lead times, surpassing both
operational AI- and physics-based models.
\\ ( https://arxiv.org/abs/2505.22535 ,  38008kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22554 (*cross-listing*)
Date: Wed, 28 May 2025 16:34:58 GMT   (29kb)

Title: Can Copulas Be Used for Feature Selection? A Machine Learning Study on
  Diabetes Risk Prediction
Authors: Agnideep Aich, Md Monzur Murshed, Amanda Mayeaux, Sameera Hewage
Categories: stat.ML cs.LG
Comments: Submitted
MSC-class: 62H05, 62H12, 62P10, 68T07
\\
  Accurate diabetes risk prediction relies on identifying key features from
complex health datasets, but conventional methods like mutual information (MI)
filters and genetic algorithms (GAs) often overlook extreme dependencies
critical for high-risk subpopulations. In this study we introduce a
feature-selection framework using the upper-tail dependence coefficient
({\lambda}U) of the novel A2 copula, which quantifies how often extreme higher
values of a predictor co-occur with diabetes diagnoses (target variable).
Applied to the CDC Diabetes Health Indicators dataset (n=253,680), our method
prioritizes five predictors (self-reported general health, high blood pressure,
body mass index, mobility limitations, and high cholesterol levels) based on
upper tail dependencies. These features match or outperform MI and GA selected
subsets across four classifiers (Random Forest, XGBoost, Logistic Regression,
Gradient Boosting), achieving accuracy up to 86.5% (XGBoost) and AUC up to
0.806 (Gradient Boosting), rivaling the full 21-feature model. Permutation
importance confirms clinical relevance, with BMI and general health driving
accuracy. To our knowledge, this is the first work to apply a copula's
upper-tail dependence for supervised feature selection, bridging extreme-value
theory and machine learning to deliver a practical toolkit for diabetes
prevention.
\\ ( https://arxiv.org/abs/2505.22554 ,  29kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22609 (*cross-listing*)
Date: Wed, 28 May 2025 17:24:33 GMT   (7011kb)

Title: Chest Disease Detection In X-Ray Images Using Deep Learning
  Classification Method
Authors: Alanna Hazlett, Naomi Ohashi, Timothy Rodriguez, Sodiq Adewole
Categories: eess.IV cs.CV cs.LG
\\
  In this work, we investigate the performance across multiple classification
models to classify chest X-ray images into four categories of COVID-19,
pneumonia, tuberculosis (TB), and normal cases. We leveraged transfer learning
techniques with state-of-the-art pre-trained Convolutional Neural Networks
(CNNs) models. We fine-tuned these pre-trained architectures on a labeled
medical x-ray images. The initial results are promising with high accuracy and
strong performance in key classification metrics such as precision, recall, and
F1 score. We applied Gradient-weighted Class Activation Mapping (Grad-CAM) for
model interpretability to provide visual explanations for classification
decisions, improving trust and transparency in clinical applications.
\\ ( https://arxiv.org/abs/2505.22609 ,  7011kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22622 (*cross-listing*)
Date: Wed, 28 May 2025 17:44:10 GMT   (2576kb,D)

Title: Principled Out-of-Distribution Generalization via Simplicity
Authors: Jiawei Ge, Amanda Wang, Shange Tang, Chi Jin
Categories: stat.ML cs.LG math.ST stat.TH
\\
  Modern foundation models exhibit remarkable out-of-distribution (OOD)
generalization, solving tasks far beyond the support of their training data.
However, the theoretical principles underpinning this phenomenon remain
elusive. This paper investigates this problem by examining the compositional
generalization abilities of diffusion models in image generation. Our analysis
reveals that while neural network architectures are expressive enough to
represent a wide range of models -- including many with undesirable behavior on
OOD inputs -- the true, generalizable model that aligns with human expectations
typically corresponds to the simplest among those consistent with the training
data.
  Motivated by this observation, we develop a theoretical framework for OOD
generalization via simplicity, quantified using a predefined simplicity metric.
We analyze two key regimes: (1) the constant-gap setting, where the true model
is strictly simpler than all spurious alternatives by a fixed gap, and (2) the
vanishing-gap setting, where the fixed gap is replaced by a smoothness
condition ensuring that models close in simplicity to the true model yield
similar predictions. For both regimes, we study the regularized maximum
likelihood estimator and establish the first sharp sample complexity guarantees
for learning the true, generalizable, simple model.
\\ ( https://arxiv.org/abs/2505.22622 ,  2576kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22638 (*cross-listing*)
Date: Wed, 28 May 2025 17:54:23 GMT   (1059kb)

Title: SimProcess: High Fidelity Simulation of Noisy ICS Physical Processes
Authors: Denis Donadel, Gabriele Crestanello, Giulio Morandini, Daniele
  Antonioli, Mauro Conti, Massimo Merro
Categories: cs.CR cs.LG
Comments: In 11th ACM Cyber-Physical System Security Workshop (CPSS '25),
  August 25-29, 2025, Hanoi, Vietnam
DOI: 10.1145/3709017.3737711
\\
  Industrial Control Systems (ICS) manage critical infrastructures like power
grids and water treatment plants. Cyberattacks on ICSs can disrupt operations,
causing severe economic, environmental, and safety issues. For example,
undetected pollution in a water plant can put the lives of thousands at stake.
ICS researchers have increasingly turned to honeypots -- decoy systems designed
to attract attackers, study their behaviors, and eventually improve defensive
mechanisms. However, existing ICS honeypots struggle to replicate the ICS
physical process, making them susceptible to detection. Accurately simulating
the noise in ICS physical processes is challenging because different factors
produce it, including sensor imperfections and external interferences.
  In this paper, we propose SimProcess, a novel framework to rank the fidelity
of ICS simulations by evaluating how closely they resemble real-world and noisy
physical processes. It measures the simulation distance from a target system by
estimating the noise distribution with machine learning models like Random
Forest. Unlike existing solutions that require detailed mathematical models or
are limited to simple systems, SimProcess operates with only a timeseries of
measurements from the real system, making it applicable to a broader range of
complex dynamic systems. We demonstrate the framework's effectiveness through a
case study using real-world power grid data from the EPIC testbed. We compare
the performance of various simulation methods, including static and generative
noise techniques. Our model correctly classifies real samples with a recall of
up to 1.0. It also identifies Gaussian and Gaussian Mixture as the best
distribution to simulate our power systems, together with a generative solution
provided by an autoencoder, thereby helping developers to improve honeypot
fidelity. Additionally, we make our code publicly available.
\\ ( https://arxiv.org/abs/2505.22638 ,  1059kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22651 (*cross-listing*)
Date: Wed, 28 May 2025 17:58:03 GMT   (6782kb)

Title: Sherlock: Self-Correcting Reasoning in Vision-Language Models
Authors: Yi Ding, Ruqi Zhang
Categories: cs.CV cs.CL cs.LG
Comments: 27 pages
\\
  Reasoning Vision-Language Models (VLMs) have shown promising performance on
complex multimodal tasks. However, they still face significant challenges: they
are highly sensitive to reasoning errors, require large volumes of annotated
data or accurate verifiers, and struggle to generalize beyond specific domains.
To address these limitations, we explore self-correction as a strategy to
enhance reasoning VLMs. We first conduct an in-depth analysis of reasoning
VLMs' self-correction abilities and identify key gaps. Based on our findings,
we introduce Sherlock, a self-correction and self-improvement training
framework. Sherlock introduces a trajectory-level self-correction objective, a
preference data construction method based on visual perturbation, and a dynamic
$\beta$ for preference tuning. Once the model acquires self-correction
capabilities using only 20k randomly sampled annotated data, it continues to
self-improve without external supervision. Built on the Llama3.2-Vision-11B
model, Sherlock achieves remarkable results across eight benchmarks, reaching
an average accuracy of 64.1 with direct generation and 65.4 after
self-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and
LlamaV-o1 (63.4) while using less than 20% of the annotated data.
\\ ( https://arxiv.org/abs/2505.22651 ,  6782kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22662 (*cross-listing*)
Date: Wed, 28 May 2025 17:59:53 GMT   (5549kb,D)

Title: AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models
Authors: Feng Luo, Yu-Neng Chuang, Guanchu Wang, Hoang Anh Duy Le, Shaochen
  Zhong, Hongyi Liu, Jiayi Yuan, Yang Sui, Vladimir Braverman, Vipin Chaudhary,
  Xia Hu
Categories: cs.CL cs.LG
\\
  The reasoning-capable large language models (LLMs) demonstrate strong
performance on complex reasoning tasks but often suffer from overthinking,
generating unnecessarily long chain-of-thought (CoT) reasoning paths for easy
reasoning questions, thereby increasing inference cost and latency. Recent
approaches attempt to address this challenge by manually deciding when to apply
long or short reasoning. However, they lack the flexibility to adapt CoT length
dynamically based on question complexity. In this paper, we propose Auto
Long-Short Reasoning (AutoL2S), a dynamic and model-agnostic framework that
enables LLMs to dynamically compress their generated reasoning path based on
the complexity of the reasoning question. AutoL2S enables a learned paradigm,
in which LLMs themselves can decide when longer reasoning is necessary and when
shorter reasoning suffices, by training on data annotated with our proposed
method, which includes both long and short CoT paths and a special <EASY>
token. We then use <EASY> token to indicate when the model can skip generating
lengthy CoT reasoning. This proposed annotation strategy can enhance the LLMs'
ability to generate shorter CoT reasoning paths with improved quality after
training. Extensive evaluation results show that AutoL2S reduces the length of
reasoning generation by up to 57% without compromising performance,
demonstrating the effectiveness of AutoL2S for scalable and efficient LLM
reasoning.
\\ ( https://arxiv.org/abs/2505.22662 ,  5549kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21585 (*cross-listing*)
Date: Tue, 27 May 2025 10:52:30 GMT   (2160kb,D)

Title: Improving flocking behaviors in street networks with vision
Authors: Guillaume Moinard, Matthieu Latapy
Categories: physics.soc-ph cs.MA cs.SI
\\
  We improve a flocking model on street networks introduced in a previous
paper. We expand the field of vision of walkers, making the model more
realistic. Under such conditions, we obtain groups of walkers whose gathering
times and robustness to break ups are better than previous results. We explain
such improvements because the alignment rule with vision guaranties walkers do
not split into divergent directions at intersections anymore, and because the
attraction rule with vision gathers distant groups. This paves the way to a
better understanding of events where walkers have collective decentralized
goals, like protests.
\\ ( https://arxiv.org/abs/2505.21585 ,  2160kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22053 (*cross-listing*)
Date: Wed, 28 May 2025 07:23:53 GMT   (6253kb,D)

Title: AudioGenie: A Training-Free Multi-Agent Framework for Diverse
  Multimodality-to-Multiaudio Generation
Authors: Yan Rong, Jinting Wang, Shan Yang, Guangzhi Lei, Li Liu
Categories: cs.SD cs.MA cs.MM eess.AS
\\
  Multimodality-to-Multiaudio (MM2MA) generation faces significant challenges
in synthesizing diverse and contextually aligned audio types (e.g., sound
effects, speech, music, and songs) from multimodal inputs (e.g., video, text,
images), owing to the scarcity of high-quality paired datasets and the lack of
robust multi-task learning frameworks. Recently, multi-agent system shows great
potential in tackling the above issues. However, directly applying it to MM2MA
task presents three critical challenges: (1) inadequate fine-grained
understanding of multimodal inputs (especially for video), (2) the inability of
single models to handle diverse audio events, and (3) the absence of
self-correction mechanisms for reliable outputs. To this end, we propose
AudioGenie, a novel training-free multi-agent system featuring a dual-layer
architecture with a generation team and a supervisor team. For the generation
team, a fine-grained task decomposition and an adaptive Mixture-of-Experts
(MoE) collaborative entity are designed for dynamic model selection, and a
trial-and-error iterative refinement module is designed for self-correction.
The supervisor team ensures temporal-spatial consistency and verifies outputs
through feedback loops. Moreover, we build MA-Bench, the first benchmark for
MM2MA tasks, comprising 198 annotated videos with multi-type audios.
Experiments demonstrate that our AudioGenie outperforms state-of-the-art (SOTA)
methods across 9 metrics in 8 tasks. User study further validate the
effectiveness of the proposed method in terms of quality, accuracy, alignment,
and aesthetic. The anonymous project website with samples can be found at
https://audiogenie.github.io/.
\\ ( https://arxiv.org/abs/2505.22053 ,  6253kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21754 (*cross-listing*)
Date: Tue, 27 May 2025 20:42:47 GMT   (21218kb)

Title: Visual Loop Closure Detection Through Deep Graph Consensus
Authors: Martin B\"uchner, Liza Dahiya, Simon Dorer, Vipul Ramtekkar, Kenji
  Nishimiya, Daniele Cattaneo, Abhinav Valada
Categories: cs.CV cs.RO
\\
  Visual loop closure detection traditionally relies on place recognition
methods to retrieve candidate loops that are validated using computationally
expensive RANSAC-based geometric verification. As false positive loop closures
significantly degrade downstream pose graph estimates, verifying a large number
of candidates in online simultaneous localization and mapping scenarios is
constrained by limited time and compute resources. While most deep loop closure
detection approaches only operate on pairs of keyframes, we relax this
constraint by considering neighborhoods of multiple keyframes when detecting
loops. In this work, we introduce LoopGNN, a graph neural network architecture
that estimates loop closure consensus by leveraging cliques of visually similar
keyframes retrieved through place recognition. By propagating deep feature
encodings among nodes of the clique, our method yields high-precision estimates
while maintaining high recall. Extensive experimental evaluations on the
TartanDrive 2.0 and NCLT datasets demonstrate that LoopGNN outperforms
traditional baselines. Additionally, an ablation study across various keypoint
extractors demonstrates that our method is robust, regardless of the type of
deep feature encodings used, and exhibits higher computational efficiency
compared to classical geometric verification baselines. We release our code,
supplementary material, and keyframe data at
https://loopgnn.cs.uni-freiburg.de.
\\ ( https://arxiv.org/abs/2505.21754 ,  21218kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22344 (*cross-listing*)
Date: Wed, 28 May 2025 13:27:42 GMT   (43044kb,D)

Title: Task-Driven Implicit Representations for Automated Design of LiDAR
  Systems
Authors: Nikhil Behari, Aaron Young, Akshat Dave, Ramesh Raskar
Categories: cs.CV cs.RO
\\
  Imaging system design is a complex, time-consuming, and largely manual
process; LiDAR design, ubiquitous in mobile devices, autonomous vehicles, and
aerial imaging platforms, adds further complexity through unique spatial and
temporal sampling requirements. In this work, we propose a framework for
automated, task-driven LiDAR system design under arbitrary constraints. To
achieve this, we represent LiDAR configurations in a continuous six-dimensional
design space and learn task-specific implicit densities in this space via
flow-based generative modeling. We then synthesize new LiDAR systems by
modeling sensors as parametric distributions in 6D space and fitting these
distributions to our learned implicit density using expectation-maximization,
enabling efficient, constraint-aware LiDAR system design. We validate our
method on diverse tasks in 3D vision, enabling automated LiDAR system design
across real-world-inspired applications in face scanning, robotic tracking, and
object detection.
\\ ( https://arxiv.org/abs/2505.22344 ,  43044kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22404 (*cross-listing*)
Date: Wed, 28 May 2025 14:34:32 GMT   (1392kb)

Title: Efficient Precision-Scalable Hardware for Microscaling (MX) Processing
  in Robotics Learning
Authors: Stef Cuyckens, Xiaoling Yi, Nitish Satya Murthy, Chao Fang, Marian
  Verhelst
Categories: cs.AR cs.RO
Comments: To appear in 2025 IEEE/ACM International Symposium on Low Power
  Electronics and Design (ISLPED 2025)
\\
  Autonomous robots require efficient on-device learning to adapt to new
environments without cloud dependency. For this edge training, Microscaling
(MX) data types offer a promising solution by combining integer and
floating-point representations with shared exponents, reducing energy
consumption while maintaining accuracy. However, the state-of-the-art
continuous learning processor, namely Dacapo, faces limitations with its
MXINT-only support and inefficient vector-based grouping during
backpropagation. In this paper, we present, to the best of our knowledge, the
first work that addresses these limitations with two key innovations: (1) a
precision-scalable arithmetic unit that supports all six MX data types by
exploiting sub-word parallelism and unified integer and floating-point
processing; and (2) support for square shared exponent groups to enable
efficient weight handling during backpropagation, removing storage redundancy
and quantization overhead. We evaluate our design against Dacapo under
iso-peak-throughput on four robotics workloads in TSMC 16nm FinFET technology
at 500MHz, reaching a 25.6% area reduction, a 51% lower memory footprint, and
4x higher effective training throughput while achieving comparable
energy-efficiency, enabling efficient robotics continual learning at the edge.
\\ ( https://arxiv.org/abs/2505.22404 ,  1392kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22421 (*cross-listing*)
Date: Wed, 28 May 2025 14:46:51 GMT   (6549kb)

Title: GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action
  Control
Authors: Anthony Chen, Wenzhao Zheng, Yida Wang, Xueyang Zhang, Kun Zhan, Peng
  Jia, Kurt Keutzer, Shangbang Zhang
Categories: cs.CV cs.RO
Comments: code will be released at https://github.com/antonioo-c/GeoDrive
\\
  Recent advancements in world models have revolutionized dynamic environment
simulation, allowing systems to foresee future states and assess potential
actions. In autonomous driving, these capabilities help vehicles anticipate the
behavior of other road users, perform risk-aware planning, accelerate training
in simulation, and adapt to novel scenarios, thereby enhancing safety and
reliability. Current approaches exhibit deficiencies in maintaining robust 3D
geometric consistency or accumulating artifacts during occlusion handling, both
critical for reliable safety assessment in autonomous navigation tasks. To
address this, we introduce GeoDrive, which explicitly integrates robust 3D
geometry conditions into driving world models to enhance spatial understanding
and action controllability. Specifically, we first extract a 3D representation
from the input frame and then obtain its 2D rendering based on the
user-specified ego-car trajectory. To enable dynamic modeling, we propose a
dynamic editing module during training to enhance the renderings by editing the
positions of the vehicles. Extensive experiments demonstrate that our method
significantly outperforms existing models in both action accuracy and 3D
spatial awareness, leading to more realistic, adaptable, and reliable scene
modeling for safer autonomous driving. Additionally, our model can generalize
to novel trajectories and offers interactive scene editing capabilities, such
as object editing and object trajectory control.
\\ ( https://arxiv.org/abs/2505.22421 ,  6549kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22429 (*cross-listing*)
Date: Wed, 28 May 2025 14:53:53 GMT   (3851kb)

Title: Zero-Shot 3D Visual Grounding from Vision-Language Models
Authors: Rong Li and Shijie Li and Lingdong Kong and Xulei Yang and Junwei
  Liang
Categories: cs.CV cs.RO
Comments: 3D-LLM/VLA @ CVPR 2025; Project Page at https://seeground.github.io/
\\
  3D Visual Grounding (3DVG) seeks to locate target objects in 3D scenes using
natural language descriptions, enabling downstream applications such as
augmented reality and robotics. Existing approaches typically rely on labeled
3D data and predefined categories, limiting scalability to open-world settings.
We present SeeGround, a zero-shot 3DVG framework that leverages 2D
Vision-Language Models (VLMs) to bypass the need for 3D-specific training. To
bridge the modality gap, we introduce a hybrid input format that pairs
query-aligned rendered views with spatially enriched textual descriptions. Our
framework incorporates two core components: a Perspective Adaptation Module
that dynamically selects optimal viewpoints based on the query, and a Fusion
Alignment Module that integrates visual and spatial signals to enhance
localization precision. Extensive evaluations on ScanRefer and Nr3D confirm
that SeeGround achieves substantial improvements over existing zero-shot
baselines -- outperforming them by 7.7% and 7.1%, respectively -- and even
rivals fully supervised alternatives, demonstrating strong generalization under
challenging conditions.
\\ ( https://arxiv.org/abs/2505.22429 ,  3851kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22436 (*cross-listing*)
Date: Wed, 28 May 2025 15:00:22 GMT   (3199kb)

Title: COSMOS: A Data-Driven Probabilistic Time Series simulator for Chemical
  Plumes across Spatial Scales
Authors: Arunava Nag, Floris van Breugel
Categories: stat.AP cs.RO
Comments: 16 pages, 4 primary figures
\\
  The development of robust odor navigation strategies for automated
environmental monitoring applications requires realistic simulations of odor
time series for agents moving across large spatial scales. Traditional
approaches that rely on computational fluid dynamics (CFD) methods can capture
the spatiotemporal dynamics of odor plumes, but are impractical for large-scale
simulations due to their computational expense. On the other hand, puff-based
simulations, although computationally tractable for large scales and capable of
capturing the stochastic nature of plumes, fail to reproduce naturalistic odor
statistics. Here, we present COSMOS (Configurable Odor Simulation Model over
Scalable Spaces), a data-driven probabilistic framework that synthesizes
realistic odor time series from spatial and temporal features of real datasets.
COSMOS generates similar distributions of key statistical features such as
whiff frequency, duration, and concentration as observed in real data, while
dramatically reducing computational overhead. By reproducing critical
statistical properties across a variety of flow regimes and scales, COSMOS
enables the development and evaluation of agent-based navigation strategies
with naturalistic odor experiences. To demonstrate its utility, we compare
odor-tracking agents exposed to CFD-generated plumes versus COSMOS simulations,
showing that both their odor experiences and resulting behaviors are quite
similar.
\\ ( https://arxiv.org/abs/2505.22436 ,  3199kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2310.08731
replaced with revised version Wed, 28 May 2025 15:48:23 GMT   (4617kb,D)

Title: Novelty Detection in Reinforcement Learning with World Models
Authors: Geigh Zollicoffer, Kenneth Eaton, Jonathan Balloch, Julia Kim, Wei
  Zhou, Robert Wright, Mark O. Riedl
Categories: cs.AI cs.LG cs.SY eess.SY
Comments: ICML Spotlight 2025
\\ ( https://arxiv.org/abs/2310.08731 ,  4617kb)
------------------------------------------------------------------------------
\\
arXiv:2406.12412
replaced with revised version Wed, 28 May 2025 13:25:29 GMT   (3493kb,D)

Title: Community Detection in Networks: A Rough Sets and Consensus Clustering
  Approach
Authors: Darian H. Grass-Boada, Leandro Gonz\'alez-Montesino, Rub\'en
  Arma\~nanzas
Categories: cs.AI cs.SI
Journal-ref: Applied Soft Computing, Volume 178, June 2025, 113219
DOI: 10.1016/j.asoc.2025.113219
\\ ( https://arxiv.org/abs/2406.12412 ,  3493kb)
------------------------------------------------------------------------------
\\
arXiv:2407.17535
replaced with revised version Wed, 28 May 2025 15:05:41 GMT   (15929kb,D)

Title: LAMBDA: A Large Model Based Data Agent
Authors: Maojun Sun, Ruijian Han, Binyan Jiang, Houduo Qi, Defeng Sun, Yancheng
  Yuan, and Jian Huang
Categories: cs.AI cs.LG cs.SE
Comments: 56 pages
MSC-class: 62-04, 62-08, 68T01, 68T09
\\ ( https://arxiv.org/abs/2407.17535 ,  15929kb)
------------------------------------------------------------------------------
\\
arXiv:2408.09946
replaced with revised version Wed, 28 May 2025 06:45:37 GMT   (3866kb,D)

Title: Fine-Grained and Thematic Evaluation of LLMs in Social Deduction Game
Authors: Byungjun Kim, Dayeon Seo, Bugeun Kim
Categories: cs.AI cs.CL
Comments: Under review, Modified title and content
\\ ( https://arxiv.org/abs/2408.09946 ,  3866kb)
------------------------------------------------------------------------------
\\
arXiv:2408.11326
replaced with revised version Wed, 28 May 2025 17:37:39 GMT   (1242kb)

Title: Automating Thought of Search: A Journey Towards Soundness and
  Completeness
Authors: Daniel Cao, Michael Katz, Harsha Kokel, Kavitha Srinivas, Shirin
  Sohrabi
Categories: cs.AI
\\ ( https://arxiv.org/abs/2408.11326 ,  1242kb)
------------------------------------------------------------------------------
\\
arXiv:2409.14191
replaced with revised version Wed, 28 May 2025 14:56:35 GMT   (1975kb,D)

Title: Addressing and Visualizing Misalignments in Human Task-Solving
  Trajectories
Authors: Sejin Kim and Hosung Lee and Sundong Kim
Categories: cs.AI cs.HC
Comments: KDD 2025 accepted
\\ ( https://arxiv.org/abs/2409.14191 ,  1975kb)
------------------------------------------------------------------------------
\\
arXiv:2501.12599
replaced with revised version Wed, 28 May 2025 03:57:30 GMT   (614kb,D)

Title: Kimi k1.5: Scaling Reinforcement Learning with LLMs
Authors: Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng
  Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang,
  Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood
  Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao
  Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu,
  Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su,
  Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu,
  Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei
  Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying
  Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao
  Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, et al. (30 additional
  authors not shown)
Categories: cs.AI cs.LG
Comments: 25 pages
\\ ( https://arxiv.org/abs/2501.12599 ,  614kb)
------------------------------------------------------------------------------
\\
arXiv:2501.19318
replaced with revised version Wed, 28 May 2025 16:59:03 GMT   (4270kb,D)

Title: MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented
  Reinforcement in Embodied Systems
Authors: Anirudh Chari, Suraj Reddy, Aditya Tiwari, Richard Lian, Brian Zhou
Categories: cs.AI
\\ ( https://arxiv.org/abs/2501.19318 ,  4270kb)
------------------------------------------------------------------------------
\\
arXiv:2502.04058
replaced with revised version Wed, 28 May 2025 14:05:11 GMT   (1281kb,D)

Title: Explanation Design in Strategic Learning: Sufficient Explanations that
  Induce Non-harmful Responses
Authors: Kiet Q. H. Vo, Siu Lun Chau, Masahiro Kato, Yixin Wang, Krikamol
  Muandet
Categories: cs.AI
\\ ( https://arxiv.org/abs/2502.04058 ,  1281kb)
------------------------------------------------------------------------------
\\
arXiv:2502.11357
replaced with revised version Wed, 28 May 2025 03:42:15 GMT   (4254kb,D)

Title: Explorer: Scaling Exploration-driven Web Trajectory Synthesis for
  Multimodal Web Agents
Authors: Vardaan Pahuja, Yadong Lu, Corby Rosset, Boyu Gou, Arindam Mitra,
  Spencer Whitehead, Yu Su, Ahmed Awadallah
Categories: cs.AI cs.HC
Comments: ACL 2025 (Findings)
\\ ( https://arxiv.org/abs/2502.11357 ,  4254kb)
------------------------------------------------------------------------------
\\
arXiv:2502.11882
replaced with revised version Wed, 28 May 2025 12:14:14 GMT   (5100kb,D)

Title: Leveraging Dual Process Theory in Language Agent Framework for Real-time
  Simultaneous Human-AI Collaboration
Authors: Shao Zhang, Xihuai Wang, Wenhao Zhang, Chaoran Li, Junru Song, Tingyu
  Li, Lin Qiu, Xuezhi Cao, Xunliang Cai, Wen Yao, Weinan Zhang, Xinbing Wang,
  Ying Wen
Categories: cs.AI cs.CL cs.HC cs.LG cs.MA
Comments: Accepted by ACL 2025 Main. Camera Ready Version
\\ ( https://arxiv.org/abs/2502.11882 ,  5100kb)
------------------------------------------------------------------------------
\\
arXiv:2502.16169
replaced with revised version Wed, 28 May 2025 13:56:42 GMT   (646kb,D)

Title: Patterns Over Principles: The Fragility of Inductive Reasoning in LLMs
  under Noisy Observations
Authors: Chunyang Li, Weiqi Wang, Tianshi Zheng, Yangqiu Song
Categories: cs.AI
\\ ( https://arxiv.org/abs/2502.16169 ,  646kb)
------------------------------------------------------------------------------
\\
arXiv:2502.16184
replaced with revised version Wed, 28 May 2025 12:48:43 GMT   (588kb,D)

Title: Robustness and Cybersecurity in the EU Artificial Intelligence Act
Authors: Henrik Nolte, Miriam Rateike, Mich\`ele Finck
Categories: cs.AI cs.CR cs.CY cs.LG
Journal-ref: The 2025 ACM Conference on Fairness, Accountability, and
  Transparency
DOI: 10.1145/3715275.3732020
\\ ( https://arxiv.org/abs/2502.16184 ,  588kb)
------------------------------------------------------------------------------
\\
arXiv:2502.20689
replaced with revised version Wed, 28 May 2025 07:27:05 GMT   (7456kb,D)

Title: WiseMind: Recontextualizing AI with a Knowledge-Guided, Theory-Informed
  Multi-Agent Framework for Instrumental and Humanistic Benefits
Authors: Yuqi Wu, Guangya Wan, Jingjing Li, Shengming Zhao, Lingfeng Ma, Tianyi
  Ye, Ion Pop, Yanbo Zhang, Jie Chen
Categories: cs.AI cs.CL
Comments: 27 pages, 13 figures
\\ ( https://arxiv.org/abs/2502.20689 ,  7456kb)
------------------------------------------------------------------------------
\\
arXiv:2503.01747
replaced with revised version Wed, 28 May 2025 15:29:03 GMT   (5873kb,D)

Title: Position: Don't Use the CLT in LLM Evals With Fewer Than a Few Hundred
  Datapoints
Authors: Sam Bowyer, Laurence Aitchison, Desi R. Ivanova
Categories: cs.AI cs.LG stat.ML
Comments: 42 pages, 39 figures. ICML 2025 Spotlight Position Paper
\\ ( https://arxiv.org/abs/2503.01747 ,  5873kb)
------------------------------------------------------------------------------
\\
arXiv:2503.10619
replaced with revised version Wed, 28 May 2025 09:31:33 GMT   (2013kb,D)

Title: Tempest: Autonomous Multi-Turn Jailbreaking of Large Language Models
  with Tree Search
Authors: Andy Zhou and Ron Arel
Categories: cs.AI cs.CL cs.CR
Comments: Accepted to ACL 2025 Main
\\ ( https://arxiv.org/abs/2503.10619 ,  2013kb)
------------------------------------------------------------------------------
\\
arXiv:2503.22241
replaced with revised version Wed, 28 May 2025 07:30:00 GMT   (3427kb,D)

Title: Agent-Centric Personalized Multiple Clustering with Multi-Modal LLMs
Authors: Ziye Chen, Yiqun Duan, Riheng Zhu, Zhenbang Sun, Mingming Gong
Categories: cs.AI
MSC-class: 68T07, 68T05, 05C82
\\ ( https://arxiv.org/abs/2503.22241 ,  3427kb)
------------------------------------------------------------------------------
\\
arXiv:2503.23631
replaced with revised version Tue, 27 May 2025 22:09:36 GMT   (1673kb,D)

Title: Intrinsically-Motivated Humans and Agents in Open-World Exploration
Authors: Aly Lidayan, Yuqing Du, Eliza Kosoy, Maria Rufova, Pieter Abbeel,
  Alison Gopnik
Categories: cs.AI
\\ ( https://arxiv.org/abs/2503.23631 ,  1673kb)
------------------------------------------------------------------------------
\\
arXiv:2505.10468
replaced with revised version Wed, 28 May 2025 01:28:08 GMT   (13076kb,D)

Title: AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and
  Challenges
Authors: Ranjan Sapkota, Konstantinos I. Roumeliotis, Manoj Karkee
Categories: cs.AI
Comments: 36 pages, 14 figures, 11 tables
\\ ( https://arxiv.org/abs/2505.10468 ,  13076kb)
------------------------------------------------------------------------------
\\
arXiv:2505.11792
replaced with revised version Wed, 28 May 2025 05:36:33 GMT   (564kb)

Title: Solver-Informed RL: Grounding Large Language Models for Authentic
  Optimization Modeling
Authors: Yitian Chen, Jingfan Xia, Siyu Shao, Dongdong Ge, Yinyu Ye
Categories: cs.AI
\\ ( https://arxiv.org/abs/2505.11792 ,  564kb)
------------------------------------------------------------------------------
\\
arXiv:2505.14510
replaced with revised version Tue, 27 May 2025 18:27:38 GMT   (2632kb,D)

Title: BACON: A fully explainable AI model with graded logic for decision
  making problems
Authors: Haishi Bai, Jozo Dujmovic, Jianwu Wang
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2505.14510 ,  2632kb)
------------------------------------------------------------------------------
\\
arXiv:2505.16832
replaced with revised version Tue, 27 May 2025 23:23:45 GMT   (8041kb,D)

Title: From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework
  for Reasoning-Driven Pedagogical Visualization
Authors: Haonian Ji, Shi Qiu, Siyang Xin, Siwei Han, Zhaorun Chen, Dake Zhang,
  Hongyi Wang, Huaxiu Yao
Categories: cs.AI cs.CL cs.CV cs.LG
Comments: 16 pages; 7 figures
\\ ( https://arxiv.org/abs/2505.16832 ,  8041kb)
------------------------------------------------------------------------------
\\
arXiv:2505.18139
replaced with revised version Tue, 27 May 2025 20:19:42 GMT   (568kb)

Title: Embracing Contradiction: Theoretical Inconsistency Will Not Impede the
  Road of Building Responsible AI Systems
Authors: Gordon Dai and Yunze Xiao
Categories: cs.AI cs.CY
Comments: 14 pages,2 figure
\\ ( https://arxiv.org/abs/2505.18139 ,  568kb)
------------------------------------------------------------------------------
\\
arXiv:2505.19099
replaced with revised version Wed, 28 May 2025 12:02:43 GMT   (7885kb,D)

Title: SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics
  Reasoning
Authors: Kun Xiang and Heng Li and Terry Jingchen Zhang and Yinya Huang and
  Zirong Liu and Peixin Qu and Jixi He and Jiaqi Chen and Yu-Jie Yuan and
  Jianhua Han and Hang Xu and Hanhui Li and Mrinmaya Sachan and Xiaodan Liang
Categories: cs.AI physics.ed-ph physics.pop-ph
Comments: 46 pages
\\ ( https://arxiv.org/abs/2505.19099 ,  7885kb)
------------------------------------------------------------------------------
\\
arXiv:2505.19641
replaced with revised version Wed, 28 May 2025 16:04:03 GMT   (424kb)

Title: SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning
  Logical Reasoning and Beyond
Authors: Junteng Liu, Yuanxiang Fan, Zhuo Jiang, Han Ding, Yongyi Hu, Chi
  Zhang, Yiqi Shi, Shitong Weng, Aili Chen, Shiqi Chen, Yunan Huang, Mozhi
  Zhang, Pengyu Zhao, Junjie Yan, Junxian He
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2505.19641 ,  424kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21419
replaced with revised version Wed, 28 May 2025 02:17:40 GMT   (2634kb)

Title: Diagnosing and Resolving Cloud Platform Instability with Multi-modal RAG
  LLMs
Authors: Yifan Wang and Kenneth P. Birman
Categories: cs.AI cs.OS
Comments: Published in EuroMLSys2025
Journal-ref: 2025, Association for Computing Machinery, Proceedings of the 5th
  Workshop on Machine Learning and Systems, 139-147, 9, series EuroMLSys '25
DOI: 10.1145/3721146.3721958
\\ ( https://arxiv.org/abs/2505.21419 ,  2634kb)
------------------------------------------------------------------------------
\\
arXiv:2201.07413
replaced with revised version Wed, 28 May 2025 13:31:02 GMT   (216kb,D)

Title: On Heuristic Models, Assumptions, and Parameters
Authors: Samuel Judson and Joan Feigenbaum
Categories: cs.CY
DOI: 10.1145/3735562
\\ ( https://arxiv.org/abs/2201.07413 ,  216kb)
------------------------------------------------------------------------------
\\
arXiv:2503.05787
replaced with revised version Wed, 28 May 2025 09:27:18 GMT   (702kb,D)

Title: Mapping the Regulatory Learning Space for the EU AI Act
Authors: Dave Lewis, Marta Lasek-Markey, Delaram Golpayegani, Harshvardhan J.
  Pandit
Categories: cs.CY
\\ ( https://arxiv.org/abs/2503.05787 ,  702kb)
------------------------------------------------------------------------------
\\
arXiv:2505.03842
replaced with revised version Wed, 28 May 2025 10:42:53 GMT   (3176kb,D)

Title: Coverage Biases in High-Resolution Satellite Imagery
Authors: Vadim Musienko, Axel Jacquet, Ingmar Weber, Till Koebe
Categories: cs.CY astro-ph.EP cs.CV
Comments: To appear in the proceedings of the ICWSM'25 Workshop on Data for the
  Wellbeing of Most Vulnerable (DWMV). Please cite accordingly
\\ ( https://arxiv.org/abs/2505.03842 ,  3176kb)
------------------------------------------------------------------------------
\\
arXiv:2205.09386
replaced with revised version Wed, 28 May 2025 15:17:36 GMT   (42kb)

Title: On the Distortion of Multi-winner Election Using Single-Candidate
  Ballots
Authors: Gennaro Auricchio, Zeyu Ren, Zihe Wang, Jie Zhang
Categories: cs.GT
\\ ( https://arxiv.org/abs/2205.09386 ,  42kb)
------------------------------------------------------------------------------
\\
arXiv:2208.09407
replaced with revised version Wed, 28 May 2025 13:18:44 GMT   (353kb,D)

Title: Learning in Stackelberg Games with Non-myopic Agents
Authors: Nika Haghtalab, Thodoris Lykouris, Sloan Nietert, Alexander Wei
Categories: cs.GT cs.DS cs.LG
Comments: An extended abstract of this work appeared at the ACM Conference on
  Economics and Computation (EC) 2022
\\ ( https://arxiv.org/abs/2208.09407 ,  353kb)
------------------------------------------------------------------------------
\\
arXiv:2411.01217
replaced with revised version Wed, 28 May 2025 04:22:11 GMT   (6702kb,D)

Title: Preference-CFR$\:$ Beyond Nash Equilibrium for Better Game Strategies
Authors: Qi Ju, Thomas Tellier, Meng Sun, Zhemei Fang, Yunfeng Luo
Categories: cs.GT cs.MA
\\ ( https://arxiv.org/abs/2411.01217 ,  6702kb)
------------------------------------------------------------------------------
\\
arXiv:2502.20770
replaced with revised version Wed, 28 May 2025 07:14:44 GMT   (132kb,D)

Title: Learning to Steer Learners in Games
Authors: Yizhou Zhang, Yi-An Ma, Eric Mazumdar
Categories: cs.GT cs.LG
\\ ( https://arxiv.org/abs/2502.20770 ,  132kb)
------------------------------------------------------------------------------
\\
arXiv:2505.07501
replaced with revised version Wed, 28 May 2025 13:00:53 GMT   (19kb)

Title: The Complexity of Pure Strategy Relevant Equilibria in Concurrent Games
Authors: Purandar Bhaduri
Categories: cs.GT cs.FL cs.LO cs.MA
\\ ( https://arxiv.org/abs/2505.07501 ,  19kb)
------------------------------------------------------------------------------
\\
arXiv:2505.14547
replaced with revised version Tue, 27 May 2025 22:43:54 GMT   (3182kb)

Title: GUARD: Constructing Realistic Two-Player Matrix and Security Games for
  Benchmarking Game-Theoretic Algorithms
Authors: Noah Krever, Jakub \v{C}ern\'y, Mo\"ise Blanchard, Christian Kroer
Categories: cs.GT
\\ ( https://arxiv.org/abs/2505.14547 ,  3182kb)
------------------------------------------------------------------------------
\\
arXiv:2409.08577
replaced with revised version Wed, 28 May 2025 06:11:58 GMT   (3527kb,D)

Title: Exploring Remote Collaborative Tasks: The Impact of Avatar
  Representation on Dyadic Haptic Interactions in Shared Virtual Environments
Authors: Genki Sasaki, Hiroshi Igarashi
Categories: cs.HC cs.RO
\\ ( https://arxiv.org/abs/2409.08577 ,  3527kb)
------------------------------------------------------------------------------
\\
arXiv:2410.03724
replaced with revised version Wed, 28 May 2025 16:01:48 GMT   (1951kb,D)

Title: Overcoming the Machine Penalty with Imperfectly Fair AI Agents
Authors: Zhen Wang, Ruiqi Song, Chen Shen, Shiya Yin, Zhao Song, Balaraju
  Battu, Lei Shi, Danyang Jia, Talal Rahwan, Shuyue Hu
Categories: cs.HC cs.AI cs.GT econ.GN q-fin.EC
\\ ( https://arxiv.org/abs/2410.03724 ,  1951kb)
------------------------------------------------------------------------------
\\
arXiv:2411.11835
replaced with revised version Tue, 27 May 2025 18:42:47 GMT   (12539kb,D)

Title: Describe Now: User-Driven Audio Description for Blind and Low Vision
  Individuals
Authors: Maryam Cheema, Hasti Seifi, and Pooyan Fazli
Categories: cs.HC
Comments: 17 pages, 14 figures
\\ ( https://arxiv.org/abs/2411.11835 ,  12539kb)
------------------------------------------------------------------------------
\\
arXiv:2503.07320
replaced with revised version Wed, 28 May 2025 07:51:40 GMT   (15206kb,D)

Title: When Trust Collides: Decoding Human-LLM Cooperation Dynamics through the
  Prisoner's Dilemma
Authors: Guanxuan Jiang, Shirao Yang, Yuyang Wang and Pan Hui
Categories: cs.HC cs.AI
\\ ( https://arxiv.org/abs/2503.07320 ,  15206kb)
------------------------------------------------------------------------------
\\
arXiv:2503.20262
replaced with revised version Wed, 28 May 2025 06:36:46 GMT   (23853kb,D)

Title: From the CDC to emerging infectious disease publics: The long-now of
  polarizing and complex health crises
Authors: Tawfiq Ammari, Anna Gutowska, Jacob Ziff, Casey Randazzo, Harihan
  Subramonyam
Categories: cs.HC cs.SI
\\ ( https://arxiv.org/abs/2503.20262 ,  23853kb)
------------------------------------------------------------------------------
\\
arXiv:2504.13904
replaced with revised version Wed, 28 May 2025 08:37:07 GMT   (5480kb,D)

Title: Generative Framework for Personalized Persuasion: Inferring Causal,
  Counterfactual, and Latent Knowledge
Authors: Donghuo Zeng, Roberto Legaspi, Yuewen Sun, Xinshuai Dong, Kazushi
  Ikeda, Peter Spirtes, Kun Zhang
Categories: cs.HC cs.AI cs.CL
Comments: 12 pages, 10 figures, 1 table. Accepted by ACM UMAP 2025
\\ ( https://arxiv.org/abs/2504.13904 ,  5480kb)
------------------------------------------------------------------------------
\\
arXiv:2505.17937
replaced with revised version Wed, 28 May 2025 06:32:16 GMT   (1279kb)

Title: Survival Games: Human-LLM Strategic Showdowns under Severe Resource
  Scarcity
Authors: Zhihong Chen, Yiqian Yang, Jinzhao Zhou, Qiang Zhang, Chin-Teng Lin,
  Yiqun Duan
Categories: cs.HC
\\ ( https://arxiv.org/abs/2505.17937 ,  1279kb)
------------------------------------------------------------------------------
\\
arXiv:2410.16875
replaced with revised version Wed, 28 May 2025 09:30:06 GMT   (10214kb)

Title: Edge-Spreading Raptor-Like LDPC Codes for 6G Wireless Systems
Authors: Yuqing Ren, Leyu Zhang, Yifei Shen, Wenqing Song, Emmanuel Boutillon,
  Alexios Balatsoukas-Stimming, and Andreas Burg
Categories: cs.IT math.IT
Comments: accepted by IEEE Transactions on Communications
\\ ( https://arxiv.org/abs/2410.16875 ,  10214kb)
------------------------------------------------------------------------------
\\
arXiv:2411.16596
replaced with revised version Wed, 28 May 2025 16:26:39 GMT   (22kb,D)

Title: Bivariate Linear Operator Codes
Authors: Aaron L. Putterman, Vadim Zaripov
Categories: cs.IT math.IT
\\ ( https://arxiv.org/abs/2411.16596 ,  22kb)
------------------------------------------------------------------------------
\\
arXiv:2504.06790
replaced with revised version Wed, 28 May 2025 17:19:42 GMT   (226kb,D)

Title: Analog Computing for Signal Processing and Communications -- Part I:
  Computing with Microwave Networks
Authors: Matteo Nerini, Bruno Clerckx
Categories: cs.IT eess.SP math.IT
Comments: Submitted to IEEE for publication
\\ ( https://arxiv.org/abs/2504.06790 ,  226kb)
------------------------------------------------------------------------------
\\
arXiv:2504.07477
replaced with revised version Wed, 28 May 2025 17:27:14 GMT   (235kb,D)

Title: Analog Computing for Signal Processing and Communications -- Part II:
  Toward Gigantic MIMO Beamforming
Authors: Matteo Nerini, Bruno Clerckx
Categories: cs.IT eess.SP math.IT
Comments: Submitted to IEEE for publication
\\ ( https://arxiv.org/abs/2504.07477 ,  235kb)
------------------------------------------------------------------------------
\\
arXiv:2505.16828
replaced with revised version Wed, 28 May 2025 11:19:32 GMT   (2723kb)

Title: Rotatable Antenna Enabled Wireless Communication and Sensing:
  Opportunities and Challenges
Authors: Beixiong Zheng, Tiantian Ma, Changsheng You, Jie Tang, Robert Schober,
  Rui Zhang
Categories: cs.IT math.IT
Comments: 8 pages, 5 figures
\\ ( https://arxiv.org/abs/2505.16828 ,  2723kb)
------------------------------------------------------------------------------
\\
arXiv:2303.09981
replaced with revised version Tue, 27 May 2025 23:52:32 GMT   (2720kb,D)

Title: Inferring Traffic Models in Terminal Airspace from Flight Tracks and
  Procedures
Authors: Soyeon Jung, Amelia Hardy, and Mykel J. Kochenderfer
Categories: cs.LG
\\ ( https://arxiv.org/abs/2303.09981 ,  2720kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19717
replaced with revised version Wed, 28 May 2025 13:51:47 GMT   (225kb,D)

Title: An Empirical Evaluation of Rewiring Approaches in Graph Neural Networks
Authors: Alessio Micheli, Domenico Tortorella
Categories: cs.LG
Comments: 8 pages, 4 figures
\\ ( https://arxiv.org/abs/2305.19717 ,  225kb)
------------------------------------------------------------------------------
\\
arXiv:2306.07651
replaced with revised version Wed, 28 May 2025 15:07:08 GMT   (31250kb,D)

Title: Variational Positive-incentive Noise: How Noise Benefits Models
Authors: Hongyuan Zhang, Sida Huang, Yubin Guo, Xuelong Li
Categories: cs.LG cs.CV
Comments: Acceptted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence
\\ ( https://arxiv.org/abs/2306.07651 ,  31250kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04378
replaced with revised version Tue, 27 May 2025 21:31:20 GMT   (8905kb,D)

Title: Watermarks in the Sand: Impossibility of Strong Watermarking for
  Generative Models
Authors: Hanlin Zhang, Benjamin L. Edelman, Danilo Francati, Daniele Venturi,
  Giuseppe Ateniese, Boaz Barak
Categories: cs.LG cs.CL cs.CR
Comments: ICML 2024. Website: https://hanlin-zhang.com/impossibility-watermarks
\\ ( https://arxiv.org/abs/2311.04378 ,  8905kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10690
replaced with revised version Wed, 28 May 2025 10:02:30 GMT   (384kb,D)

Title: Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and
  unfairness in dyadic regression models
Authors: Jorge Paz-Ruza, Amparo Alonso-Betanzos, Bertha Guijarro-Berdi\~nas,
  Brais Cancela, Carlos Eiras-Franco
Categories: cs.LG cs.AI cs.IR
\\ ( https://arxiv.org/abs/2401.10690 ,  384kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02933
replaced with revised version Wed, 28 May 2025 15:34:43 GMT   (3209kb,D)

Title: Intrinsic User-Centric Interpretability through Global Mixture of
  Experts
Authors: Vinitra Swamy, Syrielle Montariol, Julian Blackwell, Jibril Frej,
  Martin Jaggi, Tanja K\"aser
Categories: cs.LG cs.CY cs.HC
Comments: Accepted as a full paper at ICLR 2025 (top 5% of scores) in Singapore
\\ ( https://arxiv.org/abs/2402.02933 ,  3209kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19163
replaced with revised version Wed, 28 May 2025 14:34:11 GMT   (7271kb,D)

Title: Decoupled Subgraph Federated Learning
Authors: Javad Aliakbari and Johan \"Ostman and Alexandre Graell i Amat
Categories: cs.LG cs.IT math.IT
Comments: This paper has been published at the International Conference on
  Learning Representations (ICLR), 2025
Journal-ref: International Conference on Learning Representations (ICLR), 2025
\\ ( https://arxiv.org/abs/2402.19163 ,  7271kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03219
replaced with revised version Wed, 28 May 2025 09:30:28 GMT   (61kb)

Title: LC-Tsallis-INF: Generalized Best-of-Both-Worlds Linear Contextual
  Bandits
Authors: Masahiro Kato and Shinji Ito
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2403.03219 ,  61kb)
------------------------------------------------------------------------------
\\
arXiv:2405.01052
replaced with revised version Tue, 27 May 2025 20:30:14 GMT   (9702kb,D)

Title: Polynomial Chaos Expanded Gaussian Process
Authors: Dominik Polke, Tim K\"osters, Elmar Ahle, Dirk S\"offker
Categories: cs.LG cs.SY eess.SY
Comments: Manuscript: 38 pages, 12 figures, 11 tables
\\ ( https://arxiv.org/abs/2405.01052 ,  9702kb)
------------------------------------------------------------------------------
\\
arXiv:2405.12380
replaced with revised version Tue, 27 May 2025 18:57:06 GMT   (12406kb,D)

Title: Fast meta-solvers for 3D complex-shape scatterers using neural operators
  trained on a non-scattering problem
Authors: Youngkyu Lee, Shanqing Liu, Zongren Zou, Adar Kahana, Eli Turkel,
  Rishikesh Ranade, Jay Pathak, George Em Karniadakis
Categories: cs.LG physics.comp-ph
\\ ( https://arxiv.org/abs/2405.12380 ,  12406kb)
------------------------------------------------------------------------------
\\
arXiv:2405.19933
replaced with revised version Wed, 28 May 2025 07:44:29 GMT   (2573kb,D)

Title: Learning Latent Graph Structures and their Uncertainty
Authors: Alessandro Manenti, Daniele Zambon, Cesare Alippi
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2405.19933 ,  2573kb)
------------------------------------------------------------------------------
\\
arXiv:2406.04848
replaced with revised version Tue, 27 May 2025 20:08:35 GMT   (1721kb,D)

Title: CTBENCH: A Library and Benchmark for Certified Training
Authors: Yuhao Mao, Stefan Balauca, Martin Vechev
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2406.04848 ,  1721kb)
------------------------------------------------------------------------------
\\
arXiv:2406.09262
replaced with revised version Wed, 28 May 2025 17:30:47 GMT   (12104kb,D)

Title: Fully Heteroscedastic Count Regression with Deep Double Poisson Networks
Authors: Spencer Young, Porter Jenkins, Longchao Da, Jeff Dotson, Hua Wei
Categories: cs.LG
Comments: Forty-Second International Conference on Machine Learning (ICML 2025)
\\ ( https://arxiv.org/abs/2406.09262 ,  12104kb)
------------------------------------------------------------------------------
\\
arXiv:2407.08250
replaced with revised version Wed, 28 May 2025 09:10:05 GMT   (7261kb,D)

Title: Gradient Boosting Reinforcement Learning
Authors: Benjamin Fuhrer, Chen Tessler, Gal Dalal
Categories: cs.LG cs.AI
Comments: to be published in the Forty-Second International Conference on
  Machine Learning
\\ ( https://arxiv.org/abs/2407.08250 ,  7261kb)
------------------------------------------------------------------------------
\\
arXiv:2407.19580
replaced with revised version Wed, 28 May 2025 01:08:37 GMT   (110kb,D)

Title: Mini-batch Coresets for Memory-efficient Language Model Training on Data
  Mixtures
Authors: Dang Nguyen, Wenhan Yang, Rathul Anand, Yu Yang, Baharan Mirzasoleiman
Categories: cs.LG cs.AI cs.CL
Comments: 21 pages, 6 figures, 9 tables, link:
  https://github.com/BigML-CS-UCLA/CoLM
\\ ( https://arxiv.org/abs/2407.19580 ,  110kb)
------------------------------------------------------------------------------
\\
arXiv:2409.10803
replaced with revised version Wed, 28 May 2025 07:40:06 GMT   (2622kb)

Title: Quantum Kernel Learning for Small Dataset Modeling in Semiconductor
  Fabrication: Application to Ohmic Contact
Authors: Zeheng Wang, Fangzhou Wang, Liang Li, Zirui Wang, Timothy van der
  Laan, Ross C. C. Leon, Jing-Kai Huang, Muhammad Usman
Categories: cs.LG cs.ET quant-ph
Comments: Journal version 2.0
\\ ( https://arxiv.org/abs/2409.10803 ,  2622kb)
------------------------------------------------------------------------------
\\
arXiv:2409.11064
replaced with revised version Wed, 28 May 2025 08:04:12 GMT   (1645kb,D)

Title: A Hybrid Multi-Factor Network with Dynamic Sequence Modeling for Early
  Warning of Intraoperative Hypotension
Authors: Mingyue Cheng and Jintao Zhang and Zhiding Liu and Chunli Liu
Categories: cs.LG cs.RO
\\ ( https://arxiv.org/abs/2409.11064 ,  1645kb)
------------------------------------------------------------------------------
\\
arXiv:2409.18289
replaced with revised version Wed, 28 May 2025 02:54:17 GMT   (5713kb,D)

Title: Criticality and Safety Margins for Reinforcement Learning
Authors: Alexander Grushin, Walt Woods, Alvaro Velasquez, Simon Khan
Categories: cs.LG cs.AI cs.SY eess.SY
Comments: 17 pages, 10 figures
MSC-class: 68T07
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2409.18289 ,  5713kb)
------------------------------------------------------------------------------
\\
arXiv:2410.01308
replaced with revised version Wed, 28 May 2025 11:20:36 GMT   (35kb)

Title: Rethinking GNN Expressive Power from a Distributed Computational Model
  Perspective
Authors: Guanyu Cui, Yuhe Guo, Zhewei Wei, Hsin-Hao Su
Categories: cs.LG cs.AI
MSC-class: +
\\ ( https://arxiv.org/abs/2410.01308 ,  35kb)
------------------------------------------------------------------------------
\\
arXiv:2410.02622
replaced with revised version Wed, 28 May 2025 12:34:58 GMT   (5818kb,D)

Title: Diss-l-ECT: Dissecting Graph Data with Local Euler Characteristic
  Transforms
Authors: Julius von Rohrscheidt and Bastian Rieck
Categories: cs.LG math.AT
Comments: Accepted at ICML 2025
\\ ( https://arxiv.org/abs/2410.02622 ,  5818kb)
------------------------------------------------------------------------------
\\
arXiv:2410.03810
replaced with revised version Wed, 28 May 2025 12:39:33 GMT   (8433kb,D)

Title: Exploring the Limitations of Mamba in COPY and CoT Reasoning
Authors: Ruifeng Ren, Zhicong Li and Yong Liu
Categories: cs.LG cs.AI cs.CL
Comments: Mamba, Chain of Thought
\\ ( https://arxiv.org/abs/2410.03810 ,  8433kb)
------------------------------------------------------------------------------
\\
arXiv:2410.06851
replaced with revised version Wed, 28 May 2025 08:41:05 GMT   (2598kb,D)

Title: Understanding Model Ensemble in Transferable Adversarial Attack
Authors: Wei Yao, Zeliang Zhang, Huayi Tang, Yong Liu
Categories: cs.LG cs.AI
Comments: Accepted by ICML 2025
\\ ( https://arxiv.org/abs/2410.06851 ,  2598kb)
------------------------------------------------------------------------------
\\
arXiv:2410.07430
replaced with revised version Wed, 28 May 2025 12:02:08 GMT   (346kb,D)

Title: EventFlow: Forecasting Temporal Point Processes with Flow Matching
Authors: Gavin Kerrigan, Kai Nelson, Padhraic Smyth
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2410.07430 ,  346kb)
------------------------------------------------------------------------------
\\
arXiv:2410.11924
replaced with revised version Wed, 28 May 2025 11:19:31 GMT   (23601kb,D)

Title: NRFormer: Nationwide Nuclear Radiation Forecasting with Spatio-Temporal
  Transformer
Authors: Tengfei Lyu, Jindong Han, Hao Liu
Categories: cs.LG
Comments: Accepted by KDD 2025 ADS Track
\\ ( https://arxiv.org/abs/2410.11924 ,  23601kb)
------------------------------------------------------------------------------
\\
arXiv:2410.14040
replaced with revised version Wed, 28 May 2025 05:15:35 GMT   (10083kb,D)

Title: Latent Weight Diffusion: Generating reactive policies instead of
  trajectories
Authors: Shashank Hegde, Satyajeet Das, Gautam Salhotra, Gaurav S. Sukhatme
Categories: cs.LG cs.AI cs.RO
\\ ( https://arxiv.org/abs/2410.14040 ,  10083kb)
------------------------------------------------------------------------------
\\
arXiv:2411.02957
replaced with revised version Wed, 28 May 2025 15:15:49 GMT   (3682kb,D)

Title: Embedding Safety into RL: A New Take on Trust Region Methods
Authors: Nikola Milosevic and Johannes M\"uller and Nico Scherf
Categories: cs.LG cs.SY eess.SY
Comments: Accepted at ICML 2025
Journal-ref: Proceedings of the 42nd International Conference on Machine
  Learning, Vancouver, Canada. PMLR 267, 2025
\\ ( https://arxiv.org/abs/2411.02957 ,  3682kb)
------------------------------------------------------------------------------
\\
arXiv:2411.12155
replaced with revised version Wed, 28 May 2025 01:11:21 GMT   (1688kb,D)

Title: Coarse-to-fine Q-Network with Action Sequence for Data-Efficient Robot
  Learning
Authors: Younggyo Seo, Pieter Abbeel
Categories: cs.LG cs.AI cs.RO
Comments: 17 Pages. Website: https://younggyo.me/cqn-as/
\\ ( https://arxiv.org/abs/2411.12155 ,  1688kb)
------------------------------------------------------------------------------
\\
arXiv:2411.14251
replaced with revised version Wed, 28 May 2025 00:04:45 GMT   (3035kb,D)

Title: Natural Language Reinforcement Learning
Authors: Xidong Feng, Bo Liu, Yan Song, Haotian Fu, Ziyu Wan, Girish A.
  Koushik, Zhiyuan Hu, Mengyue Yang, Ying Wen, Jun Wang
Categories: cs.LG cs.AI cs.CL
Comments: 10 pages
\\ ( https://arxiv.org/abs/2411.14251 ,  3035kb)
------------------------------------------------------------------------------
\\
arXiv:2411.17284
replaced with revised version Wed, 28 May 2025 17:16:40 GMT   (5417kb,D)

Title: AutoElicit: Using Large Language Models for Expert Prior Elicitation in
  Predictive Modelling
Authors: Alexander Capstick, Rahul G. Krishnan, Payam Barnaghi
Categories: cs.LG cs.CL stat.ML
\\ ( https://arxiv.org/abs/2411.17284 ,  5417kb)
------------------------------------------------------------------------------
\\
arXiv:2411.17941
replaced with revised version Wed, 28 May 2025 00:54:03 GMT   (6118kb,D)

Title: Multi-Label Bayesian Active Learning with Inter-Label Relationships
Authors: Yuanyuan Qi, Jueqing Lu, Xiaohao Yang, Joanne Enticott, Lan Du
Categories: cs.LG
\\ ( https://arxiv.org/abs/2411.17941 ,  6118kb)
------------------------------------------------------------------------------
\\
arXiv:2411.18919
replaced with revised version Wed, 28 May 2025 02:22:15 GMT   (3556kb,D)

Title: Federated Continual Graph Learning
Authors: Yinlin Zhu, Miao Hu, Di Wu
Categories: cs.LG cs.AI cs.DB cs.SI
Comments: Accepted by SIGKDD 2025
\\ ( https://arxiv.org/abs/2411.18919 ,  3556kb)
------------------------------------------------------------------------------
\\
arXiv:2412.09059
replaced with revised version Wed, 28 May 2025 03:50:00 GMT   (7853kb,D)

Title: Go With the Flow: Fast Diffusion for Gaussian Mixture Models
Authors: George Rapakoulias, Ali Reza Pedram, Fengjiao Liu, Lingjiong Zhu, and
  Panagiotis Tsiotras
Categories: cs.LG
\\ ( https://arxiv.org/abs/2412.09059 ,  7853kb)
------------------------------------------------------------------------------
\\
arXiv:2412.10193
replaced with revised version Tue, 27 May 2025 20:06:29 GMT   (2760kb,D)

Title: Simple Guidance Mechanisms for Discrete Diffusion Models
Authors: Yair Schiff, Subham Sekhar Sahoo, Hao Phung, Guanghan Wang, Sam
  Boshar, Hugo Dalla-torre, Bernardo P. de Almeida, Alexander Rush, Thomas
  Pierrot, Volodymyr Kuleshov
Categories: cs.LG
Comments: ICLR 2025; Code to reproduce our experiments is available here:
  https://github.com/kuleshov-group/discrete-diffusion-guidance
\\ ( https://arxiv.org/abs/2412.10193 ,  2760kb)
------------------------------------------------------------------------------
\\
arXiv:2412.11180
replaced with revised version Wed, 28 May 2025 15:25:03 GMT   (9351kb,D)

Title: GNNs-to-MLPs by Teacher Injection and Dirichlet Energy Distillation
Authors: Ziang Zhou, Zhihao Ding, Jieming Shi, Qing Li, Shiqi Shen
Categories: cs.LG cs.SI
Comments: 17 pages
\\ ( https://arxiv.org/abs/2412.11180 ,  9351kb)
------------------------------------------------------------------------------
\\
arXiv:2412.18946
replaced with revised version Tue, 27 May 2025 18:44:41 GMT   (1621kb,D)

Title: Constraint-Adaptive Policy Switching for Offline Safe Reinforcement
  Learning
Authors: Yassine Chemingui, Aryan Deshwal, Honghao Wei, Alan Fern, Janardhan
  Rao Doppa
Categories: cs.LG cs.AI
Comments: Published in Proceedings of the AAAI Conference on Artificial
  Intelligence, 2025
\\ ( https://arxiv.org/abs/2412.18946 ,  1621kb)
------------------------------------------------------------------------------
\\
arXiv:2501.10543
replaced with revised version Tue, 27 May 2025 23:13:18 GMT   (2658kb,D)

Title: An Innovative Data-Driven and Adaptive Reinforcement Learning Approach
  for Context-Aware Prescriptive Process Monitoring
Authors: Mostafa Abbasi, Maziyar Khadivi, Maryam Ahang, Patricia Lasserre, Yves
  Lucet, Homayoun Najjaran
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2501.10543 ,  2658kb)
------------------------------------------------------------------------------
\\
arXiv:2501.15925
replaced with revised version Wed, 28 May 2025 03:23:09 GMT   (7863kb,D)

Title: Efficient Logit-based Knowledge Distillation of Deep Spiking Neural
  Networks for Full-Range Timestep Deployment
Authors: Chengting Yu, Xiaochen Zhao, Lei Liu, Shu Yang, Gaoang Wang, Erping
  Li, Aili Wang
Categories: cs.LG q-bio.NC
Comments: Accepted by ICML 2025
\\ ( https://arxiv.org/abs/2501.15925 ,  7863kb)
------------------------------------------------------------------------------
\\
arXiv:2501.16265
replaced with revised version Tue, 27 May 2025 20:00:08 GMT   (1073kb,D)

Title: Training Dynamics of In-Context Learning in Linear Attention
Authors: Yedi Zhang, Aaditya K. Singh, Peter E. Latham, Andrew Saxe
Categories: cs.LG
Comments: ICML 2025 Spotlight
\\ ( https://arxiv.org/abs/2501.16265 ,  1073kb)
------------------------------------------------------------------------------
\\
arXiv:2501.16349
replaced with revised version Wed, 28 May 2025 12:49:36 GMT   (829kb)

Title: Risk-Informed Diffusion Transformer for Long-Tail Trajectory Prediction
  in the Crash Scenario
Authors: Junlan Chen, Pei Liu, Zihao Zhang, Hongyi Zhao, Yufei Ji, Ziyuan Pu
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2501.16349 ,  829kb)
------------------------------------------------------------------------------
\\
arXiv:2501.19200
replaced with revised version Wed, 28 May 2025 12:26:27 GMT   (2056kb,D)

Title: A Variational Perspective on Generative Protein Fitness Optimization
Authors: Lea Bogensperger, Dominik Narnhofer, Ahmed Allam, Konrad Schindler,
  Michael Krauthammer
Categories: cs.LG
\\ ( https://arxiv.org/abs/2501.19200 ,  2056kb)
------------------------------------------------------------------------------
\\
arXiv:2501.19345
replaced with revised version Wed, 28 May 2025 07:13:54 GMT   (898kb,D)

Title: PUATE: Efficient Average Treatment Effect Estimation from Treated
  (Positive) and Unlabeled Units
Authors: Masahiro Kato and Fumiaki Kozai and Ryo Inokuchi
Categories: cs.LG econ.EM math.ST stat.ME stat.ML stat.TH
\\ ( https://arxiv.org/abs/2501.19345 ,  898kb)
------------------------------------------------------------------------------
\\
arXiv:2502.00361
replaced with revised version Wed, 28 May 2025 06:35:04 GMT   (5599kb,D)

Title: Efficient Online Reinforcement Learning for Diffusion Policy
Authors: Haitong Ma, Tianyi Chen, Kai Wang, Na Li, Bo Dai
Categories: cs.LG
Comments: 17 pages, 5 figures
\\ ( https://arxiv.org/abs/2502.00361 ,  5599kb)
------------------------------------------------------------------------------
\\
arXiv:2502.01397
replaced with revised version Wed, 28 May 2025 07:53:55 GMT   (1574kb,D)

Title: Message-Passing GNNs Fail to Approximate Sparse Triangular
  Factorizations
Authors: Vladislav Trifonov, Ekaterina Muravleva, Ivan Oseledets
Categories: cs.LG cs.AI cs.NA math.NA
\\ ( https://arxiv.org/abs/2502.01397 ,  1574kb)
------------------------------------------------------------------------------
\\
arXiv:2502.01562
replaced with revised version Wed, 28 May 2025 13:55:35 GMT   (250kb,D)

Title: Memento No More: Coaching AI Agents to Master Multiple Tasks via Hints
  Internalization
Authors: Minttu Alakuijala, Ya Gao, Georgy Ananov, Samuel Kaski, Pekka
  Marttinen, Alexander Ilin, Harri Valpola
Categories: cs.LG
\\ ( https://arxiv.org/abs/2502.01562 ,  250kb)
------------------------------------------------------------------------------
\\
arXiv:2502.01657
replaced with revised version Wed, 28 May 2025 11:56:33 GMT   (624kb,D)

Title: Improving Rule-based Reasoning in LLMs using Neurosymbolic
  Representations
Authors: Varun Dhanraj, Chris Eliasmith
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2502.01657 ,  624kb)
------------------------------------------------------------------------------
\\
arXiv:2502.01930
replaced with revised version Tue, 27 May 2025 18:54:47 GMT   (215kb,D)

Title: Robust LLM Alignment via Distributionally Robust Direct Preference
  Optimization
Authors: Zaiyan Xu, Sushil Vemuri, Kishan Panaganti, Dileep Kalathil, Rahul
  Jain, Deepak Ramachandran
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2502.01930 ,  215kb)
------------------------------------------------------------------------------
\\
arXiv:2502.02121
replaced with revised version Wed, 28 May 2025 14:26:35 GMT   (817kb,D)

Title: BILBO: BILevel Bayesian Optimization
Authors: Ruth Wan Theng Chew, Quoc Phong Nguyen, Bryan Kian Hsiang Low
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2502.02121 ,  817kb)
------------------------------------------------------------------------------
\\
arXiv:2502.02748
replaced with revised version Wed, 28 May 2025 02:22:45 GMT   (700kb,D)

Title: ReGNet: Reciprocal Space-Aware Long-Range Modeling for Crystalline
  Property Prediction
Authors: Jianan Nie, Peiyao Xiao, Kaiyi Ji, Peng Gao
Categories: cs.LG cond-mat.mtrl-sci
\\ ( https://arxiv.org/abs/2502.02748 ,  700kb)
------------------------------------------------------------------------------
\\
arXiv:2502.02921
replaced with revised version Wed, 28 May 2025 04:40:46 GMT   (4692kb,D)

Title: Robust Reward Alignment via Hypothesis Space Batch Cutting
Authors: Zhixian Xie, Haode Zhang, Yizhe Feng, Wanxin Jin
Categories: cs.LG
Comments: 19 pages, including appendix
\\ ( https://arxiv.org/abs/2502.02921 ,  4692kb)
------------------------------------------------------------------------------
\\
arXiv:2502.03424
replaced with revised version Wed, 28 May 2025 16:18:35 GMT   (1185kb,D)

Title: Prediction of the Most Fire-Sensitive Point in Building Structures with
  Differentiable Agents for Thermal Simulators
Authors: Yuan Xinjie and Khalid M. Mosalam
Categories: cs.LG
Comments: This paper has been accepted by journal Computer-Aided Civil and
  Infrastructure Engineering
\\ ( https://arxiv.org/abs/2502.03424 ,  1185kb)
------------------------------------------------------------------------------
\\
arXiv:2502.03540
replaced with revised version Tue, 27 May 2025 20:39:43 GMT   (14200kb,D)

Title: Path Planning for Masked Diffusion Model Sampling
Authors: Fred Zhangzhi Peng, Zachary Bezemek, Sawan Patel, Jarrid
  Rector-Brooks, Sherwood Yao, Avishek Joey Bose, Alexander Tong, and Pranam
  Chatterjee
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2502.03540 ,  14200kb)
------------------------------------------------------------------------------
\\
arXiv:2502.03773
replaced with revised version Wed, 28 May 2025 02:57:54 GMT   (603kb,D)

Title: ExpProof : Operationalizing Explanations for Confidential Models with
  ZKPs
Authors: Chhavi Yadav, Evan Monroe Laufer, Dan Boneh, Kamalika Chaudhuri
Categories: cs.LG cs.AI cs.CR
\\ ( https://arxiv.org/abs/2502.03773 ,  603kb)
------------------------------------------------------------------------------
\\
arXiv:2502.05807
replaced with revised version Wed, 28 May 2025 08:34:22 GMT   (6848kb,D)

Title: Devil is in the Details: Density Guidance for Detail-Aware Generation
  with Flow Models
Authors: Rafa{\l} Karczewski, Markus Heinonen, Vikas Garg
Categories: cs.LG
Comments: ICML 2025
\\ ( https://arxiv.org/abs/2502.05807 ,  6848kb)
------------------------------------------------------------------------------
\\
arXiv:2502.06970
replaced with revised version Wed, 28 May 2025 06:26:18 GMT   (658kb,D)

Title: Model Diffusion for Certifiable Few-shot Transfer Learning
Authors: Fady Rezk, Royson Lee, Henry Gouk, Timothy Hospedales, Minyoung Kim
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2502.06970 ,  658kb)
------------------------------------------------------------------------------
\\
arXiv:2502.06999
replaced with revised version Wed, 28 May 2025 14:58:36 GMT   (19805kb,D)

Title: Outsourced diffusion sampling: Efficient posterior inference in latent
  spaces of generative models
Authors: Siddarth Venkatraman, Mohsin Hasan, Minsu Kim, Luca Scimeca, Marcin
  Sendera, Yoshua Bengio, Glen Berseth, Nikolay Malkin
Categories: cs.LG
Comments: ICML 2025; code:
  https://github.com/HyperPotatoNeo/Outsourced_Diffusion_Sampling
\\ ( https://arxiv.org/abs/2502.06999 ,  19805kb)
------------------------------------------------------------------------------
\\
arXiv:2502.07864
replaced with revised version Wed, 28 May 2025 12:07:57 GMT   (3706kb,D)

Title: TransMLA: Migrating GQA Models to MLA with Full DeepSeek Compatibility
  and Speedup
Authors: Fanxu Meng, Pingzhi Tang, Zengwei Yao, Xing Sun, Muhan Zhang
Categories: cs.LG cs.AI
Comments: https://github.com/fxmeng/TransMLA
\\ ( https://arxiv.org/abs/2502.07864 ,  3706kb)
------------------------------------------------------------------------------
\\
arXiv:2502.09245
replaced with revised version Wed, 28 May 2025 11:04:00 GMT   (1135kb,D)

Title: You Do Not Fully Utilize Transformer's Representation Capacity
Authors: Gleb Gerasimov, Yaroslav Aksenov, Nikita Balagansky, Viacheslav Sinii
  and Daniil Gavrilov
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2502.09245 ,  1135kb)
------------------------------------------------------------------------------
\\
arXiv:2502.09297
replaced with revised version Wed, 28 May 2025 14:13:08 GMT   (4808kb,D)

Title: When do neural networks learn world models?
Authors: Tianren Zhang, Guanyu Chen, Feng Chen
Categories: cs.LG
Comments: ICML 2025
\\ ( https://arxiv.org/abs/2502.09297 ,  4808kb)
------------------------------------------------------------------------------
\\
arXiv:2502.09571
replaced with revised version Tue, 27 May 2025 23:08:52 GMT   (1283kb,D)

Title: DiffMS: Diffusion Generation of Molecules Conditioned on Mass Spectra
Authors: Montgomery Bohde, Mrunali Manjrekar, Runzhong Wang, Shuiwang Ji,
  Connor W. Coley
Categories: cs.LG q-bio.QM
Comments: ICML 2025
\\ ( https://arxiv.org/abs/2502.09571 ,  1283kb)
------------------------------------------------------------------------------
\\
arXiv:2502.09767
replaced with revised version Tue, 27 May 2025 20:25:10 GMT   (2433kb,D)

Title: Non-Markovian Discrete Diffusion with Causal Language Models
Authors: Yangtian Zhang, Sizhuang He, Daniel Levine, Lawrence Zhao, David
  Zhang, Syed A Rizvi, Emanuele Zappala, Rex Ying, David van Dijk
Categories: cs.LG cs.AI cs.CL
Comments: Under Review
\\ ( https://arxiv.org/abs/2502.09767 ,  2433kb)
------------------------------------------------------------------------------
\\
arXiv:2502.09844
replaced with revised version Wed, 28 May 2025 00:52:33 GMT   (3032kb,D)

Title: Solving Empirical Bayes via Transformers
Authors: Anzo Teh, Mark Jabbour, Yury Polyanskiy
Categories: cs.LG stat.ML
Comments: 28 pages, 6 figures, 12 tables. Code at
  https://github.com/Anzoteh96/eb-transformers
\\ ( https://arxiv.org/abs/2502.09844 ,  3032kb)
------------------------------------------------------------------------------
\\
arXiv:2502.09863
replaced with revised version Wed, 28 May 2025 15:55:00 GMT   (1973kb,D)

Title: Closed-Form Training Dynamics Reveal Learned Features and Linear
  Structure in Word2Vec-like Models
Authors: Dhruva Karkada, James B. Simon, Yasaman Bahri, Michael R. DeWeese
Categories: cs.LG cs.CL stat.ML
Comments: 23 pages, 6 figures
\\ ( https://arxiv.org/abs/2502.09863 ,  1973kb)
------------------------------------------------------------------------------
\\
arXiv:2502.11107
replaced with revised version Wed, 28 May 2025 08:43:07 GMT   (155kb,D)

Title: Revisiting Weak-to-Strong Generalization in Theory and Practice: Reverse
  KL vs. Forward KL
Authors: Wei Yao, Wenkai Yang, Ziqiao Wang, Yankai Lin, Yong Liu
Categories: cs.LG cs.AI
Comments: Accepted by ACL 2025 (Findings)
\\ ( https://arxiv.org/abs/2502.11107 ,  155kb)
------------------------------------------------------------------------------
\\
arXiv:2502.12170
replaced with revised version Wed, 28 May 2025 12:57:47 GMT   (1327kb,D)

Title: MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway
  Dynamic Dense Connections
Authors: Da Xiao, Qingye Meng, Shengping Li, Xingyuan Yuan
Categories: cs.LG cs.AI cs.CL
Comments: Accepted to the 42nd International Conference on Machine Learning
  (ICML'25)
\\ ( https://arxiv.org/abs/2502.12170 ,  1327kb)
------------------------------------------------------------------------------
\\
arXiv:2502.14637
replaced with revised version Wed, 28 May 2025 03:37:10 GMT   (13370kb,D)

Title: ReQFlow: Rectified Quaternion Flow for Efficient and High-Quality
  Protein Backbone Generation
Authors: Angxiao Yue, Zichong Wang, Hongteng Xu
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2502.14637 ,  13370kb)
------------------------------------------------------------------------------
\\
arXiv:2502.15522
replaced with revised version Wed, 28 May 2025 17:25:56 GMT   (2539kb,D)

Title: Solving Inverse Problems with Deep Linear Neural Networks: Global
  Convergence Guarantees for Gradient Descent with Weight Decay
Authors: Hannah Laus, Suzanna Parkinson, Vasileios Charisopoulos, Felix
  Krahmer, Rebecca Willett
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2502.15522 ,  2539kb)
------------------------------------------------------------------------------
\\
arXiv:2502.18807
replaced with revised version Wed, 28 May 2025 08:10:48 GMT   (8399kb,D)

Title: BatteryLife: A Comprehensive Dataset and Benchmark for Battery Life
  Prediction
Authors: Ruifeng Tan, Weixiang Hong, Jiayue Tang, Xibin Lu, Ruijun Ma, Xiang
  Zheng, Jia Li, Jiaqiang Huang, Tong-Yi Zhang
Categories: cs.LG cs.AI cs.DL
Comments: Accepted by KDD 2025
\\ ( https://arxiv.org/abs/2502.18807 ,  8399kb)
------------------------------------------------------------------------------
\\
arXiv:2503.01013
replaced with revised version Tue, 27 May 2025 20:50:43 GMT   (12877kb,D)

Title: Explainable Multi-modal Time Series Prediction with LLM-in-the-Loop
Authors: Yushan Jiang, Wenchao Yu, Geon Lee, Dongjin Song, Kijung Shin, Wei
  Cheng, Yanchi Liu, Haifeng Chen
Categories: cs.LG
\\ ( https://arxiv.org/abs/2503.01013 ,  12877kb)
------------------------------------------------------------------------------
\\
arXiv:2503.04992
replaced with revised version Tue, 27 May 2025 21:37:57 GMT   (2081kb,D)

Title: Wanda++: Pruning Large Language Models via Regional Gradients
Authors: Yifan Yang, Kai Zhen, Bhavana Ganesh, Aram Galstyan, Goeric
  Huybrechts, Markus M\"uller, Jonas M. K\"ubler, Rupak Vignesh Swaminathan,
  Athanasios Mouchtaris, Sravan Babu Bodapati, Nathan Susanj, Zheng Zhang, Jack
  FitzGerald, Abhishek Kumar
Categories: cs.LG cs.AI cs.CL
Comments: Paper accepted at ACL 2025 Findings
\\ ( https://arxiv.org/abs/2503.04992 ,  2081kb)
------------------------------------------------------------------------------
\\
arXiv:2503.05861
replaced with revised version Fri, 23 May 2025 20:28:06 GMT   (33338kb,D)

Title: Interpretable Visualizations of Data Spaces for Classification Problems
Authors: Christian Jorgensen, Arthur Y. Lin, Rhushil Vasavada, Rose K.
  Cersonsky
Categories: cs.LG stat.ML
Comments: 15 pages, 8 figures
MSC-class: N/A
\\ ( https://arxiv.org/abs/2503.05861 ,  33338kb)
------------------------------------------------------------------------------
\\
arXiv:2503.06639
replaced with revised version Wed, 28 May 2025 11:22:18 GMT   (529kb,D)

Title: Reinforcement Learning with Verifiable Rewards: GRPO's Effective Loss,
  Dynamics, and Success Amplification
Authors: Youssef Mroueh
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2503.06639 ,  529kb)
------------------------------------------------------------------------------
\\
arXiv:2503.07649
replaced with revised version Tue, 27 May 2025 20:50:23 GMT   (2655kb,D)

Title: TS-RAG: Retrieval-Augmented Generation based Time Series Foundation
  Models are Stronger Zero-Shot Forecaster
Authors: Kanghui Ning, Zijie Pan, Yu Liu, Yushan Jiang, James Y. Zhang, Kashif
  Rasul, Anderson Schneider, Lintao Ma, Yuriy Nevmyvaka, Dongjin Song
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2503.07649 ,  2655kb)
------------------------------------------------------------------------------
\\
arXiv:2503.09427
replaced with revised version Wed, 28 May 2025 02:22:31 GMT   (5592kb,D)

Title: Language-Enhanced Representation Learning for Single-Cell
  Transcriptomics
Authors: Yaorui Shi, Jiaqi Yang, Changhao Nai, Sihang Li, Junfeng Fang, Xiang
  Wang, Zhiyuan Liu, Yang Zhang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2503.09427 ,  5592kb)
------------------------------------------------------------------------------
\\
arXiv:2503.12734
replaced with revised version Tue, 27 May 2025 18:50:18 GMT   (1217kb,D)

Title: In-Context Linear Regression Demystified: Training Dynamics and
  Mechanistic Interpretability of Multi-Head Softmax Attention
Authors: Jianliang He, Xintian Pan, Siyu Chen, Zhuoran Yang
Categories: cs.LG stat.ML
Comments: Accepted by ICML 2025, related work added
\\ ( https://arxiv.org/abs/2503.12734 ,  1217kb)
------------------------------------------------------------------------------
\\
arXiv:2503.17674
replaced with revised version Wed, 28 May 2025 07:12:09 GMT   (4791kb,D)

Title: MultiScale Contextual Bandits for Long Term Objectives
Authors: Richa Rastogi, Yuta Saito, Thorsten Joachims
Categories: cs.LG
\\ ( https://arxiv.org/abs/2503.17674 ,  4791kb)
------------------------------------------------------------------------------
\\
arXiv:2503.19618
replaced with revised version Wed, 28 May 2025 14:42:09 GMT   (1252kb,D)

Title: Beyond Verifiable Rewards: Scaling Reinforcement Learning for Language
  Models to Unverifiable Data
Authors: Yunhao Tang, Sid Wang, Lovish Madaan, R\'emi Munos
Categories: cs.LG
\\ ( https://arxiv.org/abs/2503.19618 ,  1252kb)
------------------------------------------------------------------------------
\\
arXiv:2503.21000
replaced with revised version Tue, 27 May 2025 19:19:31 GMT   (5464kb,D)

Title: Improving User Behavior Prediction: Leveraging Annotator Metadata in
  Supervised Machine Learning Models
Authors: Lynnette Hui Xian Ng, Kokil Jaidka, Kaiyuan Tay, Hansin Ahuja, Niyati
  Chhaya
Categories: cs.LG cs.AI
Comments: Accepted at CSCW 2025
\\ ( https://arxiv.org/abs/2503.21000 ,  5464kb)
------------------------------------------------------------------------------
\\
arXiv:2504.05928
replaced with revised version Wed, 28 May 2025 06:40:26 GMT   (1292kb)

Title: Evaluation of the impact of expert knowledge: How decision support
  scores impact the effectiveness of automatic knowledge-driven feature
  engineering (aKDFE)
Authors: Olof Bj\"orneld, Tora Hammar, Daniel Nilsson, Alisa Lincke, Welf
  L\"owe
Categories: cs.LG
Comments: 43 pages, including the Appendix, 19 tables and 13 figures
MSC-class: 62R01, 68T05
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2504.05928 ,  1292kb)
------------------------------------------------------------------------------
\\
arXiv:2504.11713
replaced with revised version Wed, 28 May 2025 17:16:01 GMT   (13496kb,D)

Title: Adjoint Sampling: Highly Scalable Diffusion Samplers via Adjoint
  Matching
Authors: Aaron Havens, Benjamin Kurt Miller, Bing Yan, Carles Domingo-Enrich,
  Anuroop Sriram, Brandon Wood, Daniel Levine, Bin Hu, Brandon Amos, Brian
  Karrer, Xiang Fu, Guan-Horng Liu, Ricky T. Q. Chen
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2504.11713 ,  13496kb)
------------------------------------------------------------------------------
\\
arXiv:2504.12764
replaced with revised version Wed, 28 May 2025 17:47:46 GMT   (4139kb,D)

Title: GraphOmni: A Comprehensive and Extendable Benchmark Framework for Large
  Language Models on Graph-theoretic Tasks
Authors: Hao Xu, Xiangru Jian, Xinjian Zhao, Wei Pang, Chao Zhang, Suyuchen
  Wang, Qixin Zhang, Zhengyuan Dong, Joao Monteiro, Bang Liu, Qiuzhuang Sun,
  Tianshu Yu
Categories: cs.LG cs.DM
Comments: Project Page: https://gai-community.github.io/Graph-Omni/
\\ ( https://arxiv.org/abs/2504.12764 ,  4139kb)
------------------------------------------------------------------------------
\\
arXiv:2504.18026
replaced with revised version Tue, 27 May 2025 20:26:10 GMT   (21815kb,D)

Title: Addressing Concept Mislabeling in Concept Bottleneck Models Through
  Preference Optimization
Authors: Emiliano Penaloza, Tianyue H. Zhan, Laurent Charlin, Mateo Espinosa
  Zarlenga
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2504.18026 ,  21815kb)
------------------------------------------------------------------------------
\\
arXiv:2504.19452
replaced with revised version Wed, 28 May 2025 02:52:31 GMT   (9333kb,D)

Title: Geometry-Informed Neural Operator Transformer
Authors: Qibang Liu, Vincient Zhong, Hadi Meidani, Diab Abueidda, Seid Koric,
  Philippe Geubelle
Categories: cs.LG physics.comp-ph
\\ ( https://arxiv.org/abs/2504.19452 ,  9333kb)
------------------------------------------------------------------------------
\\
arXiv:2505.05082
replaced with revised version Wed, 28 May 2025 00:53:21 GMT   (5573kb,D)

Title: ItDPDM: Information-Theoretic Discrete Poisson Diffusion Model
Authors: Sagnik Bhattacharya, Abhiram Gorle, Ahsan Bilal, Connor Ding, Amit
  Kumar Singh Yadav, Tsachy Weissman
Categories: cs.LG cs.IT math.IT math.PR
Comments: Pre-print
\\ ( https://arxiv.org/abs/2505.05082 ,  5573kb)
------------------------------------------------------------------------------
\\
arXiv:2505.05522
replaced with revised version Wed, 28 May 2025 00:50:21 GMT   (8554kb,D)

Title: Continuous Thought Machines
Authors: Luke Darlow, Ciaran Regan, Sebastian Risi, Jeffrey Seely, Llion Jones
Categories: cs.LG cs.AI
Comments: Technical report accompanied by online project page:
  https://pub.sakana.ai/ctm/
\\ ( https://arxiv.org/abs/2505.05522 ,  8554kb)
------------------------------------------------------------------------------
\\
arXiv:2505.07503
replaced with revised version Wed, 28 May 2025 05:41:56 GMT   (1450kb)

Title: Identifying Causal Direction via Variational Bayesian Compression
Authors: Quang-Duy Tran, Bao Duong, Phuoc Nguyen, Thin Nguyen
Categories: cs.LG stat.ML
Comments: Accepted at the 42nd International Conference on Machine Learning
  (ICML2025)
\\ ( https://arxiv.org/abs/2505.07503 ,  1450kb)
------------------------------------------------------------------------------
\\
arXiv:2505.11953
replaced with revised version Wed, 28 May 2025 03:33:24 GMT   (1668kb)

Title: Exploring Criteria of Loss Reweighting to Enhance LLM Unlearning
Authors: Puning Yang, Qizhou Wang, Zhuo Huang, Tongliang Liu, Chengqi Zhang, Bo
  Han
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2505.11953 ,  1668kb)
------------------------------------------------------------------------------
\\
arXiv:2505.16583
replaced with revised version Wed, 28 May 2025 11:51:22 GMT   (10387kb,D)

Title: Training on Plausible Counterfactuals Removes Spurious Correlations
Authors: Shpresim Sadiku, Kartikeya Chitranshi, Hiroshi Kera, Sebastian Pokutta
Categories: cs.LG
\\ ( https://arxiv.org/abs/2505.16583 ,  10387kb)
------------------------------------------------------------------------------
\\
arXiv:2505.17248
replaced with revised version Tue, 27 May 2025 21:01:34 GMT   (1276kb,D)

Title: Backdoors in DRL: Four Environments Focusing on In-distribution Triggers
Authors: Chace Ashcraft, Ted Staley, Josh Carney, Cameron Hickert, Kiran Karra,
  and Nathan Drenkow
Categories: cs.LG cs.CR
\\ ( https://arxiv.org/abs/2505.17248 ,  1276kb)
------------------------------------------------------------------------------
\\
arXiv:2505.17773
replaced with revised version Wed, 28 May 2025 14:57:51 GMT   (118kb)

Title: C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in
  Large Language Models
Authors: Amir Hossein Rahmati, Sanket Jantre, Weifeng Zhang, Yucheng Wang,
  Byung-Jun Yoon, Nathan M. Urban, Xiaoning Qian
Categories: cs.LG
\\ ( https://arxiv.org/abs/2505.17773 ,  118kb)
------------------------------------------------------------------------------
\\
arXiv:2505.18116
replaced with revised version Wed, 28 May 2025 17:31:37 GMT   (584kb,D)

Title: Bridging Supervised Learning and Reinforcement Learning in Math
  Reasoning
Authors: Huayu Chen, Kaiwen Zheng, Qinsheng Zhang, Ganqu Cui, Yin Cui, Haotian
  Ye, Tsung-Yi Lin, Ming-Yu Liu, Jun Zhu, Haoxiang Wang
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2505.18116 ,  584kb)
------------------------------------------------------------------------------
\\
arXiv:2505.18763
replaced with revised version Tue, 27 May 2025 16:55:40 GMT   (4453kb)

Title: GenPO: Generative Diffusion Models Meet On-Policy Reinforcement Learning
Authors: Shutong Ding, Ke Hu, Shan Zhong, Haoyang Luo, Weinan Zhang, Jingya
  Wang, Jun Wang, Ye Shi
Categories: cs.LG
\\ ( https://arxiv.org/abs/2505.18763 ,  4453kb)
------------------------------------------------------------------------------
\\
arXiv:2505.19105
replaced with revised version Wed, 28 May 2025 07:11:21 GMT   (3229kb)

Title: Latent Mamba Operator for Partial Differential Equations
Authors: Karn Tiwari, Niladri Dutta, N M Anoop Krishnan, Prathosh A P
Categories: cs.LG
Comments: Proceedings of the 42 nd International Conference on Machine
  Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s)
\\ ( https://arxiv.org/abs/2505.19105 ,  3229kb)
------------------------------------------------------------------------------
\\
arXiv:2505.19205
replaced with revised version Wed, 28 May 2025 15:13:41 GMT   (65kb)

Title: OptiMindTune: A Multi-Agent Framework for Intelligent Hyperparameter
  Optimization
Authors: Meher Bhaskar Madiraju and Meher Sai Preetam Madiraju
Categories: cs.LG cs.AI cs.MA
Comments: 7 pages, 2 tables
\\ ( https://arxiv.org/abs/2505.19205 ,  65kb)
------------------------------------------------------------------------------
\\
arXiv:2505.19255
replaced with revised version Wed, 28 May 2025 16:58:13 GMT   (1169kb)

Title: VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on
  Multimodal Tool Use
Authors: Mingyuan Wu, Jingcheng Yang, Jize Jiang, Meitang Li, Kaizhuo Yan,
  Hanchao Yu, Minjia Zhang, Chengxiang Zhai, and Klara Nahrstedt
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2505.19255 ,  1169kb)
------------------------------------------------------------------------------
\\
arXiv:2505.19259
replaced with revised version Wed, 28 May 2025 02:16:52 GMT   (1407kb)

Title: Towards Large Reasoning Models for Agriculture
Authors: Hossein Zaremehrjerdi, Shreyan Ganguly, Ashlyn Rairdin, Elizabeth
  Tranel, Benjamin Feuer, Juan Ignacio Di Salvo, Srikanth Panthulugiri, Hernan
  Torres Pacin, Victoria Moser, Sarah Jones, Joscif G Raigne, Yanben Shen,
  Heidi M. Dornath, Aditya Balu, Adarsh Krishnamurthy, Asheesh K Singh, Arti
  Singh, Baskar Ganapathysubramanian, Chinmay Hegde, Soumik Sarkar
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2505.19259 ,  1407kb)
------------------------------------------------------------------------------
\\
arXiv:2505.19698
replaced with revised version Wed, 28 May 2025 08:56:53 GMT   (4024kb)

Title: JEDI: Latent End-to-end Diffusion Mitigates Agent-Human Performance
  Asymmetry in Model-Based Reinforcement Learning
Authors: Jing Yu Lim, Zarif Ikram, Samson Yu, Haozhe Ma, Tze-Yun Leong, Dianbo
  Liu
Categories: cs.LG cs.AI cs.RO
Comments: Preprint
\\ ( https://arxiv.org/abs/2505.19698 ,  4024kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20135
replaced with revised version Wed, 28 May 2025 16:33:14 GMT   (1123kb)

Title: Data-Distill-Net: A Data Distillation Approach Tailored for Reply-based
  Continual Learning
Authors: Wenyang Liao, Quanziang Wang, Yichen Wu, Renzhen Wang and Deyu Meng
Categories: cs.LG
\\ ( https://arxiv.org/abs/2505.20135 ,  1123kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20839
replaced with revised version Wed, 28 May 2025 12:51:23 GMT   (3269kb,D)

Title: FireQ: Fast INT4-FP8 Kernel and RoPE-aware Quantization for LLM
  Inference Acceleration
Authors: Daehyeon Baek, Jieun Choi, Jimyoung Son, Kyungmin Bin, Seungbeom Choi,
  Kihyo Moon, Minsung Jang, Hyojung Lee
Categories: cs.LG
\\ ( https://arxiv.org/abs/2505.20839 ,  3269kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20853
replaced with revised version Wed, 28 May 2025 04:31:56 GMT   (1295kb,D)

Title: Cooperation of Experts: Fusing Heterogeneous Information with Large
  Margin
Authors: Shuo Wang, Shunyang Huang, Jinghui Yuan, Zhixiang Shen, Zhao Kang
Categories: cs.LG cs.AI
Comments: Published in ICML 2025
\\ ( https://arxiv.org/abs/2505.20853 ,  1295kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20885
replaced with revised version Wed, 28 May 2025 08:05:07 GMT   (48kb)

Title: Improved Bounds for Swap Multicalibration and Swap Omniprediction
Authors: Haipeng Luo, Spandan Senapati, Vatsal Sharan
Categories: cs.LG stat.ML
Comments: v2 does a minor fix on page 16
\\ ( https://arxiv.org/abs/2505.20885 ,  48kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20970
replaced with revised version Wed, 28 May 2025 02:58:11 GMT   (2571kb)

Title: Understanding the behavior of representation forgetting in continual
  learning
Authors: Joonkyu Kim, Yejin Kim, Jy-yong Sohn
Categories: cs.LG
\\ ( https://arxiv.org/abs/2505.20970 ,  2571kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21073
replaced with revised version Wed, 28 May 2025 09:07:43 GMT   (895kb)

Title: Bridging Arbitrary and Tree Metrics via Differentiable Gromov
  Hyperbolicity
Authors: Pierre Houedry, Nicolas Courty, Florestan Martin-Baillon, Laetitia
  Chapel, Titouan Vayer
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2505.21073 ,  895kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21136
replaced with revised version Wed, 28 May 2025 06:22:06 GMT   (34196kb)

Title: SageAttention2++: A More Efficient Implementation of SageAttention2
Authors: Jintao Zhang, Xiaoming Xu, Jia Wei, Haofeng Huang, Pengle Zhang,
  Chendong Xiang, Jun Zhu, Jianfei Chen
Categories: cs.LG cs.AI cs.AR cs.CV
\\ ( https://arxiv.org/abs/2505.21136 ,  34196kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21251
replaced with revised version Wed, 28 May 2025 13:03:58 GMT   (521kb)

Title: Copresheaf Topological Neural Networks: A Generalized Deep Learning
  Framework
Authors: Mustafa Hajij, Lennart Bastian, Sarah Osentoski, Hardik Kabaria, John
  L. Davenport, Sheik Dawood, Balaji Cherukuri, Joseph G. Kocheemoolayil,
  Nastaran Shahmansouri, Adrian Lew, Theodore Papamarkou, Tolga Birdal
Categories: cs.LG
\\ ( https://arxiv.org/abs/2505.21251 ,  521kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21347
replaced with revised version Wed, 28 May 2025 02:52:41 GMT   (6943kb)

Title: OVERT: A Benchmark for Over-Refusal Evaluation on Text-to-Image Models
Authors: Ziheng Cheng, Yixiao Huang, Hui Xu, Somayeh Sojoudi, Xuandong Zhao,
  Dawn Song, Song Mei
Categories: cs.LG
\\ ( https://arxiv.org/abs/2505.21347 ,  6943kb)
------------------------------------------------------------------------------
\\
arXiv:2505.04364
replaced with revised version Wed, 28 May 2025 07:30:31 GMT   (28352kb,D)

Title: Benchmarking LLMs' Swarm intelligence
Authors: Kai Ruan, Mowen Huang, Ji-Rong Wen, Hao Sun
Categories: cs.MA cs.CL
Comments: added new ref
\\ ( https://arxiv.org/abs/2505.04364 ,  28352kb)
------------------------------------------------------------------------------
\\
arXiv:2505.05428
replaced with revised version Tue, 27 May 2025 21:50:08 GMT   (756kb)

Title: Empowering Scientific Workflows with Federated Agents
Authors: J. Gregory Pauloski, Yadu Babuji, Ryan Chard, Mansi Sakarvadia, Kyle
  Chard, Ian Foster
Categories: cs.MA cs.DC
\\ ( https://arxiv.org/abs/2505.05428 ,  756kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12008
replaced with revised version Wed, 28 May 2025 09:58:17 GMT   (21593kb,D)

Title: NanoSLAM: Enabling Fully Onboard SLAM for Tiny Robots
Authors: Vlad Niculescu, Tommaso Polonelli, Michele Magno, Luca Benini
Categories: cs.RO
Comments: 24 pages
DOI: 10.1109/JIOT.2023.3339254
\\ ( https://arxiv.org/abs/2309.12008 ,  21593kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01932
replaced with revised version Wed, 28 May 2025 15:24:13 GMT   (2638kb,D)

Title: Bridging Language, Vision and Action: Multimodal VAEs in Robotic
  Manipulation Tasks
Authors: Gabriela Sejnova, Michal Vavrecka, Karla Stepanova
Categories: cs.RO cs.LG
Comments: 7 pages, 5 figures, 2 tables, conference
Journal-ref: 2024 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS)
DOI: 10.1109/IROS58592.2024.10802160
\\ ( https://arxiv.org/abs/2404.01932 ,  2638kb)
------------------------------------------------------------------------------
\\
arXiv:2406.03200
replaced with revised version Wed, 28 May 2025 13:10:30 GMT   (5284kb,D)

Title: Adaptive Distance Functions via Kelvin Transformation
Authors: Rafael I. Cabral Muchacho and Florian T. Pokorny
Categories: cs.RO
Comments: Accepted to IEEE ICRA 2025
ACM-class: I.2.9
\\ ( https://arxiv.org/abs/2406.03200 ,  5284kb)
------------------------------------------------------------------------------
\\
arXiv:2407.07589
replaced with revised version Wed, 28 May 2025 03:38:21 GMT   (2637kb)

Title: MSC-LIO: An MSCKF-Based LiDAR-Inertial Odometry with Same-Plane Cluster
  Tracking
Authors: Tisheng Zhang, Man Yuan, Linfu Wei, Hailiang Tang, and Xiaoji Niu
Categories: cs.RO
Comments: 11 pages, 12 figures, 8 tables
Journal-ref: IEEE/ASME Transactions on Mechatronics,2025
DOI: 10.1109/TMECH.2025.3574307
\\ ( https://arxiv.org/abs/2407.07589 ,  2637kb)
------------------------------------------------------------------------------
\\
arXiv:2407.10382
replaced with revised version Tue, 27 May 2025 19:26:31 GMT   (7433kb,D)

Title: Communication- and Computation-Efficient Distributed Submodular
  Optimization in Robot Mesh Networks
Authors: Zirui Xu, Sandilya Sai Garimella, Vasileios Tzoumas
Categories: cs.RO cs.AI cs.MA cs.SY eess.SY math.OC
Comments: Accepted to IEEE Transactions on Robotics
\\ ( https://arxiv.org/abs/2407.10382 ,  7433kb)
------------------------------------------------------------------------------
\\
arXiv:2408.05719
replaced with revised version Wed, 28 May 2025 03:40:29 GMT   (2353kb)

Title: MR-ULINS: A Tightly-Coupled UWB-LiDAR-Inertial Estimator with
  Multi-Epoch Outlier Rejection
Authors: Tisheng Zhang, Man Yuan, Linfu Wei, Yan Wang, Hailiang Tang, Xiaoji
  Niu
Categories: cs.RO eess.SP
Comments: 8 pages, 9 figures
Journal-ref: IEEE Robotics and Automation Letters ( Volume: 9, Issue: 12,
  December 2024)
DOI: 10.1109/LRA.2024.3498780
\\ ( https://arxiv.org/abs/2408.05719 ,  2353kb)
------------------------------------------------------------------------------
\\
arXiv:2409.14342
replaced with revised version Wed, 28 May 2025 03:59:54 GMT   (2675kb,D)

Title: Adapting Gait Frequency for Posture-regulating Humanoid Push-recovery
  via Hierarchical Model Predictive Control
Authors: Junheng Li, Zhanhao Le, Junchao Ma, and Quan Nguyen
Categories: cs.RO cs.SY eess.SY
Comments: 7 pages, 6 figures, accepted to ICRA 2025
\\ ( https://arxiv.org/abs/2409.14342 ,  2675kb)
------------------------------------------------------------------------------
\\
arXiv:2412.02331
replaced with revised version Wed, 28 May 2025 12:23:27 GMT   (1773kb,D)

Title: Sample Efficient Robot Learning in Supervised Effect Prediction Tasks
Authors: Mehmet Arda Eren and Erhan Oztop
Categories: cs.RO cs.AI cs.LG
Comments: 15 pages, 6 figures
\\ ( https://arxiv.org/abs/2412.02331 ,  1773kb)
------------------------------------------------------------------------------
\\
arXiv:2412.11829
replaced with revised version Wed, 28 May 2025 14:21:24 GMT   (12758kb,D)

Title: Robust Contact-rich Manipulation through Implicit Motor Adaptation
Authors: Teng Xue, Amirreza Razmjoo, Suhan Shetty, Sylvain Calinon
Categories: cs.RO
\\ ( https://arxiv.org/abs/2412.11829 ,  12758kb)
------------------------------------------------------------------------------
\\
arXiv:2501.08520
replaced with revised version Tue, 27 May 2025 20:30:11 GMT   (689kb,D)

Title: Chance-Constrained Sampling-Based MPC for Collision Avoidance in
  Uncertain Dynamic Environments
Authors: Ihab S. Mohamed, Mahmoud Ali, Lantao Liu
Categories: cs.RO cs.SY eess.SY
Comments: This paper has been accepted for publication in IEEE Robotics and
  Automation Letters (RA-L), May 2025. It comprises 9 pages, 3 figures, and 7
  tables
\\ ( https://arxiv.org/abs/2501.08520 ,  689kb)
------------------------------------------------------------------------------
\\
arXiv:2502.02934
replaced with revised version Wed, 28 May 2025 03:57:38 GMT   (4499kb,D)

Title: Gait-Net-augmented Implicit Kino-dynamic MPC for Dynamic
  Variable-frequency Humanoid Locomotion over Discrete Terrains
Authors: Junheng Li, Ziwei Duan, Junchao Ma, and Quan Nguyen
Categories: cs.RO cs.SY eess.SY
Comments: 15 pages, 13 figures, RSS 2025 accepted
\\ ( https://arxiv.org/abs/2502.02934 ,  4499kb)
------------------------------------------------------------------------------
\\
arXiv:2502.11057
replaced with revised version Wed, 28 May 2025 10:55:13 GMT   (1773kb,D)

Title: A Physics-Informed Machine Learning Framework for Safe and Optimal
  Control of Autonomous Systems
Authors: Manan Tayal, Aditya Singh, Shishir Kolathaya, Somil Bansal
Categories: cs.RO cs.AI cs.SY eess.SY
Comments: 22 Pages, 12 Figures. First two authors have contributed equally.
  Accepted at ICML 2025
\\ ( https://arxiv.org/abs/2502.11057 ,  1773kb)
------------------------------------------------------------------------------
\\
arXiv:2505.02272
replaced with revised version Wed, 28 May 2025 17:28:45 GMT   (9475kb)

Title: Robust Localization, Mapping, and Navigation for Quadruped Robots
Authors: Dyuman Aditya, Junning Huang, Nico Bohlinger, Piotr Kicki, Krzysztof
  Walas, Jan Peters, Matteo Luperto, Davide Tateo
Categories: cs.RO cs.AI
Comments: 8 Pages
\\ ( https://arxiv.org/abs/2505.02272 ,  9475kb)
------------------------------------------------------------------------------
\\
arXiv:2505.06787
replaced with revised version Wed, 28 May 2025 12:45:34 GMT   (1787kb)

Title: Digital-physical testbed for ship autonomy studies in the Marine
  Cybernetics Laboratory basin
Authors: Emir Cem Gezer, Mael Korentin Ivan Moreau, Anders Sandneseng
  H{\o}gden, Dong Trong Nguyen, Roger Skjetne, Asgeir S{\o}rensen
Categories: cs.RO cs.SY eess.SY
\\ ( https://arxiv.org/abs/2505.06787 ,  1787kb)
------------------------------------------------------------------------------
\\
arXiv:2505.14030
replaced with revised version Wed, 28 May 2025 16:17:21 GMT   (17858kb)

Title: AutoBio: A Simulation and Benchmark for Robotic Automation in Digital
  Biology Laboratory
Authors: Zhiqian Lan, Yuxuan Jiang, Ruiqi Wang, Xuanbing Xie, Rongkui Zhang,
  Yicheng Zhu, Peihang Li, Tianshuo Yang, Tianxing Chen, Haoyu Gao, Xiaokang
  Yang, Xuelong Li, Hongyuan Zhang, Yao Mu, Ping Luo
Categories: cs.RO
\\ ( https://arxiv.org/abs/2505.14030 ,  17858kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20455
replaced with revised version Wed, 28 May 2025 07:21:09 GMT   (7613kb)

Title: HAND Me the Data: Fast Robot Adaptation via Hand Path Retrieval
Authors: Matthew Hong, Anthony Liang, Kevin Kim, Harshitha Rajaprakash, Jesse
  Thomason, Erdem B{\i}y{\i}k, Jesse Zhang
Categories: cs.RO
\\ ( https://arxiv.org/abs/2505.20455 ,  7613kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20828
replaced with revised version Wed, 28 May 2025 10:29:18 GMT   (8266kb)

Title: GET: Goal-directed Exploration and Targeting for Large-Scale Unknown
  Environments
Authors: Lanxiang Zheng, Ruidong Mei, Mingxin Wei, Hao Ren, Hui Cheng
Categories: cs.RO
\\ ( https://arxiv.org/abs/2505.20828 ,  8266kb)
------------------------------------------------------------------------------
\\
arXiv:2304.12647 (*cross-listing*)
replaced with revised version Wed, 28 May 2025 15:32:51 GMT   (4645kb,D)

Title: Learned Collusion
Authors: Olivier Compte (Paris School of Economics)
Categories: econ.TH cs.AI cs.GT
Comments: 41 pages, 19 figures, 14 tables
\\ ( https://arxiv.org/abs/2304.12647 ,  4645kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15876
replaced with revised version Tue, 27 May 2025 21:50:58 GMT   (6071kb,D)

Title: End-to-End Breast Cancer Radiotherapy Planning via LMMs with Consistency
  Embedding
Authors: Kwanyoung Kim, Yujin Oh, Sangjoon Park, Hwa Kyung Byun, Joongyo Lee,
  Jin Sung Kim, Yong Bae Kim, Jong Chul Ye
Categories: cs.CV cs.AI cs.LG
Comments: Accepted for Medical Image Analysis 2025
\\ ( https://arxiv.org/abs/2311.15876 ,  6071kb)
------------------------------------------------------------------------------
\\
arXiv:2405.14719 (*cross-listing*)
replaced with revised version Wed, 28 May 2025 11:11:30 GMT   (675kb,D)

Title: Decision-Focused Forecasting: A Differentiable Multistage Optimisation
  Architecture
Authors: Egon Per\v{s}ak and Miguel F. Anjos
Categories: math.OC cs.AI cs.LG
ACM-class: I.2.8
\\ ( https://arxiv.org/abs/2405.14719 ,  675kb)
------------------------------------------------------------------------------
\\
arXiv:2406.01394
replaced with revised version Wed, 28 May 2025 12:38:37 GMT   (1615kb,D)

Title: PrivacyRestore: Privacy-Preserving Inference in Large Language Models
  via Privacy Removal and Restoration
Authors: Ziqian Zeng, Jianwei Wang, Junyao Yang, Zhengdong Lu, Haoran Li,
  Huiping Zhuang, Cen Chen
Categories: cs.CR cs.AI
Comments: Accepted by ACL2025
\\ ( https://arxiv.org/abs/2406.01394 ,  1615kb)
------------------------------------------------------------------------------
\\
arXiv:2406.02633
replaced with revised version Tue, 27 May 2025 19:15:39 GMT   (102kb)

Title: Edit Distance Robust Watermarks via Indexing Pseudorandom Codes
Authors: Noah Golowich and Ankur Moitra
Categories: cs.CR cs.AI cs.LG
Comments: Appeared in NeurIPS 2024
\\ ( https://arxiv.org/abs/2406.02633 ,  102kb)
------------------------------------------------------------------------------
\\
arXiv:2406.14023
replaced with revised version Wed, 28 May 2025 07:25:26 GMT   (1005kb,D)

Title: Evaluating Implicit Bias in Large Language Models by Attacking From a
  Psychometric Perspective
Authors: Yuchen Wen, Keping Bi, Wei Chen, Jiafeng Guo, Xueqi Cheng
Categories: cs.CL cs.AI
Comments: Accepted to ACL 2025 Findings
\\ ( https://arxiv.org/abs/2406.14023 ,  1005kb)
------------------------------------------------------------------------------
\\
arXiv:2407.07004
replaced with revised version Tue, 27 May 2025 19:21:15 GMT   (938kb,D)

Title: Empirical analysis of binding precedent efficiency in Brazilian Supreme
  Court via case classification
Authors: Rapha\"el Tinarrage, Henrique Ennes, Lucas Resck, Lucas T. Gomes, Jean
  R. Ponciano, Jorge Poco
Categories: cs.CL cs.AI cs.IR cs.LG
Comments: Document similar to published version. Contains 62 pages and 21
  figures
MSC-class: 68T50 (Primary), 68T07 (Secondary)
Journal-ref: Artificial Intelligence and Law (2025)
DOI: 10.1007/s10506-025-09458-6
\\ ( https://arxiv.org/abs/2407.07004 ,  938kb)
------------------------------------------------------------------------------
\\
arXiv:2408.05758 (*cross-listing*)
replaced with revised version Wed, 28 May 2025 03:51:33 GMT   (9086kb,D)

Title: VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for
  Speech Processing
Authors: Chunyu Qiang, Wang Geng, Yi Zhao, Ruibo Fu, Tao Wang, Cheng Gong,
  Tianrui Wang, Qiuyu Liu, Jiangyan Yi, Zhengqi Wen, Chen Zhang, Hao Che,
  Longbiao Wang, Jianwu Dang, Jianhua Tao
Categories: eess.AS cs.AI cs.CL cs.SD
\\ ( https://arxiv.org/abs/2408.05758 ,  9086kb)
------------------------------------------------------------------------------
\\
arXiv:2409.16322 (*cross-listing*)
replaced with revised version Wed, 28 May 2025 10:42:35 GMT   (6600kb,D)

Title: On the Within-class Variation Issue in Alzheimer's Disease Detection
Authors: Jiawen Kang, Dongrui Han, Lingwei Meng, Jingyan Zhou, Jinchao Li,
  Xixin Wu, Helen Meng
Categories: eess.AS cs.AI cs.CL cs.LG cs.SD q-bio.NC
Comments: Accepted by InterSpeech 2025
\\ ( https://arxiv.org/abs/2409.16322 ,  6600kb)
------------------------------------------------------------------------------
\\
arXiv:2409.19291
replaced with revised version Wed, 28 May 2025 10:03:54 GMT   (453kb,D)

Title: CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified
  Multiplet Upcycling
Authors: Jihai Zhang, Xiaoye Qu, Tong Zhu, Yu Cheng
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2409.19291 ,  453kb)
------------------------------------------------------------------------------
\\
arXiv:2410.14641
replaced with revised version Wed, 28 May 2025 01:13:12 GMT   (2924kb,D)

Title: Distance between Relevant Information Pieces Causes Bias in Long-Context
  LLMs
Authors: Runchu Tian, Yanghao Li, Yuepeng Fu, Siyang Deng, Qinyu Luo, Cheng
  Qian, Shuo Wang, Xin Cong, Zhong Zhang, Yesai Wu, Yankai Lin, Huadong Wang,
  Xiaojiang Liu
Categories: cs.CL cs.AI
Comments: ACL 2025 Findings
\\ ( https://arxiv.org/abs/2410.14641 ,  2924kb)
------------------------------------------------------------------------------
\\
arXiv:2410.19064
replaced with revised version Wed, 28 May 2025 08:26:32 GMT   (2931kb,D)

Title: The Stepwise Deception: Simulating the Evolution from True News to Fake
  News with LLM Agents
Authors: Yuhan Liu, Zirui Song, Juntian Zhang, Xiaoqing Zhang, Xiuying Chen,
  Rui Yan
Categories: cs.SI cs.AI
\\ ( https://arxiv.org/abs/2410.19064 ,  2931kb)
------------------------------------------------------------------------------
\\
arXiv:2411.07404
replaced with revised version Tue, 27 May 2025 21:44:35 GMT   (10289kb,D)

Title: Controllable Context Sensitivity and the Knob Behind It
Authors: Julian Minder, Kevin Du, Niklas Stoehr, Giovanni Monea, Chris Wendler,
  Robert West, Ryan Cotterell
Categories: cs.CL cs.AI
Comments: Published as a conference paper at ICLR 2025
\\ ( https://arxiv.org/abs/2411.07404 ,  10289kb)
------------------------------------------------------------------------------
\\
arXiv:2411.11171
replaced with revised version Wed, 28 May 2025 12:38:43 GMT   (1278kb,D)

Title: LL\"aMmlein: Compact and Competitive German-Only Language Models from
  Scratch
Authors: Jan Pfister, Julia Wunderle, Andreas Hotho
Categories: cs.CL cs.AI cs.LG
Comments: camera ready @ACL25;
  https://www.informatik.uni-wuerzburg.de/datascience/projects/nlp/llammlein/
\\ ( https://arxiv.org/abs/2411.11171 ,  1278kb)
------------------------------------------------------------------------------
\\
arXiv:2411.17170
replaced with revised version Wed, 28 May 2025 11:20:49 GMT   (2774kb,D)

Title: Overcoming Non-monotonicity in Transducer-based Streaming Generation
Authors: Zhengrui Ma, Yang Feng, Min Zhang
Categories: cs.CL cs.AI
Comments: ICML25; Codes: https://github.com/ictnlp/MonoAttn-Transducer
\\ ( https://arxiv.org/abs/2411.17170 ,  2774kb)
------------------------------------------------------------------------------
\\
arXiv:2412.10419
replaced with revised version Wed, 28 May 2025 16:35:37 GMT   (14737kb,D)

Title: Preference Adaptive and Sequential Text-to-Image Generation
Authors: Ofir Nabati, Guy Tennenholtz, ChihWei Hsu, Moonkyung Ryu, Deepak
  Ramachandran, Yinlam Chow, Xiang Li, Craig Boutilier
Categories: cs.CV cs.AI cs.CL cs.LG cs.SY eess.SY
Comments: Accepted to ICML 2025 Link to PASTA dataset:
  https://www.kaggle.com/datasets/googleai/pasta-data
\\ ( https://arxiv.org/abs/2412.10419 ,  14737kb)
------------------------------------------------------------------------------
\\
arXiv:2412.14689
replaced with revised version Wed, 28 May 2025 05:22:58 GMT   (9527kb,D)

Title: How to Synthesize Text Data without Model Collapse?
Authors: Xuekai Zhu, Daixuan Cheng, Hengli Li, Kaiyan Zhang, Ermo Hua, Xingtai
  Lv, Ning Ding, Zhouhan Lin, Zilong Zheng, Bowen Zhou
Categories: cs.CL cs.AI cs.LG
Comments: Accepted at ICML 2025
\\ ( https://arxiv.org/abs/2412.14689 ,  9527kb)
------------------------------------------------------------------------------
\\
arXiv:2412.14775 (*cross-listing*)
replaced with revised version Wed, 28 May 2025 13:38:16 GMT   (2522kb,D)

Title: Energy and polarization based on-line interference mitigation in radio
  interferometry
Authors: Sarod Yatawatta, Albert-Jan Boonstra, Chris P. Broekema
Categories: astro-ph.IM cs.AI
Comments: Accepted: Astronomy and Computing
\\ ( https://arxiv.org/abs/2412.14775 ,  2522kb)
------------------------------------------------------------------------------
\\
arXiv:2412.16926
replaced with revised version Wed, 28 May 2025 07:02:54 GMT   (780kb,D)

Title: Revisiting In-Context Learning with Long Context Language Models
Authors: Jinheon Baek, Sun Jae Lee, Prakhar Gupta, Geunseob Oh, Siddharth
  Dalmia, Prateek Kolhar
Categories: cs.CL cs.AI cs.LG
Comments: ACL Findings 2025
\\ ( https://arxiv.org/abs/2412.16926 ,  780kb)
------------------------------------------------------------------------------
\\
arXiv:2501.03124
replaced with revised version Wed, 28 May 2025 07:55:32 GMT   (2469kb,D)

Title: PRMBench: A Fine-grained and Challenging Benchmark for Process-Level
  Reward Models
Authors: Mingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou, Yu Cheng
Categories: cs.CL cs.AI cs.LG
Comments: Accepted by ACL 2025 Main. Project Page: https://prmbench.github.io/
\\ ( https://arxiv.org/abs/2501.03124 ,  2469kb)
------------------------------------------------------------------------------
\\
arXiv:2501.06365
replaced with revised version Wed, 28 May 2025 15:06:30 GMT   (251kb,D)

Title: Gender-Neutral Large Language Models for Medical Applications: Reducing
  Bias in PubMed Abstracts
Authors: Elizabeth Schaefer, Kirk Roberts
Categories: cs.CL cs.AI cs.IR
Comments: 9 pages, 4 figures
\\ ( https://arxiv.org/abs/2501.06365 ,  251kb)
------------------------------------------------------------------------------
\\
arXiv:2501.08316
replaced with revised version Tue, 27 May 2025 21:22:25 GMT   (15366kb,D)

Title: Diffusion Adversarial Post-Training for One-Step Video Generation
Authors: Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, Lu Jiang
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2501.08316 ,  15366kb)
------------------------------------------------------------------------------
\\
arXiv:2501.13567
replaced with revised version Wed, 28 May 2025 08:20:05 GMT   (615kb,D)

Title: K-COMP: Retrieval-Augmented Medical Domain Question Answering With
  Knowledge-Injected Compressor
Authors: Jeonghun Cho, Gary Geunbae Lee
Categories: cs.CL cs.AI
Comments: Accepted at NAACL 2025 (Main, long paper)
\\ ( https://arxiv.org/abs/2501.13567 ,  615kb)
------------------------------------------------------------------------------
\\
arXiv:2501.13953
replaced with revised version Wed, 28 May 2025 08:41:02 GMT   (3582kb,D)

Title: Redundancy Principles for MLLMs Benchmarks
Authors: Zicheng Zhang, Xiangyu Zhao, Xinyu Fang, Chunyi Li, Xiaohong Liu,
  Xiongkuo Min, Haodong Duan, Kai Chen, Guangtao Zhai
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2501.13953 ,  3582kb)
------------------------------------------------------------------------------
\\
arXiv:2501.15081
replaced with revised version Wed, 28 May 2025 04:39:13 GMT   (310kb,D)

Title: Can Large Language Models Be Trusted as Evolutionary Optimizers for
  Network-Structured Combinatorial Problems?
Authors: Jie Zhao, Tao Wen, Kang Hao Cheong
Categories: cs.NE cs.AI
\\ ( https://arxiv.org/abs/2501.15081 ,  310kb)
------------------------------------------------------------------------------
\\
arXiv:2502.00136
replaced with revised version Wed, 28 May 2025 05:28:29 GMT   (5411kb,D)

Title: A Checks-and-Balances Framework for Context-Aware Ethical AI Alignment
Authors: Edward Y. Chang
Categories: cs.CL cs.AI
Comments: 20 pages, 7 tables, 6 figures. arXiv admin note: substantial text
  overlap with arXiv:2405.07076
ACM-class: F.2.2
\\ ( https://arxiv.org/abs/2502.00136 ,  5411kb)
------------------------------------------------------------------------------
\\
arXiv:2502.00619 (*cross-listing*)
replaced with revised version Tue, 27 May 2025 20:28:19 GMT   (6530kb,D)

Title: Distribution-aware Fairness Learning in Medical Image Segmentation From
  A Control-Theoretic Perspective
Authors: Yujin Oh, Pengfei Jin, Sangjoon Park, Sekeun Kim, Siyeop Yoon,
  Kyungsang Kim, Jin Sung Kim, Xiang Li, Quanzheng Li
Categories: eess.IV cs.AI cs.CV
Comments: ICML 2025 spotlight, see https://openreview.net/forum?id=BUONdewsBa
\\ ( https://arxiv.org/abs/2502.00619 ,  6530kb)
------------------------------------------------------------------------------
\\
arXiv:2502.01235 (*cross-listing*)
replaced with revised version Wed, 28 May 2025 09:34:19 GMT   (324kb,D)

Title: LoRA-One: One-Step Full Gradient Could Suffice for Fine-Tuning Large
  Language Models, Provably and Efficiently
Authors: Yuanhe Zhang, Fanghui Liu, Yudong Chen
Categories: stat.ML cs.AI cs.LG
Comments: Accepted by ICML 2025 (Spotlight)
\\ ( https://arxiv.org/abs/2502.01235 ,  324kb)
------------------------------------------------------------------------------
\\
arXiv:2502.02406
replaced with revised version Tue, 27 May 2025 21:46:43 GMT   (861kb,D)

Title: LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in
  Multimodal Large Language Models
Authors: Tzu-Tao Chang, Shivaram Venkataraman
Categories: cs.CV cs.AI cs.DC cs.LG
\\ ( https://arxiv.org/abs/2502.02406 ,  861kb)
------------------------------------------------------------------------------
\\
arXiv:2502.03671
replaced with revised version Wed, 28 May 2025 05:08:18 GMT   (767kb,D)

Title: Advancing Reasoning in Large Language Models: Promising Methods and
  Approaches
Authors: Avinash Patil, Aryan Jadon
Categories: cs.CL cs.AI
Comments: 9 Pages, 1 Figure, IEEE Format
\\ ( https://arxiv.org/abs/2502.03671 ,  767kb)
------------------------------------------------------------------------------
\\
arXiv:2502.05242
replaced with revised version Wed, 28 May 2025 14:27:44 GMT   (4941kb,D)

Title: Beyond External Monitors: Enhancing Transparency of Large Language
  Models for Easier Monitoring
Authors: Guanxu Chen, Dongrui Liu, Tao Luo, Lijie Hu, Jing Shao
Categories: cs.CL cs.AI cs.CV cs.LG
Comments: 25 pages,6 figures,13 tables
\\ ( https://arxiv.org/abs/2502.05242 ,  4941kb)
------------------------------------------------------------------------------
\\
arXiv:2502.05749
replaced with revised version Wed, 28 May 2025 09:21:32 GMT   (26703kb,D)

Title: UniDB: A Unified Diffusion Bridge Framework via Stochastic Optimal
  Control
Authors: Kaizhen Zhu, Mokai Pan, Yuexin Ma, Yanwei Fu, Jingyi Yu, Jingya Wang,
  Ye Shi
Categories: cs.CV cs.AI cs.SY eess.SY
\\ ( https://arxiv.org/abs/2502.05749 ,  26703kb)
------------------------------------------------------------------------------
\\
arXiv:2502.09082
replaced with revised version Wed, 28 May 2025 07:23:09 GMT   (15071kb,D)

Title: CoSER: Coordinating LLM-Based Persona Simulation of Established Roles
Authors: Xintao Wang, Heng Wang, Yifei Zhang, Xinfeng Yuan, Rui Xu, Jen-tse
  Huang, Siyu Yuan, Haoran Guo, Jiangjie Chen, Shuchang Zhou, Wei Wang, Yanghua
  Xiao
Categories: cs.CL cs.AI
Comments: Accepted by ICML 2025
\\ ( https://arxiv.org/abs/2502.09082 ,  15071kb)
------------------------------------------------------------------------------
\\
arXiv:2502.11190
replaced with revised version Wed, 28 May 2025 16:57:30 GMT   (2055kb,D)

Title: ReLearn: Unlearning via Learning for Large Language Models
Authors: Haoming Xu, Ningyuan Zhao, Liming Yang, Sendong Zhao, Shumin Deng,
  Mengru Wang, Bryan Hooi, Nay Oo, Huajun Chen, Ningyu Zhang
Categories: cs.CL cs.AI cs.CV cs.HC cs.LG
Comments: ACL 2025
\\ ( https://arxiv.org/abs/2502.11190 ,  2055kb)
------------------------------------------------------------------------------
\\
arXiv:2502.13458
replaced with revised version Tue, 27 May 2025 22:35:37 GMT   (8089kb,D)

Title: ThinkGuard: Deliberative Slow Thinking Leads to Cautious Guardrails
Authors: Xiaofei Wen, Wenxuan Zhou, Wenjie Jacky Mo, Muhao Chen
Categories: cs.CL cs.AI cs.CR cs.LG
Comments: ACL 2025
\\ ( https://arxiv.org/abs/2502.13458 ,  8089kb)
------------------------------------------------------------------------------
\\
arXiv:2502.13913
replaced with revised version Wed, 28 May 2025 15:25:29 GMT   (10054kb,D)

Title: How Do LLMs Perform Two-Hop Reasoning in Context?
Authors: Tianyu Guo, Hanlin Zhu, Ruiqi Zhang, Jiantao Jiao, Song Mei, Michael
  I. Jordan, Stuart Russell
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2502.13913 ,  10054kb)
------------------------------------------------------------------------------
\\
arXiv:2502.15920
replaced with revised version Wed, 28 May 2025 02:15:07 GMT   (9600kb,D)

Title: Self-Taught Agentic Long Context Understanding
Authors: Yufan Zhuang, Xiaodong Yu, Jialian Wu, Ximeng Sun, Ze Wang, Jiang Liu,
  Yusheng Su, Jingbo Shang, Zicheng Liu, Emad Barsoum
Categories: cs.CL cs.AI
Comments: Published at ACL 2025 Main Conference
\\ ( https://arxiv.org/abs/2502.15920 ,  9600kb)
------------------------------------------------------------------------------
\\
arXiv:2502.20578
replaced with revised version Wed, 28 May 2025 10:39:11 GMT   (12906kb,D)

Title: Interpreting CLIP with Hierarchical Sparse Autoencoders
Authors: Vladimir Zaigrajew, Hubert Baniecki, Przemyslaw Biecek
Categories: cs.CV cs.AI cs.LG
Journal-ref: Proceedings of the 42st International Conference on Machine
  Learning (ICML 2025)
\\ ( https://arxiv.org/abs/2502.20578 ,  12906kb)
------------------------------------------------------------------------------
\\
arXiv:2503.02235
replaced with revised version Wed, 28 May 2025 01:05:58 GMT   (3527kb,D)

Title: Deficient Excitation in Parameter Learning
Authors: Ganghui Cao, Shimin Wang, Martin Guay, Jinzhi Wang, Zhisheng Duan,
  Marios M. Polycarpou
Categories: eess.SY cs.AI cs.SY eess.SP math.OC
Comments: 16 pages,9 figures
\\ ( https://arxiv.org/abs/2503.02235 ,  3527kb)
------------------------------------------------------------------------------
\\
arXiv:2503.02972
replaced with revised version Wed, 28 May 2025 07:44:43 GMT   (3989kb,D)

Title: LINGOLY-TOO: Disentangling Reasoning from Knowledge with Templatised
  Orthographic Obfuscation
Authors: Jude Khouja, Karolina Korgul, Simi Hellsten, Lingyi Yang, Vlad Neacsu,
  Harry Mayne, Ryan Kearns, Andrew Bean, Adam Mahdi
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2503.02972 ,  3989kb)
------------------------------------------------------------------------------
\\
arXiv:2503.06226
replaced with revised version Tue, 27 May 2025 23:59:22 GMT   (167kb)

Title: Optimal Output Feedback Learning Control for Discrete-Time Linear
  Quadratic Regulation
Authors: Kedi Xie, Martin Guay, Shimin Wang, Fang Deng, Maobin Lu
Categories: eess.SY cs.AI cs.MA cs.SY math.OC
Comments: 16 pages, 5 figures
\\ ( https://arxiv.org/abs/2503.06226 ,  167kb)
------------------------------------------------------------------------------
\\
arXiv:2503.07265
replaced with revised version Tue, 27 May 2025 18:05:23 GMT   (10897kb,D)

Title: WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image
  Generation
Authors: Yuwei Niu, Munan Ning, Mengren Zheng, Weiyang Jin, Bin Lin, Peng Jin,
  Jiaqi Liao, Chaoran Feng, Kunpeng Ning, Bin Zhu, Li Yuan
Categories: cs.CV cs.AI cs.CL
Comments: Code, data and leaderboard: https://github.com/PKU-YuanGroup/WISE
ACM-class: I.2.7; I.2.10; I.4.9
\\ ( https://arxiv.org/abs/2503.07265 ,  10897kb)
------------------------------------------------------------------------------
\\
arXiv:2503.12821
replaced with revised version Wed, 28 May 2025 07:52:59 GMT   (2565kb,D)

Title: From Head to Tail: Towards Balanced Representation in Large
  Vision-Language Models through Adaptive Data Calibration
Authors: Mingyang Song, Xiaoye Qu, Jiawei Zhou, Yu Cheng
Categories: cs.CV cs.AI
Comments: Accepted by CVPR 2025
\\ ( https://arxiv.org/abs/2503.12821 ,  2565kb)
------------------------------------------------------------------------------
\\
arXiv:2503.17933
replaced with revised version Wed, 28 May 2025 04:49:23 GMT   (8068kb,D)

Title: Experience Retrieval-Augmentation with Electronic Health Records Enables
  Accurate Discharge QA
Authors: Justice Ou, Tinglin Huang, Yilun Zhao, Ziyang Yu, Peiqing Lu, Rex Ying
Categories: cs.CL cs.AI cs.IR
\\ ( https://arxiv.org/abs/2503.17933 ,  8068kb)
------------------------------------------------------------------------------
\\
arXiv:2503.23907
replaced with revised version Wed, 28 May 2025 11:29:58 GMT   (4479kb,D)

Title: HumanAesExpert: Advancing a Multi-Modality Foundation Model for Human
  Image Aesthetic Assessment
Authors: Zhichao Liao, Xiaokun Liu, Wenyu Qin, Qingyu Li, Qiulin Wang, Pengfei
  Wan, Di Zhang, Long Zeng, Pingfa Feng
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2503.23907 ,  4479kb)
------------------------------------------------------------------------------
\\
arXiv:2504.01002
replaced with revised version Wed, 28 May 2025 14:52:38 GMT   (1766kb,D)

Title: Token embeddings violate the manifold hypothesis
Authors: Michael Robinson and Sourya Dey and Tony Chiang
Categories: cs.CL cs.AI
Comments: 27 pages, 6 figures, 9 tables
MSC-class: 53Z50, 62H15
\\ ( https://arxiv.org/abs/2504.01002 ,  1766kb)
------------------------------------------------------------------------------
\\
arXiv:2504.03561
replaced with revised version Wed, 28 May 2025 17:03:17 GMT   (1815kb,D)

Title: SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge
  Refinement
Authors: Runnan Fang, Xiaobin Wang, Yuan Liang, Shuofei Qiao, Jialong Wu, Zekun
  Xi, Ningyu Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen
Categories: cs.CL cs.AI cs.CV cs.LG cs.MA
Comments: ACL 2025 Findings
\\ ( https://arxiv.org/abs/2504.03561 ,  1815kb)
------------------------------------------------------------------------------
\\
arXiv:2504.08411
replaced with revised version Wed, 28 May 2025 03:30:56 GMT   (3448kb,D)

Title: A Knowledge-guided Adversarial Defense for Resisting Malicious Visual
  Manipulation
Authors: Dawei Zhou, Suzhi Gang, Decheng Liu, Tongliang Liu, Nannan Wang, Xinbo
  Gao
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2504.08411 ,  3448kb)
------------------------------------------------------------------------------
\\
arXiv:2504.08775
replaced with revised version Tue, 27 May 2025 21:30:34 GMT   (26220kb,D)

Title: Layers at Similar Depths Generate Similar Activations Across LLM
  Architectures
Authors: Christopher Wolfram, Aaron Schein
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2504.08775 ,  26220kb)
------------------------------------------------------------------------------
\\
arXiv:2504.15585
replaced with revised version Wed, 28 May 2025 07:33:33 GMT   (9778kb,D)

Title: A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training
  and Deployment
Authors: Kun Wang, Guibin Zhang, Zhenhong Zhou, Jiahao Wu, Miao Yu, Shiqian
  Zhao, Chenlong Yin, Jinhu Fu, Yibo Yan, Hanjun Luo, Liang Lin, Zhihao Xu,
  Haolang Lu, Xinye Cao, Xinyun Zhou, Weifei Jin, Fanci Meng, Junyuan Mao, Yu
  Wang, Hao Wu, Minghe Wang, Fan Zhang, Junfeng Fang, Wenjie Qu, Yue Liu,
  Chengwei Liu, Yifan Zhang, Qiankun Li, Chongye Guo, Yalan Qin, Zhaoxin Fan,
  Yi Ding, Donghai Hong, Jiaming Ji, Yingxin Lai, Zitong Yu, Xinfeng Li, Yifan
  Jiang, Yanhui Li, Xinyu Deng, Junlin Wu, Dongxia Wang, Yihao Huang, Yufei
  Guo, Jen-tse Huang, Qiufeng Wang, Wenxuan Wang, Dongrui Liu, Yanwei Yue,
  Wenke Huang, Guancheng Wan, Heng Chang, Tianlin Li, Yi Yu, Chenghao Li,
  Jiawei Li, Lei Bai, Jie Zhang, Qing Guo, Jingyi Wang, Tianlong Chen, Joey
  Tianyi Zhou, Xiaojun Jia, Weisong Sun, Cong Wu, Jing Chen, Xuming Hu, et al.
  (32 additional authors not shown)
Categories: cs.CR cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2504.15585 ,  9778kb)
------------------------------------------------------------------------------
\\
arXiv:2504.16145
replaced with revised version Wed, 28 May 2025 14:11:06 GMT   (1901kb,D)

Title: Progressive Language-guided Visual Learning for Multi-Task Visual
  Grounding
Authors: Jingchao Wang, Hong Wang, Wenlong Zhang, Kunhua Ji, Dingjiang Huang,
  Yefeng Zheng
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2504.16145 ,  1901kb)
------------------------------------------------------------------------------
\\
arXiv:2504.16979 (*cross-listing*)
replaced with revised version Wed, 28 May 2025 11:18:22 GMT   (1666kb)

Title: Automating tumor-infiltrating lymphocyte assessment in breast cancer
  histopathology images using QuPath: a transparent and accessible machine
  learning pipeline
Authors: Masoud Tafavvoghi, Lars Ailo Bongo, Andr\'e Berli Delgado, Nikita
  Shvetsov, Anders Sildnes, Line Moi, Lill-Tove Rasmussen Busund, Kajsa
  M{\o}llersen
Categories: q-bio.QM cs.AI cs.CV
Comments: 16 Pages, 9 Figures, 3 tables
\\ ( https://arxiv.org/abs/2504.16979 ,  1666kb)
------------------------------------------------------------------------------
\\
arXiv:2505.10537
replaced with revised version Tue, 27 May 2025 22:00:27 GMT   (1329kb,D)

Title: LibIQ: Toward Real-Time Spectrum Classification in O-RAN dApps
Authors: Filippo Olimpieri, Noemi Giustini, Andrea Lacava, Salvatore D'Oro,
  Tommaso Melodia and Francesca Cuomo
Categories: cs.NI cs.AI
Comments: 6 pages, 5 figures, 2 tables
\\ ( https://arxiv.org/abs/2505.10537 ,  1329kb)
------------------------------------------------------------------------------
\\
arXiv:2505.10832
replaced with revised version Wed, 28 May 2025 11:56:56 GMT   (2098kb)

Title: Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models
  via Multi-Stage RL
Authors: Songjun Tu, Jiahao Lin, Qichao Zhang, Xiangyu Tian, Linjing Li,
  Xiangyuan Lan, Dongbin Zhao
Categories: cs.CL cs.AI
Comments: Fisrt Submitted on 16 May 2025; Update on 28 May 2025
MSC-class: 68T50
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2505.10832 ,  2098kb)
------------------------------------------------------------------------------
\\
arXiv:2505.11277
replaced with revised version Wed, 28 May 2025 02:19:51 GMT   (634kb)

Title: Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning
  of LLMs
Authors: Yaorui Shi, Sihang Li, Chang Wu, Zhiyuan Liu, Junfeng Fang, Hengxing
  Cai, An Zhang, Xiang Wang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2505.11277 ,  634kb)
------------------------------------------------------------------------------
\\
arXiv:2505.12312
replaced with revised version Wed, 28 May 2025 08:31:26 GMT   (534kb)

Title: Visuospatial Cognitive Assistant
Authors: Qi Feng
Categories: cs.CV cs.AI cs.CL cs.LG cs.RO
Comments: Author list corrected. In version 1, Hidetoshi Shimodaira was
  included as a co-author without their consent and has been removed from the
  author list
\\ ( https://arxiv.org/abs/2505.12312 ,  534kb)
------------------------------------------------------------------------------
\\
arXiv:2505.12363
replaced with revised version Wed, 28 May 2025 08:29:25 GMT   (1382kb)

Title: Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts
Authors: Qi Feng
Categories: cs.CV cs.AI cs.CL cs.LG cs.RO
Comments: In version 1, Hidetoshi Shimodaira was included as a co-author
  without their consent and has been removed from the author list
\\ ( https://arxiv.org/abs/2505.12363 ,  1382kb)
------------------------------------------------------------------------------
\\
arXiv:2505.13077
replaced with revised version Wed, 28 May 2025 10:48:01 GMT   (3343kb)

Title: Advancing Sequential Numerical Prediction in Autoregressive Models
Authors: Xiang Fei, Jinghui Lu, Qi Sun, Hao Feng, Yanjie Wang, Wei Shi, An-Lan
  Wang, Jingqun Tang, Can Huang
Categories: cs.CL cs.AI cs.LG
Comments: Accepted to ACL 2025 Main Conference
\\ ( https://arxiv.org/abs/2505.13077 ,  3343kb)
------------------------------------------------------------------------------
\\
arXiv:2505.13182
replaced with revised version Wed, 28 May 2025 06:30:45 GMT   (469kb)

Title: Information Science Principles of Machine Learning: A Causal Chain
  Meta-Framework Based on Formalized Information Mapping
Authors: Jianfeng Xu
Categories: cs.LO cs.AI
\\ ( https://arxiv.org/abs/2505.13182 ,  469kb)
------------------------------------------------------------------------------
\\
arXiv:2505.14664
replaced with revised version Wed, 28 May 2025 13:27:59 GMT   (9520kb)

Title: AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of
  Cross-Modal Embeddings
Authors: Yilin Ye, Junchao Huang, Xingchen Zeng, Jiazhi Xia, Wei Zeng
Categories: cs.CV cs.AI cs.HC cs.LG
\\ ( https://arxiv.org/abs/2505.14664 ,  9520kb)
------------------------------------------------------------------------------
\\
arXiv:2505.14827
replaced with revised version Wed, 28 May 2025 01:56:17 GMT   (442kb,D)

Title: Text Generation Beyond Discrete Token Sampling
Authors: Yufan Zhuang, Liyuan Liu, Chandan Singh, Jingbo Shang, Jianfeng Gao
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2505.14827 ,  442kb)
------------------------------------------------------------------------------
\\
arXiv:2505.17002
replaced with revised version Wed, 28 May 2025 11:34:57 GMT   (831kb)

Title: PAEFF: Precise Alignment and Enhanced Gated Feature Fusion for
  Face-Voice Association
Authors: Abdul Hannan, Muhammad Arslan Manzoor, Shah Nawaz, Muhammad Irzam
  Liaqat, Markus Schedl, Mubashir Noman
Categories: cs.CV cs.AI
Comments: Accepted at InterSpeech 2025
\\ ( https://arxiv.org/abs/2505.17002 ,  831kb)
------------------------------------------------------------------------------
\\
arXiv:2505.17367
replaced with revised version Tue, 27 May 2025 16:57:06 GMT   (20269kb)

Title: EVM-Fusion: An Explainable Vision Mamba Architecture with Neural
  Algorithmic Fusion
Authors: Zichuan Yang
Categories: cs.CV cs.AI
Comments: 14 pages, 4 figures
\\ ( https://arxiv.org/abs/2505.17367 ,  20269kb)
------------------------------------------------------------------------------
\\
arXiv:2505.17928
replaced with revised version Wed, 28 May 2025 09:21:00 GMT   (710kb)

Title: Towards Practical Defect-Focused Automated Code Review
Authors: Junyi Lu, Lili Jiang, Xiaojia Li, Jianbing Fang, Fengjun Zhang, Li
  Yang, Chun Zuo
Categories: cs.SE cs.AI cs.CL cs.LG
Comments: Accepted as Spotlight at the 42nd International Conference on Machine
  Learning (ICML 2025)
\\ ( https://arxiv.org/abs/2505.17928 ,  710kb)
------------------------------------------------------------------------------
\\
arXiv:2505.18079
replaced with revised version Wed, 28 May 2025 08:30:39 GMT   (457kb)

Title: Deep Video Discovery: Agentic Search with Tool Use for Long-form Video
  Understanding
Authors: Xiaoyi Zhang, Zhaoyang Jia, Zongyu Guo, Jiahao Li, Bin Li, Houqiang
  Li, Yan Lu
Categories: cs.CV cs.AI cs.CL
Comments: V2 draft. Under review
\\ ( https://arxiv.org/abs/2505.18079 ,  457kb)
------------------------------------------------------------------------------
\\
arXiv:2505.18799
replaced with revised version Wed, 28 May 2025 02:41:12 GMT   (1384kb,D)

Title: ALPS: Attention Localization and Pruning Strategy for Efficient
  Alignment of Large Language Models
Authors: Hao Chen, Haoze Li, Zhiqing Xiao, Lirong Gao, Qi Zhang, Xiaomeng Hu,
  Ningtao Wang, Xing Fu, Junbo Zhao
Categories: cs.CL cs.AI
Comments: 17 pages, 8 figures, 14 tables
\\ ( https://arxiv.org/abs/2505.18799 ,  1384kb)
------------------------------------------------------------------------------
\\
arXiv:2505.18927
replaced with revised version Wed, 28 May 2025 16:18:17 GMT   (122kb,D)

Title: Moderating Harm: Benchmarking Large Language Models for Cyberbullying
  Detection in YouTube Comments
Authors: Amel Muminovic
Categories: cs.CL cs.AI
Comments: Preprint. 9 pages, 3 tables, 1 figure. Not yet submitted to a
  journal. Feedback welcome
\\ ( https://arxiv.org/abs/2505.18927 ,  122kb)
------------------------------------------------------------------------------
\\
arXiv:2505.18975
replaced with revised version Wed, 28 May 2025 06:37:58 GMT   (2011kb,D)

Title: FastMamba: A High-Speed and Efficient Mamba Accelerator on FPGA with
  Accurate Quantization
Authors: Aotao Wang, Haikuo Shao, Shaobo Ma, Zhongfeng Wang
Categories: cs.AR cs.AI
\\ ( https://arxiv.org/abs/2505.18975 ,  2011kb)
------------------------------------------------------------------------------
\\
arXiv:2505.19623
replaced with revised version Wed, 28 May 2025 14:32:56 GMT   (1106kb,D)

Title: AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender
  Systems
Authors: Yu Shang, Peijie Liu, Yuwei Yan, Zijing Wu, Leheng Sheng, Yuanqing Yu,
  Chumeng Jiang, An Zhang, Fengli Xu, Yu Wang, Min Zhang, Yong Li
Categories: cs.IR cs.AI
Comments: 15 pages, 6 figures
\\ ( https://arxiv.org/abs/2505.19623 ,  1106kb)
------------------------------------------------------------------------------
\\
arXiv:2505.19663
replaced with revised version Wed, 28 May 2025 06:20:39 GMT   (33kb)

Title: A Comprehensive Real-World Assessment of Audio Watermarking Algorithms:
  Will They Survive Neural Codecs?
Authors: Yigitcan \"Ozer, Woosung Choi, Joan Serr\`a, Mayank Kumar Singh,
  Wei-Hsiang Liao, Yuki Mitsufuji
Categories: cs.SD cs.AI cs.CR cs.LG eess.AS
Comments: 5 pages; 5 tables; accepted at INTERSPEECH 2025
\\ ( https://arxiv.org/abs/2505.19663 ,  33kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20072
replaced with revised version Wed, 28 May 2025 09:07:11 GMT   (1021kb)

Title: Incentivizing Strong Reasoning from Weak Supervision
Authors: Yige Yuan, Teng Xiao, Shuchang Tao, Xue Wang, Jinyang Gao, Bolin Ding,
  Bingbing Xu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2505.20072 ,  1021kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20081
replaced with revised version Wed, 28 May 2025 06:53:42 GMT   (2234kb)

Title: Inference-time Alignment in Continuous Space
Authors: Yige Yuan, Teng Xiao, Li Yunfan, Bingbing Xu, Shuchang Tao, Yunqi Qiu,
  Huawei Shen, Xueqi Cheng
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2505.20081 ,  2234kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20292
replaced with revised version Wed, 28 May 2025 11:44:33 GMT   (45085kb)

Title: OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for
  Subject-to-Video Generation
Authors: Shenghai Yuan, Xianyi He, Yufan Deng, Yang Ye, Jinfa Huang, Bin Lin,
  Jiebo Luo, Li Yuan
Categories: cs.CV cs.AI
Comments: Code and Dataset: https://github.com/PKU-YuanGroup/OpenS2V-Nexus
\\ ( https://arxiv.org/abs/2505.20292 ,  45085kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20445
replaced with revised version Wed, 28 May 2025 07:11:15 GMT   (40kb)

Title: In-context Language Learning for Endangered Languages in Speech
  Recognition
Authors: Zhaolin Li, Jan Niehues
Categories: cs.CL cs.AI
Comments: Interspeech2025
\\ ( https://arxiv.org/abs/2505.20445 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20718
replaced with revised version Wed, 28 May 2025 15:54:19 GMT   (9350kb)

Title: VLM Can Be a Good Assistant: Enhancing Embodied Visual Tracking with
  Self-Improving Vision-Language Models
Authors: Kui Wu, Shuhang Xu, Hao Chen, Churan Wang, Zhoujun Li, Yizhou Wang,
  Fangwei Zhong
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2505.20718 ,  9350kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20767
replaced with revised version Wed, 28 May 2025 06:17:19 GMT   (8492kb,D)

Title: CogniBench: A Legal-inspired Framework and Dataset for Assessing
  Cognitive Faithfulness of Large Language Models
Authors: Xiaqiang Tang, Jian Li, Keyu Hu, Du Nan, Xiaolong Li, Xi Zhang, Weigao
  Sun, Sihong Xie
Categories: cs.CL cs.AI
Comments: ACL 2025
\\ ( https://arxiv.org/abs/2505.20767 ,  8492kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20813
replaced with revised version Wed, 28 May 2025 01:17:49 GMT   (8801kb,D)

Title: RSCF: Relation-Semantics Consistent Filter for Entity Embedding of
  Knowledge Graph
Authors: Junsik Kim, Jinwook Park, Kangil Kim
Categories: cs.CL cs.AI
Comments: Accepted to ACL 2025, 17 pages, 10 figures
\\ ( https://arxiv.org/abs/2505.20813 ,  8801kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20973
replaced with revised version Wed, 28 May 2025 09:35:54 GMT   (10024kb)

Title: Towards Conversational Development Environments: Using Theory-of-Mind
  and Multi-Agent Architectures for Requirements Refinement
Authors: Keheliya Gallaba and Ali Arabat and Dayi Lin and Mohammed Sayagh and
  Ahmed E. Hassan
Categories: cs.SE cs.AI
\\ ( https://arxiv.org/abs/2505.20973 ,  10024kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21040
replaced with revised version Wed, 28 May 2025 07:02:51 GMT   (347kb,D)

Title: FCKT: Fine-Grained Cross-Task Knowledge Transfer with Semantic
  Contrastive Learning for Targeted Sentiment Analysis
Authors: Wei Chen, Zhao Zhang, Meng Yuan, Kepeng Xu and Fuzhen Zhuang
Categories: cs.CL cs.AI
Comments: 11 pages, 6 figures
\\ ( https://arxiv.org/abs/2505.21040 ,  347kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21277
replaced with revised version Wed, 28 May 2025 14:16:10 GMT   (3746kb)

Title: Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks
  through Expanding Strategy Space
Authors: Yao Huang, Yitong Sun, Shouwei Ruan, Yichi Zhang, Yinpeng Dong,
  Xingxing Wei
Categories: cs.CR cs.AI cs.CL
Comments: 19 pages, 20 figures, accepted by ACL 2025, Findings
\\ ( https://arxiv.org/abs/2505.21277 ,  3746kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21329
replaced with revised version Wed, 28 May 2025 11:44:41 GMT   (152kb)

Title: Something's Fishy In The Data Lake: A Critical Re-evaluation of Table
  Union Search Benchmarks
Authors: Allaa Boutaleb, Bernd Amann, Hubert Naacke, Rafael Angarita
Categories: cs.IR cs.AI cs.CL cs.DB cs.LG
Comments: Accepted @ ACL 2025's Table Representation Learning Workshop (TRL)
\\ ( https://arxiv.org/abs/2505.21329 ,  152kb)
------------------------------------------------------------------------------
\\
arXiv:2410.16665
replaced with revised version Wed, 28 May 2025 03:21:43 GMT   (7336kb,D)

Title: SafetyAnalyst: Interpretable, Transparent, and Steerable Safety
  Moderation for AI Behavior
Authors: Jing-Jing Li, Valentina Pyatkin, Max Kleiman-Weiner, Liwei Jiang,
  Nouha Dziri, Anne G. E. Collins, Jana Schaich Borg, Maarten Sap, Yejin Choi,
  Sydney Levine
Categories: cs.CL cs.CY
Comments: Accepted to ICML 2025
\\ ( https://arxiv.org/abs/2410.16665 ,  7336kb)
------------------------------------------------------------------------------
\\
arXiv:2505.02208
replaced with revised version Wed, 28 May 2025 13:42:03 GMT   (4616kb,D)

Title: Grassroots Federation: Fair Governance of Large-Scale, Decentralized,
  Sovereign Digital Communities
Authors: Ehud Shapiro and Nimrod Talmon
Categories: cs.DC cs.CY cs.SI
\\ ( https://arxiv.org/abs/2505.02208 ,  4616kb)
------------------------------------------------------------------------------
\\
arXiv:2505.04229
replaced with revised version Wed, 28 May 2025 10:36:36 GMT   (694kb,D)

Title: A Weak Supervision Learning Approach Towards an Equitable Mobility
  Estimation
Authors: Theophilus Aidoo, Till Koebe, Akansh Maurya, Hewan Shrestha, Ingmar
  Weber
Categories: cs.CV cs.CY
Comments: To appear in the proceedings of the ICWSM'25 Workshop on Data for the
  Wellbeing of Most Vulnerable (DWMV). Please cite accordingly
\\ ( https://arxiv.org/abs/2505.04229 ,  694kb)
------------------------------------------------------------------------------
\\
arXiv:2406.03674
replaced with revised version Wed, 28 May 2025 04:35:29 GMT   (1321kb,D)

Title: Learning Safe Strategies for Value Maximizing Buyers in Uniform Price
  Auctions
Authors: Negin Golrezaei and Sourav Sahoo
Categories: cs.DS cs.GT
Comments: 61 pages, 4 figures. To appear at ICML 2025
\\ ( https://arxiv.org/abs/2406.03674 ,  1321kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06762
replaced with revised version Wed, 28 May 2025 03:25:26 GMT   (2651kb,D)

Title: Personality-aware Student Simulation for Conversational Intelligent
  Tutoring Systems
Authors: Zhengyuan Liu, Stella Xin Yin, Geyu Lin, Nancy F. Chen
Categories: cs.CL cs.HC
\\ ( https://arxiv.org/abs/2404.06762 ,  2651kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20667
replaced with revised version Wed, 28 May 2025 05:39:28 GMT   (880kb)

Title: How Do Experts Make Sense of Integrated Process Models?
Authors: Tianwa Chen, Barbara Weber, Graeme Shanks, Gianluca Demartini, Marta
  Indulska, Shazia Sadiq
Categories: cs.IR cs.HC cs.SE
\\ ( https://arxiv.org/abs/2505.20667 ,  880kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20701
replaced with revised version Wed, 28 May 2025 04:47:54 GMT   (982kb)

Title: System-driven Cloud Architecture Design Support with Structured State
  Management and Guided Decision Assistance
Authors: Ryosuke Kohita, Akira Kasuga
Categories: cs.SE cs.HC
\\ ( https://arxiv.org/abs/2505.20701 ,  982kb)
------------------------------------------------------------------------------
\\
arXiv:2501.00824
replaced with revised version Wed, 28 May 2025 02:15:54 GMT   (3845kb,D)

Title: How Breakable Is Privacy: Probing and Resisting Model Inversion Attacks
  in Collaborative Inference
Authors: Rongke Liu
Categories: cs.CR cs.IT math.IT
Comments: 15 pages, 5 figures, 6 tables. The experimental data have been
  corrected, and some explanations have been supplemented
\\ ( https://arxiv.org/abs/2501.00824 ,  3845kb)
------------------------------------------------------------------------------
\\
arXiv:2502.17841 (*cross-listing*)
replaced with revised version Wed, 28 May 2025 02:29:05 GMT   (2142kb,D)

Title: Quantifying interdisciplinary synergy in higher STEM education
Authors: Gahyoun Gim, Jinhyuk Yun, Sang Hoon Lee
Categories: physics.soc-ph cond-mat.stat-mech cs.IT math.IT physics.ed-ph
Comments: 24 pages, 9 figures, 3 supplementary tables
\\ ( https://arxiv.org/abs/2502.17841 ,  2142kb)
------------------------------------------------------------------------------
\\
arXiv:2505.18302
replaced with revised version Tue, 27 May 2025 19:42:32 GMT   (1106kb)

Title: Sampling Strategies for Efficient Training of Deep Learning Object
  Detection Algorithms
Authors: Gefei Shen, Yung-Hong Sun, Yu Hen Hu, Hongrui Jiang
Categories: cs.CV cs.IT math.IT
\\ ( https://arxiv.org/abs/2505.18302 ,  1106kb)
------------------------------------------------------------------------------
\\
arXiv:2308.00957 (*cross-listing*)
replaced with revised version Wed, 28 May 2025 00:53:29 GMT   (883kb,D)

Title: Improving the Variance of Differentially Private Randomized Experiments
  through Clustering
Authors: Adel Javanmard, Vahab Mirrokni, Jean Pouget-Abadie
Categories: stat.ML cs.CR cs.LG stat.ME
Comments: 35 pages, 8 figures, accepted at International Conference on Machine
  Learning (ICML 2025)
\\ ( https://arxiv.org/abs/2308.00957 ,  883kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18083
replaced with revised version Wed, 28 May 2025 01:01:10 GMT   (697kb,D)

Title: Meta Co-Training: Two Views are Better than One
Authors: Jay C. Rothenberger, Dimitrios I. Diochnos
Categories: cs.CV cs.LG
Comments: 16 pages, 16 figures, 11 tables, for implementation see
  https://github.com/JayRothenberger/Meta-Co-Training
ACM-class: I.2.6; I.4.10
\\ ( https://arxiv.org/abs/2311.18083 ,  697kb)
------------------------------------------------------------------------------
\\
arXiv:2407.11873 (*cross-listing*)
replaced with revised version Wed, 28 May 2025 14:12:53 GMT   (110kb,D)

Title: Infinite-dimensional Mahalanobis Distance with Applications to
  Kernelized Novelty Detection
Authors: Nikita Zozoulenko, Thomas Cass, Lukas Gonon
Categories: stat.ML cs.LG math.PR
\\ ( https://arxiv.org/abs/2407.11873 ,  110kb)
------------------------------------------------------------------------------
\\
arXiv:2407.14967
replaced with revised version Wed, 28 May 2025 08:09:42 GMT   (281kb,D)

Title: Base and Exponent Prediction in Mathematical Expressions using
  Multi-Output CNN
Authors: Md Laraib Salam, Akash S Balsaraf, Gaurav Gupta and Ashish Rajeshwar
  Kulkarni
Categories: cs.CV cs.LG
Comments: 4 pages, 9 figures
\\ ( https://arxiv.org/abs/2407.14967 ,  281kb)
------------------------------------------------------------------------------
\\
arXiv:2409.09392 (*cross-listing*)
replaced with revised version Wed, 28 May 2025 13:47:31 GMT   (0kb,I)

Title: Topological Eigenvalue Theorems for Tensor Analysis in Multi-Modal Data
  Fusion
Authors: Ronald Katende
Categories: stat.ML cs.LG stat.CO
Comments: Error in Results. Need to re-run them
\\ ( https://arxiv.org/abs/2409.09392 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2410.06025
replaced with revised version Wed, 28 May 2025 17:28:20 GMT   (36369kb,D)

Title: Shielded Diffusion: Generating Novel and Diverse Images using Sparse
  Repellency
Authors: Michael Kirchhof, James Thornton, Louis B\'ethune, Pierre Ablin,
  Eugene Ndiaye, Marco Cuturi
Categories: cs.CV cs.LG stat.ML
Comments: Accepted at ICML 2025
\\ ( https://arxiv.org/abs/2410.06025 ,  36369kb)
------------------------------------------------------------------------------
\\
arXiv:2410.14787 (*cross-listing*)
replaced with revised version Tue, 27 May 2025 20:36:47 GMT   (989kb,D)

Title: Privacy for Free in the Overparameterized Regime
Authors: Simone Bombari and Marco Mondelli
Categories: stat.ML cs.CR cs.LG
Comments: Update after PNAS revision
\\ ( https://arxiv.org/abs/2410.14787 ,  989kb)
------------------------------------------------------------------------------
\\
arXiv:2410.14788 (*cross-listing*)
replaced with revised version Wed, 28 May 2025 14:24:03 GMT   (2816kb,D)

Title: Simultaneously Solving FBSDEs and their Associated Semilinear Elliptic
  PDEs with Small Neural Operators
Authors: Takashi Furuya and Anastasis Kratsios
Categories: math.OC cs.LG cs.NA math.NA math.PR q-fin.CP
Comments: 36 pages + references
\\ ( https://arxiv.org/abs/2410.14788 ,  2816kb)
------------------------------------------------------------------------------
\\
arXiv:2410.15361 (*cross-listing*)
replaced with revised version Wed, 28 May 2025 11:59:33 GMT   (36871kb,D)

Title: A Novel Characterization of the Population Area Under the Risk Coverage
  Curve (AURC) and Rates of Finite Sample Estimators
Authors: Han Zhou, Jordy Van Landeghem, Teodora Popordanoska, Matthew B.
  Blaschko
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2410.15361 ,  36871kb)
------------------------------------------------------------------------------
\\
arXiv:2410.16106 (*cross-listing*)
replaced with revised version Wed, 28 May 2025 00:49:57 GMT   (3443kb,D)

Title: Statistical Inference for Temporal Difference Learning with Linear
  Function Approximation
Authors: Weichen Wu, Gen Li, Yuting Wei, Alessandro Rinaldo
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2410.16106 ,  3443kb)
------------------------------------------------------------------------------
\\
arXiv:2410.16295 (*cross-listing*)
replaced with revised version Tue, 27 May 2025 21:36:46 GMT   (417kb,D)

Title: Universal Approximation of Mean-Field Models via Transformers
Authors: Shiba Biswal, Karthik Elamvazhuthi, and Rishi Sonthalia
Categories: physics.comp-ph cond-mat.dis-nn cond-mat.stat-mech cs.LG stat.ML
\\ ( https://arxiv.org/abs/2410.16295 ,  417kb)
------------------------------------------------------------------------------
\\
arXiv:2410.18774 (*cross-listing*)
replaced with revised version Wed, 28 May 2025 04:13:41 GMT   (777kb,D)

Title: A Stochastic Approximation Approach for Efficient Decentralized
  Optimization on Random Networks
Authors: Chung-Yiu Yau, Haoming Liu, Hoi-To Wai
Categories: math.OC cs.DC cs.LG
Comments: 35 pages, 9 figures, 7 tables
\\ ( https://arxiv.org/abs/2410.18774 ,  777kb)
------------------------------------------------------------------------------
\\
arXiv:2411.01293
replaced with revised version Wed, 28 May 2025 08:16:32 GMT   (3141kb,D)

Title: Diffusion Models as Cartoonists: The Curious Case of High Density
  Regions
Authors: Rafa{\l} Karczewski, Markus Heinonen, Vikas Garg
Categories: cs.CV cs.LG
Comments: ICLR 2025
\\ ( https://arxiv.org/abs/2411.01293 ,  3141kb)
------------------------------------------------------------------------------
\\
arXiv:2411.08937
replaced with revised version Wed, 28 May 2025 07:47:50 GMT   (2365kb,D)

Title: Dual-Head Knowledge Distillation: Enhancing Logits Utilization with an
  Auxiliary Head
Authors: Penghui Yang, Chen-Chen Zong, Sheng-Jun Huang, Lei Feng, Bo An
Categories: cs.CV cs.LG
Comments: Accepted by KDD 2025
\\ ( https://arxiv.org/abs/2411.08937 ,  2365kb)
------------------------------------------------------------------------------
\\
arXiv:2412.08356
replaced with revised version Wed, 28 May 2025 12:20:41 GMT   (1812kb,D)

Title: Zero-Shot Mono-to-Binaural Speech Synthesis
Authors: Alon Levkovitch and Julian Salazar and Soroosh Mariooryad and RJ
  Skerry-Ryan and Nadav Bar and Bastiaan Kleijn and Eliya Nachmani
Categories: cs.SD cs.LG eess.AS
\\ ( https://arxiv.org/abs/2412.08356 ,  1812kb)
------------------------------------------------------------------------------
\\
arXiv:2412.12465
replaced with revised version Wed, 28 May 2025 13:05:34 GMT   (975kb,D)

Title: Core Context Aware Transformers for Long Context Language Modeling
Authors: Yaofo Chen, Zeng You, Shuhai Zhang, Haokun Li, Yirui Li, Yaowei Wang,
  Mingkui Tan
Categories: cs.CL cs.LG
Comments: Accepted for publication at ICML 2025
\\ ( https://arxiv.org/abs/2412.12465 ,  975kb)
------------------------------------------------------------------------------
\\
arXiv:2412.18208 (*cross-listing*)
replaced with revised version Wed, 28 May 2025 10:50:35 GMT   (501kb,D)

Title: Quantum framework for Reinforcement Learning: Integrating Markov
  decision process, quantum arithmetic, and trajectory search
Authors: Thet Htar Su, Shaswot Shresthamali, and Masaaki Kondo
Categories: quant-ph cs.LG
Journal-ref: Physical Review A (2025)
DOI: 10.1103/5lfr-xb8m
\\ ( https://arxiv.org/abs/2412.18208 ,  501kb)
------------------------------------------------------------------------------
\\
arXiv:2501.00421
replaced with revised version Tue, 27 May 2025 20:33:04 GMT   (29kb)

Title: Outlier-Robust Linear System Identification Under Heavy-tailed Noise
Authors: Vinay Kanakeri and Aritra Mitra
Categories: eess.SY cs.LG cs.SY math.OC
\\ ( https://arxiv.org/abs/2501.00421 ,  29kb)
------------------------------------------------------------------------------
\\
arXiv:2501.00777
replaced with revised version Wed, 28 May 2025 15:18:07 GMT   (5945kb,D)

Title: FitCF: A Framework for Automatic Feature Importance-guided
  Counterfactual Example Generation
Authors: Qianli Wang, Nils Feldhus, Simon Ostermann, Luis Felipe Villa-Arenas,
  Sebastian M\"oller, Vera Schmitt
Categories: cs.CL cs.LG
Comments: ACL 2025 Findings; camera-ready version
\\ ( https://arxiv.org/abs/2501.00777 ,  5945kb)
------------------------------------------------------------------------------
\\
arXiv:2501.14431
replaced with revised version Wed, 28 May 2025 06:18:34 GMT   (16110kb,D)

Title: Domaino1s: Guiding LLM Reasoning for Explainable Answers in High-Stakes
  Domains
Authors: Xu Chu, Zhijie Tan, Hanlin Xue, Guanyu Wang, Tong Mo, Weiping Li
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2501.14431 ,  16110kb)
------------------------------------------------------------------------------
\\
arXiv:2501.18283 (*cross-listing*)
replaced with revised version Wed, 28 May 2025 12:25:44 GMT   (8790kb,D)

Title: Random Feature Representation Boosting
Authors: Nikita Zozoulenko, Thomas Cass, Lukas Gonon
Categories: stat.ML cs.LG
Comments: To appear in ICML 2025
\\ ( https://arxiv.org/abs/2501.18283 ,  8790kb)
------------------------------------------------------------------------------
\\
arXiv:2502.00602
replaced with revised version Wed, 28 May 2025 04:42:15 GMT   (258kb,D)

Title: Mitigating Heterogeneous Token Overfitting in LLM Knowledge Editing
Authors: Tianci Liu and Ruirui Li and Zihan Dong and Hui Liu and Xianfeng Tang
  and Qingyu Yin and Linjun Zhang and Haoyu Wang and Jing Gao
Categories: cs.CL cs.LG
Comments: ICML 2025
\\ ( https://arxiv.org/abs/2502.00602 ,  258kb)
------------------------------------------------------------------------------
\\
arXiv:2502.01347 (*cross-listing*)
replaced with revised version Tue, 27 May 2025 20:47:48 GMT   (506kb,D)

Title: Spurious Correlations in High Dimensional Regression: The Roles of
  Regularization, Simplicity Bias and Over-Parameterization
Authors: Simone Bombari and Marco Mondelli
Categories: stat.ML cs.LG
Comments: Revision after ICML 2025 reviews
\\ ( https://arxiv.org/abs/2502.01347 ,  506kb)
------------------------------------------------------------------------------
\\
arXiv:2502.03210 (*cross-listing*)
replaced with revised version Wed, 28 May 2025 12:21:28 GMT   (1413kb,D)

Title: From Kernels to Features: A Multi-Scale Adaptive Theory of Feature
  Learning
Authors: Noa Rubin, Kirsten Fischer, Javed Lindner, David Dahmen, Inbar
  Seroussi, Zohar Ringel, Michael Kr\"amer, Moritz Helias
Categories: cond-mat.dis-nn cs.LG stat.ML
Comments: 33 pages, 12 figures, accepted at International Conference on Machine
  Learning 2025
\\ ( https://arxiv.org/abs/2502.03210 ,  1413kb)
------------------------------------------------------------------------------
\\
arXiv:2502.06067 (*cross-listing*)
replaced with revised version Wed, 28 May 2025 13:55:38 GMT   (2327kb,D)

Title: Smooth Sailing: Lipschitz-Driven Uncertainty Quantification for Spatial
  Association
Authors: David R. Burt, Renato Berlinghieri, Stephen Bates, Tamara Broderick
Categories: stat.ML cs.LG stat.ME
Comments: The first two authors contributed equally; 36 pages, 14 figures
\\ ( https://arxiv.org/abs/2502.06067 ,  2327kb)
------------------------------------------------------------------------------
\\
arXiv:2502.07365
replaced with revised version Wed, 28 May 2025 08:04:23 GMT   (535kb,D)

Title: LongReD: Mitigating Short-Text Degradation of Long-Context Large
  Language Models via Restoration Distillation
Authors: Zican Dong, Junyi Li, Jinhao Jiang, Mingyu Xu, Wayne Xin Zhao,
  Bingning Wang, Weipeng Chen
Categories: cs.CL cs.LG
Comments: ACL2025 Main
\\ ( https://arxiv.org/abs/2502.07365 ,  535kb)
------------------------------------------------------------------------------
\\
arXiv:2502.10158 (*cross-listing*)
replaced with revised version Wed, 28 May 2025 02:29:42 GMT   (11590kb,D)

Title: Combinatorial Reinforcement Learning with Preference Feedback
Authors: Joongkyu Lee, Min-hwan Oh
Categories: stat.ML cs.LG
Comments: Accepted at ICML 2025
\\ ( https://arxiv.org/abs/2502.10158 ,  11590kb)
------------------------------------------------------------------------------
\\
arXiv:2502.14350
replaced with revised version Wed, 28 May 2025 08:50:44 GMT   (0kb,I)

Title: Optimize Cardinality Estimation Model Pretraining by Simplifying the
  Training Datasets
Authors: Boyang Fang
Categories: cs.DB cs.LG
Comments: Preliminary analysis uncovered data quality concerns in our
  experimental dataset
\\ ( https://arxiv.org/abs/2502.14350 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2503.09674
replaced with revised version Tue, 27 May 2025 19:19:48 GMT   (5725kb,D)

Title: Probabilistic Reasoning with LLMs for k-anonymity Estimation
Authors: Jonathan Zheng, Sauvik Das, Alan Ritter, Wei Xu
Categories: cs.CL cs.LG
Comments: 9 pages, preprint
\\ ( https://arxiv.org/abs/2503.09674 ,  5725kb)
------------------------------------------------------------------------------
\\
arXiv:2503.09790
replaced with revised version Tue, 27 May 2025 23:48:45 GMT   (5957kb,D)

Title: Constrained Discrete Diffusion
Authors: Michael Cardei, Jacob K Christopher, Thomas Hartvigsen, Brian R.
  Bartoldson, Bhavya Kailkhura, Ferdinando Fioretto
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2503.09790 ,  5957kb)
------------------------------------------------------------------------------
\\
arXiv:2503.10460
replaced with revised version Wed, 28 May 2025 12:32:29 GMT   (2698kb,D)

Title: Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and
  Beyond
Authors: Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin
  Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng,
  Shousheng Jia, Xiangzheng Zhang
Categories: cs.CL cs.LG
Comments: v4: ACL'25 industry track camera ready; v3: minor modifications; v2:
  better writing & format for later submission; all release at
  https://github.com/Qihoo360/Light-R1
\\ ( https://arxiv.org/abs/2503.10460 ,  2698kb)
------------------------------------------------------------------------------
\\
arXiv:2503.11688
replaced with revised version Wed, 28 May 2025 07:40:39 GMT   (5665kb,D)

Title: Towards Resilient and Sustainable Global Industrial Systems: An
  Evolutionary-Based Approach
Authors: V\'aclav Jirkovsk\'y, Ji\v{r}\'i Kubal\'ik, Petr Kadera, Arnd
  Schirrmann, Andreas Mitschke, Andreas Zindel
Categories: cs.NE cs.LG math.OC
Comments: Preprint submitted to Expert Systems with Applications
\\ ( https://arxiv.org/abs/2503.11688 ,  5665kb)
------------------------------------------------------------------------------
\\
arXiv:2503.19385
replaced with revised version Wed, 28 May 2025 14:20:39 GMT   (28504kb,D)

Title: Inference-Time Scaling for Flow Models via Stochastic Generation and
  Rollover Budget Forcing
Authors: Jaihoon Kim, Taehoon Yoon, Jisung Hwang, Minhyuk Sung
Categories: cs.CV cs.LG
Comments: Project page: https://flow-inference-time-scaling.github.io/
\\ ( https://arxiv.org/abs/2503.19385 ,  28504kb)
------------------------------------------------------------------------------
\\
arXiv:2503.22194
replaced with revised version Wed, 28 May 2025 07:22:19 GMT   (43908kb,D)

Title: ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation
Authors: Yunhong Min, Daehyeon Choi, Kyeongmin Yeo, Jihyun Lee, Minhyuk Sung
Categories: cs.CV cs.LG
Comments: Project Page: https://origen2025.github.io
\\ ( https://arxiv.org/abs/2503.22194 ,  43908kb)
------------------------------------------------------------------------------
\\
arXiv:2504.03312
replaced with revised version Wed, 28 May 2025 11:37:18 GMT   (66kb,D)

Title: Evaluating Compact LLMs for Zero-Shot Iberian Language Tasks on End-User
  Devices
Authors: Lu\'is Couto Seller, \'I\~nigo Sanz Torres, Adri\'an
  Vogel-Fern\'andez, Carlos Gonz\'alez Carballo, Pedro Miguel S\'anchez
  S\'anchez, Adri\'an Carruana Mart\'in, Enrique de Miguel Ambite
Categories: cs.CL cs.LG
Comments: Accepted at SEPLN 2025 conference
\\ ( https://arxiv.org/abs/2504.03312 ,  66kb)
------------------------------------------------------------------------------
\\
arXiv:2504.06792
replaced with revised version Wed, 28 May 2025 08:08:05 GMT   (889kb,D)

Title: Domain-Specific Pruning of Large Mixture-of-Experts Models with Few-shot
  Demonstrations
Authors: Zican Dong and Han Peng and Peiyu Liu and Wayne Xin Zhao and Dong Wu
  and Feng Xiao and Zhifeng Wang
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2504.06792 ,  889kb)
------------------------------------------------------------------------------
\\
arXiv:2504.17836 (*cross-listing*)
replaced with revised version Tue, 27 May 2025 22:43:54 GMT   (1060kb,D)

Title: Learning Enhanced Ensemble Filters
Authors: Eviatar Bach, Ricardo Baptista, Edoardo Calvello, Bohan Chen, Andrew
  Stuart
Categories: stat.ML cs.LG cs.SY eess.SY physics.comp-ph
Comments: Preprint submitted to Journal of Computational Physics
\\ ( https://arxiv.org/abs/2504.17836 ,  1060kb)
------------------------------------------------------------------------------
\\
arXiv:2505.04937 (*cross-listing*)
replaced with revised version Wed, 28 May 2025 06:07:45 GMT   (543kb,D)

Title: Generalization Analysis for Supervised Contrastive Representation
  Learning under Non-IID Settings
Authors: Nong Minh Hieu and Antoine Ledent
Categories: stat.ML cs.LG
Comments: 40 pages, 3 figures. Published in the 42th Proceedings of
  International Conference in Machine Learning (ICML 2025)
\\ ( https://arxiv.org/abs/2505.04937 ,  543kb)
------------------------------------------------------------------------------
\\
arXiv:2505.05151 (*cross-listing*)
replaced with revised version Wed, 28 May 2025 14:11:47 GMT   (3558kb,D)

Title: Overcoming Dimensional Factorization Limits in Discrete Diffusion Models
  through Quantum Joint Distribution Learning
Authors: Chuangtao Chen, Qinglin Zhao, MengChu Zhou, Zhimin He, Haozhen Situ
Categories: quant-ph cs.LG
Comments: Comments are welcome
\\ ( https://arxiv.org/abs/2505.05151 ,  3558kb)
------------------------------------------------------------------------------
\\
arXiv:2505.14310
replaced with revised version Wed, 28 May 2025 10:52:31 GMT   (3490kb,D)

Title: Taming Recommendation Bias with Causal Intervention on Evolving Personal
  Popularity
Authors: Shiyin Tan, Dongyuan Li, Renhe Jiang, Zhen Wang, Xingtong Yu, Manabu
  Okumura
Categories: cs.IR cs.LG
\\ ( https://arxiv.org/abs/2505.14310 ,  3490kb)
------------------------------------------------------------------------------
\\
arXiv:2505.17999
replaced with revised version Wed, 28 May 2025 14:16:45 GMT   (1564kb,D)

Title: Revisiting Feature Interactions from the Perspective of Quadratic Neural
  Networks for Click-through Rate Prediction
Authors: Honghao Li, Yiwen Zhang, Yi Zhang, Lei Sang, and Jieming Zhu
Categories: cs.IR cs.LG
Comments: KDD'25 accepted
\\ ( https://arxiv.org/abs/2505.17999 ,  1564kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20498
replaced with revised version Wed, 28 May 2025 02:10:51 GMT   (36663kb)

Title: ControlTac: Force- and Position-Controlled Tactile Data Augmentation
  with a Single Reference Image
Authors: Dongyu Luo, Kelin Yu, Amir-Hossein Shahidzadeh, Cornelia Ferm\"uller,
  Yiannis Aloimonos, Ruohan Gao
Categories: cs.CV cs.LG cs.RO
Comments: 22 pages, 11 figures, 7 tables
\\ ( https://arxiv.org/abs/2505.20498 ,  36663kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20538
replaced with revised version Wed, 28 May 2025 14:54:54 GMT   (6384kb,D)

Title: AstroVisBench: A Code Benchmark for Scientific Computing and
  Visualization in Astronomy
Authors: Sebastian Antony Joseph, Syed Murtaza Husain, Stella S. R. Offner,
  St\'ephanie Juneau, Paul Torrey, Adam S. Bolton, Juan P. Farias, Niall
  Gaffney, Greg Durrett, Junyi Jessy Li
Categories: cs.CL astro-ph.IM cs.LG
\\ ( https://arxiv.org/abs/2505.20538 ,  6384kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20789
replaced with revised version Wed, 28 May 2025 02:36:11 GMT   (17324kb,D)

Title: Integrating Intermediate Layer Optimization and Projected Gradient
  Descent for Solving Inverse Problems with Diffusion Models
Authors: Yang Zheng, Wen Li, Zhaoqiang Liu
Categories: cs.CV cs.LG
Comments: ICML 2025
\\ ( https://arxiv.org/abs/2505.20789 ,  17324kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21356
replaced with revised version Wed, 28 May 2025 01:58:07 GMT   (5224kb)

Title: Towards Robust Automated Perceptual Voice Quality Assessment with Speech
  Foundation Models
Authors: Whenty Ariyanti, Kuan-Yu Chen, Sabato Marco Siniscalchi, Hsin-Min
  Wang, Yu Tsao
Categories: cs.SD cs.LG eess.AS
\\ ( https://arxiv.org/abs/2505.21356 ,  5224kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21357
replaced with revised version Wed, 28 May 2025 09:24:45 GMT   (3659kb)

Title: AgriFM: A Multi-source Temporal Remote Sensing Foundation Model for Crop
  Mapping
Authors: Wenyuan Li, Shunlin Liang, Keyan Chen, Yongzhe Chen, Han Ma, Jianglei
  Xu, Yichuan Ma, Shikang Guan, Husheng Fang and Zhenwei Shi
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2505.21357 ,  3659kb)
------------------------------------------------------------------------------
\\
arXiv:2409.17680
replaced with revised version Tue, 27 May 2025 20:26:36 GMT   (31922kb,D)

Title: Event-based Stereo Depth Estimation: A Survey
Authors: Suman Ghosh and Guillermo Gallego
Categories: cs.CV cs.RO
Comments: 28 pages, 24 figures, 7 tables
\\ ( https://arxiv.org/abs/2409.17680 ,  31922kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---